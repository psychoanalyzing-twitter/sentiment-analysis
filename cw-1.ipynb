{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Distribution\n",
    "| Member | ID | Tasks |\n",
    "|---|---|---|\n",
    "|Bhavika| - | - | \n",
    "|Alora| - | - |\n",
    "|Andrea| - | - |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.12.1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tweepy) (2.28.0)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.0.12)\n"
     ]
    }
   ],
   "source": [
    "%pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keys for authentication\n",
    "consumer_key = \"ONLQ0vLSAXM2tZDnckominEcK\"\n",
    "consumer_secret = \"MpP7XurdGn1wpufX3rrfQgrs4AROnaw9ZiiZzKN6exr3zEDlN6\"\n",
    "access_token = \"1274112117738737664-J2DjjoqqzN11CNp8bpnYzVp1dUYKM4\"\n",
    "access_token_secret = \"3coYFJVCqHnOmD2m9Qt6zs7j1I24lhlPTU71eUOq4Zqg9\"\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAALJslQEAAAAA7EWdkTwrVDM44LIuewNascHjvoY%3DeXmhQyuXgo8zXJiSTMfqtmPA393HHO2W7nSrk0bK6b67SuJeb0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate\n",
    "auth = tweepy.OAuth1UserHandler(\n",
    "    consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tweets(query: str, amount: int) -> list:\n",
    "    \"\"\"Scrapes a specified number of tweets based on given query and location.\n",
    "\n",
    "    Method from https://www.sahilfruitwala.com/guide-to-extract-tweets-using-tweepy#how-to-retrieve-specific-number-of-tweets-using-tweepy \n",
    "    \n",
    "    Args:\n",
    "        query (str): The query\n",
    "    Returns:\n",
    "        list (str): List of contents of tweets.\n",
    "    \"\"\"\n",
    "    extracted_tweets = []\n",
    "    for tweet in tweepy.Cursor(api.search_tweets, query, count=100, tweet_mode=\"extended\", result_type=\"recent\").items(amount):\n",
    "        extracted_tweets.append(tweet.full_text)\n",
    "    return extracted_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(tweets: list, csv_name: str) -> None:\n",
    "    \"\"\"Appends the list of tweets to csv.\n",
    "    \n",
    "    Largely copied from https://gist.github.com/anku255/0cebd75cce675f2b56de1ef48ec06575.\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Writing {0} tweets to {1}\".format(len(tweets), csv_name))\n",
    "    tweets_for_csv = [[tweet.encode(\"utf-8\")] for tweet in tweets]\n",
    "    with open(csv_name, \"a+\") as file:\n",
    "        writer = csv.writer(file, delimiter=\",\")\n",
    "        writer.writerows(tweets_for_csv)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The queries for scraping tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "queries = [\n",
    "    [\"(metaverse OR meta verse OR #metaverse OR #meta #verse) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(innovation OR #innovation OR innovate OR innovative OR #innovate OR #innovative) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(sustainability OR #sustainability OR sustainable) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(technology footprint OR technology OR #technologyfootprint) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(artificial intelligence OR ai OR #ai OR #artificialintelligence) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(ethics OR #ethics OR ethical OR #ethical) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(inflation OR #inflation) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cop 28 OR cop28 OR #cop28 OR #cop #28) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(museum of the future OR @museumofthefuture OR #museumofthefuture OR #museum #of #the #future) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(bit coin OR bitcoin OR #bitcoin OR #crypto OR crpyto OR cryptocurrency) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cyber attack OR #cyberattack OR #cyber OR #cyberattacks OR #cyber #attack) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(robot OR robots OR #robots OR #robot) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(drone OR drones OR #drone OR #drones) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(hacker OR hacking OR #hacker OR #hacking OR #hack) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(chatgpt OR #chatgpt OR #chat #gpt OR chat gpt) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cybersecurity OR #cybersecurityOR #cybersec OR cybersec OR cyber security OR #cyber #security) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(sustainabletech OR sustainable ai OR sustainable technology OR #sustainabletech OR #sustainableai OR #sustainabletechnology) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(computing OR quantum computing OR #quantum #computing OR #computing) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(automation OR #automation) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(distributed cloud OR #distributedcloud OR #distributed #cloud) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(big data OR #big #data OR bigdata OR #bigdata) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(ar OR #ar OR #augmentedreality OR #augmented #reality) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(data mining OR #data #mining OR #datamining) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(software OR #software) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(tech OR technology OR #tech OR #technology) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(digital OR digital transformation OR #digital #transformation OR #digitaltransformation) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(blockchain OR #blockchain) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(coders_hq OR coders hq OR #coders OR #coders #hq OR @coders_hq) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(nft OR non-fungible token OR non fungible token OR nfts OR #nfts OR #nonfungibletoken OR #nft) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(binance OR @binance OR #binance) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(uae hackathon OR hackathon OR #hackathon) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(internet of things OR IoT OR #iot OR #internet #of #things OR #internetofthings) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(software engineering OR #softwareengineering) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(#coding OR coding) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(@uaeai OR #uaeai Or #uae #ai) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(technology OR tech OT #tech Or #technology) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(nft OR #nft OR #nfts OR nfts) AND (place_country:AE OR uae) lang:en -filter:retweets\"]   ,\n",
    "    [\"(UAE codes OR #UAE_codes OR #UAEcodes) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(Microsoft Hololens OR Hololens OR #Hololens) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(GITEX or #gitex) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(information OR #information) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(internet OR #internet) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(computer OR science OR computer science OR #computerscience OR #computer OR #science) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(technological OR #technological) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(engineering OR #engineering) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(computing OR #computing) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(systems OR #systems) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(electronics OR #electronics) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(application OR #application OR app OR #app) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(robotics OR #robotics) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(high tech OR high technology OR #hightech OR #hightechnology OR #high #technology OR #high #tech) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(nanotech OR nano tech OR nanotechnology OR nano technology OR #nanotech OR #nano #tech OR #nano #technology OR #nanotechnology) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(biotech OR bio tech OR biotechnology OR bio technology OR #biotech OR #bio #tech OR #bio #technology OR #biotechnology) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(code OR #code) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(automation OR #automation OR automate OR #automate) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(virtual OR #virtual OR online OR #online) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(technological advancements OR #technological #advancements OR #advancements OR #techadvancements) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(bionics OR #bionics) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cyberscience OR #cyberscience) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(google OR #google OR @google) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(oracle OR #oracle OR @oracle) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(ibm OR #ibm OR @ibm) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(bionics OR #bionics) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(sap OR #sap) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(global knowledge OR #globalknowledge OR #global #knowledge) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(amazon OR #amazon OR @amazon) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(hewlett-packard OR Hewlett Packard OR HP OR #hewlett-packard OR #Hewlett #Packard OR #HP) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cisco OR #cisco OR @cisco) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(dell OR #dell OR @dell) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cisco OR #cisco OR @cisco) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(@WorldGovSummit OR #WGS OR #WGS2023 OR World Goverment Summit) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(masdar OR #masdar OR @masdar) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(renewable energy OR clean energy OR #renewableenergy OR #cleanenergy) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(machine learning OR #machinelearning OR ML OR #ML) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(recommender systems OR #RS OR #recommender #systems OR recommender engine) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(deep learning OR #deeplearning) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(research OR #research) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commented out below to prevent re-running code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Scraping tweets and writing them to dataset.csv\"\"\"\n",
    "# csv_name=\"dataset.csv\"\n",
    "# total = 0\n",
    "# for query in queries:\n",
    "#     tweets = scrape_tweets(query=query, amount=500)\n",
    "#     total += len(tweets)\n",
    "#     print(\"Found {0} tweets related to the query {1}\".format(len(tweets), query))\n",
    "#     write_to_csv(tweets=tweets,csv_name=csv_name)\n",
    "# print(\"Done. {0} rows.\".format(total))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section cleans the scraped tweets dataset of duplicate tweets, URLs, @'s and #s."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "- Do not modify original csv `dataset.csv`. The dataset with no duplicates is stored in `updated_dataset.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates\n",
    "dataset = pd.read_csv(\"dataset.csv\", header=None)\n",
    "dataset.rename(columns={0: 'Text'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'Joined UAE Minister &amp;amp; President-Designat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Foreign Secretary @AmbVMKwatra participated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'As the UAE marks National Environment Day, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'RT @KhalifaAlgaz: Did you know that the UAE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'RT @tawhidChtioui: \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21974</th>\n",
       "      <td>b'Send our special gifts to your loved ones!\\x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21975</th>\n",
       "      <td>b'Trainer code: 2735 9457 4031\\nfrom \\xf0\\x9f\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21976</th>\n",
       "      <td>b\"We are looking for ICY SNOW , SANDSTORM .\\nI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21977</th>\n",
       "      <td>b'Make #MONEY from #home through PC #APP \\nMak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21978</th>\n",
       "      <td>b'#Earn Money from #home through PC #APP\\nwith...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21979 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text\n",
       "0      b'Joined UAE Minister &amp; President-Designat...\n",
       "1      b'Foreign Secretary @AmbVMKwatra participated ...\n",
       "2      b'As the UAE marks National Environment Day, w...\n",
       "3      b'RT @KhalifaAlgaz: Did you know that the UAE ...\n",
       "4      b'RT @tawhidChtioui: \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\...\n",
       "...                                                  ...\n",
       "21974  b'Send our special gifts to your loved ones!\\x...\n",
       "21975  b'Trainer code: 2735 9457 4031\\nfrom \\xf0\\x9f\\...\n",
       "21976  b\"We are looking for ICY SNOW , SANDSTORM .\\nI...\n",
       "21977  b'Make #MONEY from #home through PC #APP \\nMak...\n",
       "21978  b'#Earn Money from #home through PC #APP\\nwith...\n",
       "\n",
       "[21979 rows x 1 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'Joined UAE Minister &amp;amp; President-Designat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Foreign Secretary @AmbVMKwatra participated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'As the UAE marks National Environment Day, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'RT @KhalifaAlgaz: Did you know that the UAE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'RT @tawhidChtioui: \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21888</th>\n",
       "      <td>b'@m_ut67 @modgovae @Forsan_UAE Dear sir pleas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21889</th>\n",
       "      <td>b'@UAE_BARQ Dear sir please my help you sir my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21890</th>\n",
       "      <td>b'@UAE_BARQ Dear sir please my help you sir my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21915</th>\n",
       "      <td>b\"#PhonePe launched a service on Tuesday that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21962</th>\n",
       "      <td>b'@WeLoveIndia7 Saudi k log bhikhari h ? \\nIn ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5878 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text\n",
       "0      b'Joined UAE Minister &amp; President-Designat...\n",
       "1      b'Foreign Secretary @AmbVMKwatra participated ...\n",
       "2      b'As the UAE marks National Environment Day, w...\n",
       "3      b'RT @KhalifaAlgaz: Did you know that the UAE ...\n",
       "4      b'RT @tawhidChtioui: \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\...\n",
       "...                                                  ...\n",
       "21888  b'@m_ut67 @modgovae @Forsan_UAE Dear sir pleas...\n",
       "21889  b'@UAE_BARQ Dear sir please my help you sir my...\n",
       "21890  b'@UAE_BARQ Dear sir please my help you sir my...\n",
       "21915  b\"#PhonePe launched a service on Tuesday that ...\n",
       "21962  b'@WeLoveIndia7 Saudi k log bhikhari h ? \\nIn ...\n",
       "\n",
       "[5878 rows x 1 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset = dataset.drop_duplicates().copy()\n",
    "updated_dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing URLs, @'s and hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    b'Joined UAE Minister &amp; President-Designat...\n",
       "1    b'Foreign Secretary  participated in a meeting...\n",
       "2    b'As the UAE marks National Environment Day, w...\n",
       "3    b'RT  Did you know that the UAE is developing ...\n",
       "4    b'RT  \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\x8f UAE lunar r...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = updated_dataset['Text'].copy()\n",
    "updated_dataset['Cleaned Text'] = tweets.str.replace(r'@[^\\s]+|#[^\\s]+|https?:\\/\\/\\S+|www\\.\\S+', '',regex=True)\n",
    "updated_dataset['Cleaned Text'].head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicates again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5398, 1)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset = updated_dataset.drop('Text', axis=1)\n",
    "updated_dataset = updated_dataset.drop_duplicates()\n",
    "updated_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dataset.to_csv('updated-dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cleaned Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'Joined UAE Minister &amp;amp; President-Designat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Foreign Secretary  participated in a meeting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'As the UAE marks National Environment Day, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'RT  Did you know that the UAE is developing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'RT  \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\x8f UAE lunar r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21888</th>\n",
       "      <td>b'   Dear sir please my help you sir my two ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21889</th>\n",
       "      <td>b' Dear sir please my help you sir my two kidn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21890</th>\n",
       "      <td>b' Dear sir please my help you sir my two kidn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21915</th>\n",
       "      <td>b\" launched a service on Tuesday that will all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21962</th>\n",
       "      <td>b' Saudi k log bhikhari h ? \\nIn Middle East, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5398 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Cleaned Text\n",
       "0      b'Joined UAE Minister &amp; President-Designat...\n",
       "1      b'Foreign Secretary  participated in a meeting...\n",
       "2      b'As the UAE marks National Environment Day, w...\n",
       "3      b'RT  Did you know that the UAE is developing ...\n",
       "4      b'RT  \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\x8f UAE lunar r...\n",
       "...                                                  ...\n",
       "21888  b'   Dear sir please my help you sir my two ki...\n",
       "21889  b' Dear sir please my help you sir my two kidn...\n",
       "21890  b' Dear sir please my help you sir my two kidn...\n",
       "21915  b\" launched a service on Tuesday that will all...\n",
       "21962  b' Saudi k log bhikhari h ? \\nIn Middle East, ...\n",
       "\n",
       "[5398 rows x 1 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset = pd.read_csv('updated-dataset.csv', index_col=0)\n",
    "updated_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Labelling\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data labelling, we decided to use first use TextBlob. TextBlob calculates the subjectivity and polarity of a text to classify the text as 'Positive', 'Negative' or 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: TextBlob in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from TextBlob) (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.1->TextBlob) (2022.10.31)\n",
      "Requirement already satisfied: click in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.1->TextBlob) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.1->TextBlob) (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.1->TextBlob) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk>=3.1->TextBlob) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get subjectivity\n",
    "def getSubjectivity(text):\n",
    "   return TextBlob(text).sentiment.subjectivity\n",
    "  \n",
    " \n",
    "#Function to get the polarity\n",
    "def getPolarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "   \n",
    "#Create two new columns ‘Subjectivity’ & ‘Polarity’\n",
    "updated_dataset['TextBlob Subjectivity'] =    updated_dataset['Cleaned Text'].apply(getSubjectivity)\n",
    "updated_dataset['TextBlob Polarity'] = updated_dataset['Cleaned Text'].apply(getPolarity)\n",
    "def getAnalysis(score):\n",
    "  if score < 0:\n",
    "    return 'Negative'\n",
    "  elif score == 0:\n",
    "    return 'Neutral'\n",
    "  else:\n",
    "    return 'Positive'\n",
    "\n",
    "updated_dataset['TextBlob Sentiment'] =  updated_dataset['TextBlob Polarity'].apply(getAnalysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive    2643\n",
       "Neutral     2276\n",
       "Negative     479\n",
       "Name: TextBlob Sentiment, dtype: int64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset['TextBlob Sentiment'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that TextBlob classified the majority of tweets to be Positive. To compare, we also decided to use Vader as well to observe if there was a major difference in classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaderSentiment) (2.28.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->vaderSentiment) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->vaderSentiment) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->vaderSentiment) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->vaderSentiment) (3.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "def getSentiment(text):\n",
    "    return sentiment.polarity_scores(text)\n",
    "    \n",
    "updated_dataset['Vader Analysis'] = updated_dataset['Cleaned Text'].apply(getSentiment)\n",
    "\n",
    "def sentimentAnalysis(sentiment_dict):\n",
    "    if sentiment_dict['compound'] >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif sentiment_dict['compound'] <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "updated_dataset['Vader Sentiment'] = updated_dataset['Vader Analysis'].apply(sentimentAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vader Analysis</th>\n",
       "      <th>Vader Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.638, 'pos': 0.362, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.816, 'pos': 0.184, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.874, 'pos': 0.126, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.819, 'pos': 0.181, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21888</th>\n",
       "      <td>{'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21889</th>\n",
       "      <td>{'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21890</th>\n",
       "      <td>{'neg': 0.074, 'neu': 0.719, 'pos': 0.206, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21915</th>\n",
       "      <td>{'neg': 0.043, 'neu': 0.774, 'pos': 0.183, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21962</th>\n",
       "      <td>{'neg': 0.196, 'neu': 0.777, 'pos': 0.028, 'co...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5398 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Vader Analysis Vader Sentiment\n",
       "0      {'neg': 0.0, 'neu': 0.638, 'pos': 0.362, 'comp...        Positive\n",
       "1      {'neg': 0.0, 'neu': 0.816, 'pos': 0.184, 'comp...        Positive\n",
       "2      {'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...        Positive\n",
       "3      {'neg': 0.0, 'neu': 0.874, 'pos': 0.126, 'comp...        Positive\n",
       "4      {'neg': 0.0, 'neu': 0.819, 'pos': 0.181, 'comp...        Positive\n",
       "...                                                  ...             ...\n",
       "21888  {'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...        Positive\n",
       "21889  {'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...        Positive\n",
       "21890  {'neg': 0.074, 'neu': 0.719, 'pos': 0.206, 'co...        Positive\n",
       "21915  {'neg': 0.043, 'neu': 0.774, 'pos': 0.183, 'co...        Positive\n",
       "21962  {'neg': 0.196, 'neu': 0.777, 'pos': 0.028, 'co...        Negative\n",
       "\n",
       "[5398 rows x 2 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset[['Vader Analysis', 'Vader Sentiment']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive    3335\n",
       "Neutral     1691\n",
       "Negative     372\n",
       "Name: Vader Sentiment, dtype: int64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset['Vader Sentiment'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**approach → first data cleaning + tokenization + lemmatization**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Non-English Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    b'Joined UAE Minister &amp; President-Designat...\n",
       "1    b'Foreign Secretary  participated in a meeting...\n",
       "2    b'As the UAE marks National Environment Day, w...\n",
       "3    b'RT  Did you know that the UAE is developing ...\n",
       "4    b'RT   UAE lunar rover will test 1st artificia...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nonenglish(tweet):\n",
    "    tweet = re.sub(r'\\\\x[a-zA-Z0-9]+', '', tweet) # tweets in different language changed to hex code \\xhh\n",
    "    tweet = re.sub(r'\\\\n', '', tweet) # removing new line character as well\n",
    "    return tweet\n",
    "\n",
    "updated_dataset['Cleaned Text'] = updated_dataset['Cleaned Text'].apply(lambda tweet: nonenglish(tweet))\n",
    "updated_dataset['Cleaned Text'].head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    b'Joined UAE Minister &amp; President-Designat...\n",
       "1    b'Foreign Secretary  participated meeting foca...\n",
       "2    b'As UAE marks National Environment Day, remai...\n",
       "3    b'RT  Did know UAE developing Arabic ChatGPT u...\n",
       "4    b'RT   UAE lunar rover test 1st artificial int...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "# contains 318 stopwords, including but not limited to articles and prepositions\n",
    "stopwords = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "updated_dataset['Cleaned Text'] = updated_dataset['Cleaned Text'].apply(lambda tweet: ' '.join([text for text in tweet.split(' ') if text not in stopwords]))\n",
    "updated_dataset['Cleaned Text'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Symbols and Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    bJoined UAE Minister amp PresidentDesignate  H...\n",
       "1    bForeign Secretary  participated meeting focal...\n",
       "2    bAs UAE marks National Environment Day remain ...\n",
       "3    bRT  Did know UAE developing Arabic ChatGPT us...\n",
       "4    bRT   UAE lunar rover test 1st artificial inte...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "punctuations = string.punctuation\n",
    "print(punctuations)\n",
    "\n",
    "updated_dataset['Cleaned Text'] = updated_dataset['Cleaned Text'].apply(lambda tweet: tweet.translate({ord(punc): None for punc in punctuations}))\n",
    "updated_dataset['Cleaned Text'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    bJoined UAE Minister amp PresidentDesignate  H...\n",
       "1    bForeign Secretary  participated meeting focal...\n",
       "2    bAs UAE marks National Environment Day remain ...\n",
       "3    bRT  Did know UAE developing Arabic ChatGPT us...\n",
       "4    bRT   UAE lunar rover test st artificial intel...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset['Cleaned Text'] = updated_dataset['Cleaned Text'].apply(lambda tweet: re.sub('[0-9]+', '', tweet))\n",
    "updated_dataset['Cleaned Text'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing 'b' and 'RT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Joined UAE Minister amp PresidentDesignate  HE...\n",
       "1    Foreign Secretary  participated meeting focal ...\n",
       "2    As UAE marks National Environment Day remain c...\n",
       "3      Did know UAE developing Araic ChatGPT using ...\n",
       "4       UAE lunar rover test st artificial intellig...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset['Cleaned Text'] = updated_dataset['Cleaned Text'].apply(lambda tweet: re.sub('bRT|b', '', tweet))\n",
    "updated_dataset['Cleaned Text'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Joined, UAE, Minister, amp, PresidentDesignat...\n",
       "1    [Foreign, Secretary, participated, meeting, fo...\n",
       "2    [As, UAE, marks, National, Environment, Day, r...\n",
       "3    [Did, know, UAE, developing, Araic, ChatGPT, u...\n",
       "4    [UAE, lunar, rover, test, st, artificial, inte...\n",
       "Name: Tokenized Text, dtype: object"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as data has been completely cleaned, we can simply tokenize by converting strings to lists\n",
    "updated_dataset['Tokenized Text'] = updated_dataset['Cleaned Text'].apply(lambda tweet: tweet.split())\n",
    "updated_dataset['Tokenized Text'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [joined, uae, minister, amp, presidentdesignat...\n",
       "1    [foreign, secretary, participated, meeting, fo...\n",
       "2    [as, uae, mark, national, environment, day, re...\n",
       "3    [did, know, uae, developing, araic, chatgpt, u...\n",
       "4    [uae, lunar, rover, test, st, artificial, inte...\n",
       "Name: Lemmatized Text, dtype: object"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "lemmatizing = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "updated_dataset['Lemmatized Text'] = updated_dataset['Tokenized Text'].apply(lambda tweet: [lemmatizing.lemmatize(text).lower() for text in tweet])\n",
    "updated_dataset['Lemmatized Text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode TextBlob Sentiments into numbers\n",
    "updated_dataset['Encoded_TextBlob_Sentiment'] = updated_dataset['TextBlob Sentiment'].apply(lambda x: 1 if x=='Positive' else -1 if x=='Negative' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dataset.reset_index(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Representation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section experiments with different document representations, namely:\n",
    "- Bag of Words\n",
    "- N-grams\n",
    "- TF-IDF\n",
    "- CBOW\n",
    "- Skip-gram\n",
    "- Pre-trained Word2Vec model by Google\n",
    "\n",
    "\n",
    "Each of the representations are testing using Naive Bayes, and the high performance models are used in the next stage of the pipeline.\n",
    "\n",
    "Note: Arbitrarily using lemmatization and textblob sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = updated_dataset['Lemmatized Text']\n",
    "y = updated_dataset['Encoded_TextBlob_Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    updated_dataset['Lemmatized Text'], \n",
    "    updated_dataset['Encoded_TextBlob_Sentiment'],\n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=updated_dataset['Encoded_TextBlob_Sentiment']\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bow = CountVectorizer(\n",
    "    ngram_range= (1,1), # unigram\n",
    "    preprocessor= lambda x: x, # override preprocessing\n",
    "    tokenizer= lambda x: x, # override tokenization\n",
    ")\n",
    "\n",
    "nb_bow = Pipeline([\n",
    "    ('bow', bow),\n",
    "    ('naive_bayes', MultinomialNB())\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.71      0.28      0.40        96\n",
      "           0       0.85      0.58      0.69       455\n",
      "           1       0.67      0.92      0.77       529\n",
      "\n",
      "    accuracy                           0.72      1080\n",
      "   macro avg       0.74      0.60      0.62      1080\n",
      "weighted avg       0.75      0.72      0.71      1080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_bow.fit(X_train, y_train)\n",
    "y_pred_bow = nb_bow.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_bow))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams\n",
    "How to find best n-gram number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.77      0.35      0.49        96\n",
      "           0       0.87      0.62      0.72       455\n",
      "           1       0.69      0.93      0.79       529\n",
      "\n",
      "    accuracy                           0.75      1080\n",
      "   macro avg       0.78      0.63      0.67      1080\n",
      "weighted avg       0.77      0.75      0.74      1080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngram = CountVectorizer(\n",
    "    ngram_range= (1,3), # unigram\n",
    "    preprocessor= lambda x: x, # override preprocessing\n",
    "    tokenizer= lambda x: x, # override tokenization\n",
    ")\n",
    "\n",
    "ngram_nb = Pipeline([\n",
    "    ('ngram', ngram),\n",
    "    ('naive_bayes', MultinomialNB())\n",
    "])\n",
    "\n",
    "ngram_nb.fit(X_train, y_train)\n",
    "ngram_nb_y_pred = ngram_nb.predict(X_test)\n",
    "print(classification_report(y_test, ngram_nb_y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.09      0.17        96\n",
      "           0       0.88      0.56      0.68       455\n",
      "           1       0.65      0.95      0.77       529\n",
      "\n",
      "    accuracy                           0.71      1080\n",
      "   macro avg       0.84      0.53      0.54      1080\n",
      "weighted avg       0.77      0.71      0.68      1080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    preprocessor= lambda x: x, # override preprocessing\n",
    "    tokenizer= lambda x: x, # override tokenization\n",
    ")\n",
    "\n",
    "tfidf_nb = Pipeline([\n",
    "    ('tfidf', tfidf),\n",
    "    ('naive_bayes', MultinomialNB())\n",
    "])\n",
    "\n",
    "tfidf_nb.fit(X_train, y_train)\n",
    "tfidf_nb_y_pred = tfidf_nb.predict(X_test)\n",
    "print(classification_report(y_test, tfidf_nb_y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible parameters to tweak is window, cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = Word2Vec(\n",
    "    sentences=X,\n",
    "    window=10, # how many context words before and after target word to consider\n",
    "    min_count=2,\n",
    "    workers=4, # number of threads\n",
    "    sg=0, # 0 to use cbow, 1 for skipgram\n",
    "    cbow_mean=0, # 0 uses sum of context word vectors, 1 uses mean\n",
    ")\n",
    "cbow.train(X, total_examples=cbow.corpus_count,epochs=cbow.epochs)\n",
    "# After training, KeyedVector object is used\n",
    "cbow_model = cbow.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def normalize_vectors(input_vectors):\n",
    "    \"\"\"Normalizes vectors to be in range [0,1)\n",
    "    \n",
    "    Args:\n",
    "        input_vectors (np.ndarray): List of vectors, each vector is np.ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    return MinMaxScaler().fit(input_vectors).transform(input_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence(sentence, wv_model):\n",
    "    \"\"\"Convert a sentence to a vector by summing word vectors in the sentence.\n",
    "\n",
    "    Another option could be to average word vectors rather than just sum.\n",
    "    \n",
    "    Arg: \n",
    "        sentence (list): Tokenized sentence\n",
    "    Returns:\n",
    "        (np.ndarray): Vector representation of sentence\n",
    "    \"\"\"\n",
    "    vector_size = wv_model.vector_size\n",
    "    wv_res = np.zeros(vector_size)\n",
    "    count = 0\n",
    "    for w in sentence:\n",
    "        if w in wv_model:\n",
    "            # count += 1\n",
    "            wv_res += wv_model[w]\n",
    "    # wv_res = wv_res/count\n",
    "    return wv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00        96\n",
      "           0       0.66      0.42      0.51       455\n",
      "           1       0.57      0.85      0.68       529\n",
      "\n",
      "    accuracy                           0.59      1080\n",
      "   macro avg       0.41      0.42      0.40      1080\n",
      "weighted avg       0.56      0.59      0.55      1080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the embeddings of train and test set\n",
    "X_train_cbow = normalize_vectors(np.array([vectorize_sentence(x,cbow_model) for x in X_train]))\n",
    "X_test_cbow = normalize_vectors(np.array([vectorize_sentence(x,cbow_model) for x in X_test]))\n",
    "\n",
    "#test NB\n",
    "nb_cbow = MultinomialNB()\n",
    "nb_cbow.fit(X_train_cbow, y_train)\n",
    "y_pred_cbow = nb_cbow.predict(X_test_cbow)\n",
    "print(classification_report(y_test, y_pred_cbow, zero_division=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram = Word2Vec(\n",
    "    sentences=X,\n",
    "    window=10,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    sg=1, # 0 to use cbow, 1 for skipgram\n",
    ")\n",
    "# Train the model\n",
    "skipgram.train(X, total_examples=skipgram.corpus_count,epochs=skipgram.epochs)\n",
    "\n",
    "# After training, KeyedVector object is used\n",
    "skipgram_model = skipgram.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00        96\n",
      "           0       0.64      0.48      0.55       455\n",
      "           1       0.57      0.80      0.67       529\n",
      "\n",
      "    accuracy                           0.59      1080\n",
      "   macro avg       0.40      0.43      0.41      1080\n",
      "weighted avg       0.55      0.59      0.56      1080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the embeddings of train and test set\n",
    "X_train_sg = normalize_vectors(np.array([vectorize_sentence(x,skipgram_model) for x in X_train]))\n",
    "X_test_sg = normalize_vectors(np.array([vectorize_sentence(x,skipgram_model) for x in X_test]))\n",
    "#test on NB\n",
    "nb_sg = MultinomialNB()\n",
    "nb_sg.fit(X_train_sg, y_train)\n",
    "y_pred_sg = nb_sg.predict(X_test_sg)\n",
    "print(classification_report(y_test, y_pred_sg, zero_division=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pre-trained Word2Vec Model\n",
    "Using pre-trained word2vec model from Google, which has 3 million words. Trained on 100 billion words from the google news dataset. Unsure if it uses CBOW or SkipGram or both\n",
    "\n",
    "From https://thinkingneuron.com/how-to-classify-text-using-word2vec/\n",
    "Google Model can be downloaded from https://thinkingneuron.com/how-to-classify-text-using-word2vec/#:~:text=download%20link%3A%20https%3A%2F%2Fdrive.google.com%2Ffile%2Fd%2F0b7xkcwpi5kdynlnuttlss21pqmm%2Fedit%3Fusp%3Dsharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/d/Andrea/Heriot-Watt/year-4/f20aa/f20aa-coursework-1/GoogleNews-vectors-negative300.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [155], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# load the word vectors from Google Model (1GB)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Will take time because 3 mil words. \u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Download model from https://thinkingneuron.com/how-to-classify-text-using-word2vec/#:~:text=download%20link%3A%20https%3A%2F%2Fdrive.google.com%2Ffile%2Fd%2F0b7xkcwpi5kdynlnuttlss21pqmm%2Fedit%3Fusp%3Dsharing\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m GoogleModel \u001b[39m=\u001b[39m gensim\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mKeyedVectors\u001b[39m.\u001b[39;49mload_word2vec_format(\u001b[39m'\u001b[39;49m\u001b[39m/mnt/d/Andrea/Heriot-Watt/year-4/f20aa/f20aa-coursework-1/GoogleNews-vectors-negative300.bin\u001b[39;49m\u001b[39m'\u001b[39;49m, binary\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,)\n",
      "File \u001b[1;32mc:\\Users\\bhavika\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\models\\keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1672\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m   1673\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_word2vec_format\u001b[39m(\n\u001b[0;32m   1674\u001b[0m         \u001b[39mcls\u001b[39m, fname, fvocab\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m, unicode_errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1675\u001b[0m         limit\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, datatype\u001b[39m=\u001b[39mREAL, no_header\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1676\u001b[0m     ):\n\u001b[0;32m   1677\u001b[0m     \u001b[39m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[0;32m   1678\u001b[0m \n\u001b[0;32m   1679\u001b[0m \u001b[39m    Warnings\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1717\u001b[0m \n\u001b[0;32m   1718\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1719\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_word2vec_format(\n\u001b[0;32m   1720\u001b[0m         \u001b[39mcls\u001b[39;49m, fname, fvocab\u001b[39m=\u001b[39;49mfvocab, binary\u001b[39m=\u001b[39;49mbinary, encoding\u001b[39m=\u001b[39;49mencoding, unicode_errors\u001b[39m=\u001b[39;49municode_errors,\n\u001b[0;32m   1721\u001b[0m         limit\u001b[39m=\u001b[39;49mlimit, datatype\u001b[39m=\u001b[39;49mdatatype, no_header\u001b[39m=\u001b[39;49mno_header,\n\u001b[0;32m   1722\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\bhavika\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\models\\keyedvectors.py:2048\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2045\u001b[0m             counts[word] \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(count)\n\u001b[0;32m   2047\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mloading projection weights from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, fname)\n\u001b[1;32m-> 2048\u001b[0m \u001b[39mwith\u001b[39;00m utils\u001b[39m.\u001b[39;49mopen(fname, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m fin:\n\u001b[0;32m   2049\u001b[0m     \u001b[39mif\u001b[39;00m no_header:\n\u001b[0;32m   2050\u001b[0m         \u001b[39m# deduce both vocab_size & vector_size from 1st pass over file\u001b[39;00m\n\u001b[0;32m   2051\u001b[0m         \u001b[39mif\u001b[39;00m binary:\n",
      "File \u001b[1;32mc:\\Users\\bhavika\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\smart_open\\smart_open_lib.py:177\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[39mif\u001b[39;00m transport_params \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     transport_params \u001b[39m=\u001b[39m {}\n\u001b[1;32m--> 177\u001b[0m fobj \u001b[39m=\u001b[39m _shortcut_open(\n\u001b[0;32m    178\u001b[0m     uri,\n\u001b[0;32m    179\u001b[0m     mode,\n\u001b[0;32m    180\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m    181\u001b[0m     buffering\u001b[39m=\u001b[39;49mbuffering,\n\u001b[0;32m    182\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m    183\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    184\u001b[0m     newline\u001b[39m=\u001b[39;49mnewline,\n\u001b[0;32m    185\u001b[0m )\n\u001b[0;32m    186\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[39mreturn\u001b[39;00m fobj\n",
      "File \u001b[1;32mc:\\Users\\bhavika\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\smart_open\\smart_open_lib.py:363\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[39mif\u001b[39;00m errors \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m    361\u001b[0m     open_kwargs[\u001b[39m'\u001b[39m\u001b[39merrors\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m errors\n\u001b[1;32m--> 363\u001b[0m \u001b[39mreturn\u001b[39;00m _builtin_open(local_path, mode, buffering\u001b[39m=\u001b[39mbuffering, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mopen_kwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/d/Andrea/Heriot-Watt/year-4/f20aa/f20aa-coursework-1/GoogleNews-vectors-negative300.bin'"
     ]
    }
   ],
   "source": [
    "# load the word vectors from Google Model (1GB)\n",
    "# Will take time because 3 mil words. \n",
    "# Download model from https://thinkingneuron.com/how-to-classify-text-using-word2vec/#:~:text=download%20link%3A%20https%3A%2F%2Fdrive.google.com%2Ffile%2Fd%2F0b7xkcwpi5kdynlnuttlss21pqmm%2Fedit%3Fusp%3Dsharing\n",
    "GoogleModel = gensim.models.KeyedVectors.load_word2vec_format('/mnt/d/Andrea/Heriot-Watt/year-4/f20aa/f20aa-coursework-1/GoogleNews-vectors-negative300.bin', binary=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence_2(sentences, wv_model):\n",
    "    \"\"\"Convert a list of sentences to a vectors.\n",
    "\n",
    "    By summing word vectors in the sentence.\n",
    "    Another option could be to average word vectors rather than just sum.\n",
    "    \n",
    "    Arg: \n",
    "        sentence (pd.Series): Series consisting of list of tokenized texts \n",
    "    Returns:\n",
    "        (np.ndarray): Vector representation of sentences\n",
    "    \"\"\"\n",
    "    vector_size = wv_model.vector_size\n",
    "    count = 0\n",
    "    vector_sentences = []\n",
    "\n",
    "    for i in range(len(sentences)): \n",
    "        sentence = np.zeros(vector_size)\n",
    "        for word in sentences[i]:\n",
    "            count+=1\n",
    "            if word in wv_model:\n",
    "                sentence += wv_model[word]\n",
    "        # sentence = sentence/count\n",
    "        vector_sentences.append(sentence)\n",
    "    vector_sentences_df = pd.DataFrame(vector_sentences)\n",
    "    \n",
    "    return vector_sentences_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert text data to W2V vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GoogleModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [157], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m w2v_data \u001b[39m=\u001b[39m vectorize_sentence_2(X, GoogleModel)\n\u001b[0;32m      2\u001b[0m w2v_data\u001b[39m.\u001b[39mreset_index(inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m w2v_data[\u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39my\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GoogleModel' is not defined"
     ]
    }
   ],
   "source": [
    "w2v_data = vectorize_sentence_2(X, GoogleModel)\n",
    "w2v_data.reset_index(inplace=True, drop=True)\n",
    "w2v_data['sentiment']=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.517578</td>\n",
       "      <td>2.149384</td>\n",
       "      <td>0.244385</td>\n",
       "      <td>1.057068</td>\n",
       "      <td>0.005615</td>\n",
       "      <td>-0.110962</td>\n",
       "      <td>-1.076660</td>\n",
       "      <td>-4.466125</td>\n",
       "      <td>2.398560</td>\n",
       "      <td>1.838623</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.183201</td>\n",
       "      <td>2.587189</td>\n",
       "      <td>-1.779480</td>\n",
       "      <td>1.421364</td>\n",
       "      <td>-3.537811</td>\n",
       "      <td>-1.348389</td>\n",
       "      <td>-1.448334</td>\n",
       "      <td>-0.404602</td>\n",
       "      <td>0.051758</td>\n",
       "      <td>4.679199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.202881</td>\n",
       "      <td>1.526184</td>\n",
       "      <td>1.248779</td>\n",
       "      <td>0.022171</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.535217</td>\n",
       "      <td>1.304497</td>\n",
       "      <td>-1.866699</td>\n",
       "      <td>1.610382</td>\n",
       "      <td>0.834839</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.435616</td>\n",
       "      <td>-0.429688</td>\n",
       "      <td>0.471130</td>\n",
       "      <td>0.044685</td>\n",
       "      <td>-0.072174</td>\n",
       "      <td>0.426514</td>\n",
       "      <td>1.126831</td>\n",
       "      <td>-0.328308</td>\n",
       "      <td>0.306915</td>\n",
       "      <td>1.507935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.591887</td>\n",
       "      <td>2.016541</td>\n",
       "      <td>1.312225</td>\n",
       "      <td>1.879883</td>\n",
       "      <td>-1.195282</td>\n",
       "      <td>-0.182129</td>\n",
       "      <td>2.341568</td>\n",
       "      <td>-2.139465</td>\n",
       "      <td>1.937492</td>\n",
       "      <td>1.381104</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.698547</td>\n",
       "      <td>2.054138</td>\n",
       "      <td>-2.196167</td>\n",
       "      <td>0.245865</td>\n",
       "      <td>-0.480743</td>\n",
       "      <td>0.292847</td>\n",
       "      <td>-0.172760</td>\n",
       "      <td>-0.098694</td>\n",
       "      <td>1.208496</td>\n",
       "      <td>0.037415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.187256</td>\n",
       "      <td>0.425644</td>\n",
       "      <td>0.694336</td>\n",
       "      <td>0.746582</td>\n",
       "      <td>-0.830971</td>\n",
       "      <td>0.188629</td>\n",
       "      <td>0.263794</td>\n",
       "      <td>-1.604248</td>\n",
       "      <td>0.192200</td>\n",
       "      <td>0.070190</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.465820</td>\n",
       "      <td>0.378479</td>\n",
       "      <td>-1.248840</td>\n",
       "      <td>0.507454</td>\n",
       "      <td>-0.283112</td>\n",
       "      <td>0.090332</td>\n",
       "      <td>0.951538</td>\n",
       "      <td>-0.135376</td>\n",
       "      <td>-0.143707</td>\n",
       "      <td>0.971436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.289551</td>\n",
       "      <td>0.470154</td>\n",
       "      <td>0.040894</td>\n",
       "      <td>1.148956</td>\n",
       "      <td>-0.361826</td>\n",
       "      <td>-0.663452</td>\n",
       "      <td>1.364746</td>\n",
       "      <td>-1.629761</td>\n",
       "      <td>0.779358</td>\n",
       "      <td>0.507080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206055</td>\n",
       "      <td>2.159424</td>\n",
       "      <td>-0.898987</td>\n",
       "      <td>1.384079</td>\n",
       "      <td>-0.363281</td>\n",
       "      <td>0.470703</td>\n",
       "      <td>-1.204590</td>\n",
       "      <td>-1.422424</td>\n",
       "      <td>-0.242188</td>\n",
       "      <td>-0.068848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5393</th>\n",
       "      <td>-0.779114</td>\n",
       "      <td>1.713898</td>\n",
       "      <td>0.965210</td>\n",
       "      <td>2.606506</td>\n",
       "      <td>-1.261993</td>\n",
       "      <td>0.792786</td>\n",
       "      <td>0.899628</td>\n",
       "      <td>0.057434</td>\n",
       "      <td>2.080322</td>\n",
       "      <td>1.345978</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.424438</td>\n",
       "      <td>-0.558105</td>\n",
       "      <td>-2.832397</td>\n",
       "      <td>-0.131470</td>\n",
       "      <td>1.480103</td>\n",
       "      <td>1.432678</td>\n",
       "      <td>-2.717224</td>\n",
       "      <td>-1.542297</td>\n",
       "      <td>0.498291</td>\n",
       "      <td>0.041809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5394</th>\n",
       "      <td>-0.779114</td>\n",
       "      <td>1.713898</td>\n",
       "      <td>0.965210</td>\n",
       "      <td>2.606506</td>\n",
       "      <td>-1.261993</td>\n",
       "      <td>0.792786</td>\n",
       "      <td>0.899628</td>\n",
       "      <td>0.057434</td>\n",
       "      <td>2.080322</td>\n",
       "      <td>1.345978</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.424438</td>\n",
       "      <td>-0.558105</td>\n",
       "      <td>-2.832397</td>\n",
       "      <td>-0.131470</td>\n",
       "      <td>1.480103</td>\n",
       "      <td>1.432678</td>\n",
       "      <td>-2.717224</td>\n",
       "      <td>-1.542297</td>\n",
       "      <td>0.498291</td>\n",
       "      <td>0.041809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5395</th>\n",
       "      <td>-0.779114</td>\n",
       "      <td>1.713898</td>\n",
       "      <td>0.965210</td>\n",
       "      <td>2.606506</td>\n",
       "      <td>-1.261993</td>\n",
       "      <td>0.792786</td>\n",
       "      <td>0.899628</td>\n",
       "      <td>0.057434</td>\n",
       "      <td>2.080322</td>\n",
       "      <td>1.345978</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.424438</td>\n",
       "      <td>-0.558105</td>\n",
       "      <td>-2.832397</td>\n",
       "      <td>-0.131470</td>\n",
       "      <td>1.480103</td>\n",
       "      <td>1.432678</td>\n",
       "      <td>-2.717224</td>\n",
       "      <td>-1.542297</td>\n",
       "      <td>0.498291</td>\n",
       "      <td>0.041809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5396</th>\n",
       "      <td>-0.170532</td>\n",
       "      <td>0.555901</td>\n",
       "      <td>0.524551</td>\n",
       "      <td>1.156067</td>\n",
       "      <td>-0.659790</td>\n",
       "      <td>0.719757</td>\n",
       "      <td>-0.405373</td>\n",
       "      <td>-0.833130</td>\n",
       "      <td>0.974854</td>\n",
       "      <td>0.811157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787354</td>\n",
       "      <td>1.457672</td>\n",
       "      <td>0.223541</td>\n",
       "      <td>0.326981</td>\n",
       "      <td>0.550018</td>\n",
       "      <td>-0.178589</td>\n",
       "      <td>0.676849</td>\n",
       "      <td>-0.140881</td>\n",
       "      <td>0.202820</td>\n",
       "      <td>-0.080078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5397</th>\n",
       "      <td>-0.910950</td>\n",
       "      <td>1.570068</td>\n",
       "      <td>0.804808</td>\n",
       "      <td>3.015259</td>\n",
       "      <td>-0.817932</td>\n",
       "      <td>-0.055786</td>\n",
       "      <td>-0.946167</td>\n",
       "      <td>-2.200195</td>\n",
       "      <td>0.376193</td>\n",
       "      <td>1.857975</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.392151</td>\n",
       "      <td>2.878784</td>\n",
       "      <td>-0.290070</td>\n",
       "      <td>1.094849</td>\n",
       "      <td>-1.044220</td>\n",
       "      <td>-1.327454</td>\n",
       "      <td>-2.159546</td>\n",
       "      <td>-0.651855</td>\n",
       "      <td>0.339622</td>\n",
       "      <td>1.169434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5398 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0    -1.517578  2.149384  0.244385  1.057068  0.005615 -0.110962 -1.076660   \n",
       "1    -2.202881  1.526184  1.248779  0.022171 -0.643616 -0.535217  1.304497   \n",
       "2    -1.591887  2.016541  1.312225  1.879883 -1.195282 -0.182129  2.341568   \n",
       "3    -0.187256  0.425644  0.694336  0.746582 -0.830971  0.188629  0.263794   \n",
       "4    -1.289551  0.470154  0.040894  1.148956 -0.361826 -0.663452  1.364746   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5393 -0.779114  1.713898  0.965210  2.606506 -1.261993  0.792786  0.899628   \n",
       "5394 -0.779114  1.713898  0.965210  2.606506 -1.261993  0.792786  0.899628   \n",
       "5395 -0.779114  1.713898  0.965210  2.606506 -1.261993  0.792786  0.899628   \n",
       "5396 -0.170532  0.555901  0.524551  1.156067 -0.659790  0.719757 -0.405373   \n",
       "5397 -0.910950  1.570068  0.804808  3.015259 -0.817932 -0.055786 -0.946167   \n",
       "\n",
       "           7         8         9    ...       290       291       292  \\\n",
       "0    -4.466125  2.398560  1.838623  ... -2.183201  2.587189 -1.779480   \n",
       "1    -1.866699  1.610382  0.834839  ... -2.435616 -0.429688  0.471130   \n",
       "2    -2.139465  1.937492  1.381104  ... -1.698547  2.054138 -2.196167   \n",
       "3    -1.604248  0.192200  0.070190  ... -0.465820  0.378479 -1.248840   \n",
       "4    -1.629761  0.779358  0.507080  ...  0.206055  2.159424 -0.898987   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5393  0.057434  2.080322  1.345978  ... -0.424438 -0.558105 -2.832397   \n",
       "5394  0.057434  2.080322  1.345978  ... -0.424438 -0.558105 -2.832397   \n",
       "5395  0.057434  2.080322  1.345978  ... -0.424438 -0.558105 -2.832397   \n",
       "5396 -0.833130  0.974854  0.811157  ...  0.787354  1.457672  0.223541   \n",
       "5397 -2.200195  0.376193  1.857975  ... -1.392151  2.878784 -0.290070   \n",
       "\n",
       "           293       294       295       296       297       298       299  \n",
       "0     1.421364 -3.537811 -1.348389 -1.448334 -0.404602  0.051758  4.679199  \n",
       "1     0.044685 -0.072174  0.426514  1.126831 -0.328308  0.306915  1.507935  \n",
       "2     0.245865 -0.480743  0.292847 -0.172760 -0.098694  1.208496  0.037415  \n",
       "3     0.507454 -0.283112  0.090332  0.951538 -0.135376 -0.143707  0.971436  \n",
       "4     1.384079 -0.363281  0.470703 -1.204590 -1.422424 -0.242188 -0.068848  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5393 -0.131470  1.480103  1.432678 -2.717224 -1.542297  0.498291  0.041809  \n",
       "5394 -0.131470  1.480103  1.432678 -2.717224 -1.542297  0.498291  0.041809  \n",
       "5395 -0.131470  1.480103  1.432678 -2.717224 -1.542297  0.498291  0.041809  \n",
       "5396  0.326981  0.550018 -0.178589  0.676849 -0.140881  0.202820 -0.080078  \n",
       "5397  1.094849 -1.044220 -1.327454 -2.159546 -0.651855  0.339622  1.169434  \n",
       "\n",
       "[5398 rows x 300 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and test values\n",
    "X_w2v = w2v_data.iloc[:,:-1].values\n",
    "y_w2v = w2v_data.iloc[:,-1].values\n",
    "\n",
    "X_w2v_scaled = normalize_vectors(X_w2v)\n",
    "\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(\n",
    "    X_w2v_scaled, y_w2v, test_size=0.2, random_state=42,\n",
    "    stratify= y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_w2v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [158], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m nb \u001b[39m=\u001b[39m MultinomialNB()\n\u001b[1;32m----> 2\u001b[0m nb\u001b[39m.\u001b[39mfit(X_train_w2v, y_train_w2v)\n\u001b[0;32m      3\u001b[0m y_pred_w2v \u001b[39m=\u001b[39m nb\u001b[39m.\u001b[39mpredict(X_test_w2v)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(classification_report(y_test_w2v, y_pred_w2v, zero_division\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_w2v' is not defined"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_w2v, y_train_w2v)\n",
    "y_pred_w2v = nb.predict(X_test_w2v)\n",
    "print(classification_report(y_test_w2v, y_pred_w2v, zero_division=0))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Text Representation Models\n",
    "The different models are compared based on their weighted precision, recall, and f1 score.\n",
    "Values that are closer to one are optimal.\n",
    "\n",
    "|Model|Precision|Recall|F-1|\n",
    "|---|---|---|---|\n",
    "|Bag Of Words (Unigram)|0.75|0.73|0.71|\n",
    "|Trigrams|0.77|0.75|0.73|\n",
    "|TF-IDF|0.77|0.71|0.68|\n",
    "|CBOW (Trained on scraped tweets corpus)|0.58|0.57|0.5|\n",
    "|Skip-gram (Trained on scraped tweets corpus)|0.6|0.60|0.54|\n",
    "|Word2Vec model (Trained on Google News Corpus)|0.59|0.65|0.62|\n",
    "\n",
    "\n",
    "It was expected that the more sophisticated models - those which take into account the context of words - would perform better. However, based on the results the more primitive approaches performed better. This could be due to our small dataset.\n",
    "\n",
    "The tri-gram model was the best performer in terms of F-1 score, which is the harmonic mean of the precision and recall values.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression with Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.80      0.38      0.51        96\n",
      "           0       0.77      0.86      0.81       455\n",
      "           1       0.82      0.81      0.82       529\n",
      "\n",
      "    accuracy                           0.79      1080\n",
      "   macro avg       0.80      0.68      0.71      1080\n",
      "weighted avg       0.80      0.79      0.79      1080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "ngram = CountVectorizer(\n",
    "    ngram_range= (1, 3), # ngram\n",
    "    preprocessor= lambda x: x, # override preprocessing\n",
    "    tokenizer= lambda x: x, # override tokenization\n",
    ")\n",
    "\n",
    "ngram_lr = Pipeline([\n",
    "    ('ngram', ngram),\n",
    "    ('logistic_regression', LogisticRegression())\n",
    "])\n",
    "\n",
    "ngram_lr.fit(X_train, y_train)\n",
    "ngram_lr_y_pred = ngram_lr.predict(X_test)\n",
    "print(classification_report(y_test, ngram_lr_y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "48e2f1a5c55ea795dcf139e8514e387d8556933def71ce5b25b026d48c40902d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

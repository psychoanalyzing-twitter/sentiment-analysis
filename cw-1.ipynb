{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Distribution\n",
    "| Member | ID | Tasks |\n",
    "|---|---|---|\n",
    "|Bhavika| - | - | \n",
    "|Alora| - | - |\n",
    "|Andrea| - | - |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.12.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tweepy) (2.28.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keys for authentication\n",
    "consumer_key = \"ONLQ0vLSAXM2tZDnckominEcK\"\n",
    "consumer_secret = \"MpP7XurdGn1wpufX3rrfQgrs4AROnaw9ZiiZzKN6exr3zEDlN6\"\n",
    "access_token = \"1274112117738737664-J2DjjoqqzN11CNp8bpnYzVp1dUYKM4\"\n",
    "access_token_secret = \"3coYFJVCqHnOmD2m9Qt6zs7j1I24lhlPTU71eUOq4Zqg9\"\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAALJslQEAAAAA7EWdkTwrVDM44LIuewNascHjvoY%3DeXmhQyuXgo8zXJiSTMfqtmPA393HHO2W7nSrk0bK6b67SuJeb0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate\n",
    "auth = tweepy.OAuth1UserHandler(\n",
    "    consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tweets(query: str, amount: int) -> list:\n",
    "    \"\"\"Scrapes a specified number of tweets based on given query and location.\n",
    "\n",
    "    Method from https://www.sahilfruitwala.com/guide-to-extract-tweets-using-tweepy#how-to-retrieve-specific-number-of-tweets-using-tweepy \n",
    "    \n",
    "    Args:\n",
    "        query (str): The query\n",
    "    Returns:\n",
    "        list (str): List of contents of tweets.\n",
    "    \"\"\"\n",
    "    extracted_tweets = []\n",
    "    for tweet in tweepy.Cursor(api.search_tweets, query, count=100, tweet_mode=\"extended\", result_type=\"recent\").items(amount):\n",
    "        extracted_tweets.append(tweet.full_text)\n",
    "    return extracted_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(tweets: list, csv_name: str) -> None:\n",
    "    \"\"\"Appends the list of tweets to csv.\n",
    "    \n",
    "    Largely copied from https://gist.github.com/anku255/0cebd75cce675f2b56de1ef48ec06575.\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Writing {0} tweets to {1}\".format(len(tweets), csv_name))\n",
    "    tweets_for_csv = [[tweet.encode(\"utf-8\")] for tweet in tweets]\n",
    "    with open(csv_name, \"a+\") as file:\n",
    "        writer = csv.writer(file, delimiter=\",\")\n",
    "        writer.writerows(tweets_for_csv)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The queries for scraping tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "queries = [\n",
    "    [\"(metaverse OR meta verse OR #metaverse OR #meta #verse) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(innovation OR #innovation OR innovate OR innovative OR #innovate OR #innovative) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(sustainability OR #sustainability OR sustainable) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(technology footprint OR technology OR #technologyfootprint) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(artificial intelligence OR ai OR #ai OR #artificialintelligence) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(ethics OR #ethics OR ethical OR #ethical) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(inflation OR #inflation) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cop 28 OR cop28 OR #cop28 OR #cop #28) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(museum of the future OR @museumofthefuture OR #museumofthefuture OR #museum #of #the #future) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(bit coin OR bitcoin OR #bitcoin OR #crypto OR crpyto OR cryptocurrency) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cyber attack OR #cyberattack OR #cyber OR #cyberattacks OR #cyber #attack) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(robot OR robots OR #robots OR #robot) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(drone OR drones OR #drone OR #drones) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(hacker OR hacking OR #hacker OR #hacking OR #hack) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(chatgpt OR #chatgpt OR #chat #gpt OR chat gpt) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cybersecurity OR #cybersecurityOR #cybersec OR cybersec OR cyber security OR #cyber #security) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(sustainabletech OR sustainable ai OR sustainable technology OR #sustainabletech OR #sustainableai OR #sustainabletechnology) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(computing OR quantum computing OR #quantum #computing OR #computing) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(automation OR #automation) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(distributed cloud OR #distributedcloud OR #distributed #cloud) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(big data OR #big #data OR bigdata OR #bigdata) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(ar OR #ar OR #augmentedreality OR #augmented #reality) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(data mining OR #data #mining OR #datamining) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(software OR #software) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(tech OR technology OR #tech OR #technology) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(digital OR digital transformation OR #digital #transformation OR #digitaltransformation) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(blockchain OR #blockchain) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(coders_hq OR coders hq OR #coders OR #coders #hq OR @coders_hq) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(nft OR non-fungible token OR non fungible token OR nfts OR #nfts OR #nonfungibletoken OR #nft) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(binance OR @binance OR #binance) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(uae hackathon OR hackathon OR #hackathon) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(internet of things OR IoT OR #iot OR #internet #of #things OR #internetofthings) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(software engineering OR #softwareengineering) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(#coding OR coding) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(@uaeai OR #uaeai Or #uae #ai) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(technology OR tech OT #tech Or #technology) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(nft OR #nft OR #nfts OR nfts) AND (place_country:AE OR uae) lang:en -filter:retweets\"]   ,\n",
    "    [\"(UAE codes OR #UAE_codes OR #UAEcodes) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(Microsoft Hololens OR Hololens OR #Hololens) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(GITEX or #gitex) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(information OR #information) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(internet OR #internet) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(computer OR science OR computer science OR #computerscience OR #computer OR #science) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(technological OR #technological) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(engineering OR #engineering) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(computing OR #computing) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(systems OR #systems) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(electronics OR #electronics) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(application OR #application OR app OR #app) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(robotics OR #robotics) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(high tech OR high technology OR #hightech OR #hightechnology OR #high #technology OR #high #tech) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(nanotech OR nano tech OR nanotechnology OR nano technology OR #nanotech OR #nano #tech OR #nano #technology OR #nanotechnology) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(biotech OR bio tech OR biotechnology OR bio technology OR #biotech OR #bio #tech OR #bio #technology OR #biotechnology) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(code OR #code) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(automation OR #automation OR automate OR #automate) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(virtual OR #virtual OR online OR #online) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(technological advancements OR #technological #advancements OR #advancements OR #techadvancements) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(bionics OR #bionics) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cyberscience OR #cyberscience) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(google OR #google OR @google) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(oracle OR #oracle OR @oracle) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(ibm OR #ibm OR @ibm) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(bionics OR #bionics) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(sap OR #sap) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(global knowledge OR #globalknowledge OR #global #knowledge) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(amazon OR #amazon OR @amazon) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(hewlett-packard OR Hewlett Packard OR HP OR #hewlett-packard OR #Hewlett #Packard OR #HP) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cisco OR #cisco OR @cisco) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(dell OR #dell OR @dell) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cisco OR #cisco OR @cisco) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(@WorldGovSummit OR #WGS OR #WGS2023 OR World Goverment Summit) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(masdar OR #masdar OR @masdar) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(renewable energy OR clean energy OR #renewableenergy OR #cleanenergy) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(machine learning OR #machinelearning OR ML OR #ML) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(recommender systems OR #RS OR #recommender #systems OR recommender engine) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(deep learning OR #deeplearning) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(research OR #research) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commented out below to prevent re-running code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Scraping tweets and writing them to dataset.csv\"\"\"\n",
    "# csv_name=\"dataset.csv\"\n",
    "# total = 0\n",
    "# for query in queries:\n",
    "#     tweets = scrape_tweets(query=query, amount=500)\n",
    "#     total += len(tweets)\n",
    "#     print(\"Found {0} tweets related to the query {1}\".format(len(tweets), query))\n",
    "#     write_to_csv(tweets=tweets,csv_name=csv_name)\n",
    "# print(\"Done. {0} rows.\".format(total))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section cleans the scraped tweets dataset of duplicate tweets, URLs, @'s and #s."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "- Do not modify original csv `dataset.csv`. The dataset with no duplicates is stored in `updated_dataset.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates\n",
    "dataset = pd.read_csv(\"dataset.csv\", header=None)\n",
    "dataset.rename(columns={0: 'Text'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'Joined UAE Minister &amp;amp; President-Designat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Foreign Secretary @AmbVMKwatra participated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'As the UAE marks National Environment Day, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'RT @KhalifaAlgaz: Did you know that the UAE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'RT @tawhidChtioui: \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21974</th>\n",
       "      <td>b'Send our special gifts to your loved ones!\\x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21975</th>\n",
       "      <td>b'Trainer code: 2735 9457 4031\\nfrom \\xf0\\x9f\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21976</th>\n",
       "      <td>b\"We are looking for ICY SNOW , SANDSTORM .\\nI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21977</th>\n",
       "      <td>b'Make #MONEY from #home through PC #APP \\nMak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21978</th>\n",
       "      <td>b'#Earn Money from #home through PC #APP\\nwith...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21979 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text\n",
       "0      b'Joined UAE Minister &amp; President-Designat...\n",
       "1      b'Foreign Secretary @AmbVMKwatra participated ...\n",
       "2      b'As the UAE marks National Environment Day, w...\n",
       "3      b'RT @KhalifaAlgaz: Did you know that the UAE ...\n",
       "4      b'RT @tawhidChtioui: \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\...\n",
       "...                                                  ...\n",
       "21974  b'Send our special gifts to your loved ones!\\x...\n",
       "21975  b'Trainer code: 2735 9457 4031\\nfrom \\xf0\\x9f\\...\n",
       "21976  b\"We are looking for ICY SNOW , SANDSTORM .\\nI...\n",
       "21977  b'Make #MONEY from #home through PC #APP \\nMak...\n",
       "21978  b'#Earn Money from #home through PC #APP\\nwith...\n",
       "\n",
       "[21979 rows x 1 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'Joined UAE Minister &amp;amp; President-Designat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Foreign Secretary @AmbVMKwatra participated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'As the UAE marks National Environment Day, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'RT @KhalifaAlgaz: Did you know that the UAE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'RT @tawhidChtioui: \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21888</th>\n",
       "      <td>b'@m_ut67 @modgovae @Forsan_UAE Dear sir pleas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21889</th>\n",
       "      <td>b'@UAE_BARQ Dear sir please my help you sir my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21890</th>\n",
       "      <td>b'@UAE_BARQ Dear sir please my help you sir my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21915</th>\n",
       "      <td>b\"#PhonePe launched a service on Tuesday that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21962</th>\n",
       "      <td>b'@WeLoveIndia7 Saudi k log bhikhari h ? \\nIn ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5878 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text\n",
       "0      b'Joined UAE Minister &amp; President-Designat...\n",
       "1      b'Foreign Secretary @AmbVMKwatra participated ...\n",
       "2      b'As the UAE marks National Environment Day, w...\n",
       "3      b'RT @KhalifaAlgaz: Did you know that the UAE ...\n",
       "4      b'RT @tawhidChtioui: \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\...\n",
       "...                                                  ...\n",
       "21888  b'@m_ut67 @modgovae @Forsan_UAE Dear sir pleas...\n",
       "21889  b'@UAE_BARQ Dear sir please my help you sir my...\n",
       "21890  b'@UAE_BARQ Dear sir please my help you sir my...\n",
       "21915  b\"#PhonePe launched a service on Tuesday that ...\n",
       "21962  b'@WeLoveIndia7 Saudi k log bhikhari h ? \\nIn ...\n",
       "\n",
       "[5878 rows x 1 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset = dataset.drop_duplicates().copy()\n",
    "updated_dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing URLs, @'s and hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = updated_dataset['Text'].copy()\n",
    "updated_dataset['Cleaned Text'] = tweets.str.replace(r'@[^\\s]+|#[^\\s]+|https?:\\/\\/\\S+|www\\.\\S+', '',regex=True)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Duplicates again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5398, 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset = updated_dataset.drop('Text', axis=1)\n",
    "updated_dataset = updated_dataset.drop_duplicates()\n",
    "updated_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dataset.to_csv('updated-dataset.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Labelling\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data labelling, we decided to use first use TextBlob. TextBlob calculates the subjectivity and polarity of a text to classify the text as 'Positive', 'Negative' or 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: TextBlob in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from TextBlob) (3.8.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.1->TextBlob) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.1->TextBlob) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.1->TextBlob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.1->TextBlob) (2022.10.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk>=3.1->TextBlob) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get subjectivity\n",
    "def getSubjectivity(text):\n",
    "   return TextBlob(text).sentiment.subjectivity\n",
    "  \n",
    " \n",
    "#Function to get the polarity\n",
    "def getPolarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "   \n",
    "#Create two new columns ‘Subjectivity’ & ‘Polarity’\n",
    "updated_dataset['TextBlob Subjectivity'] =    updated_dataset['Cleaned Text'].apply(getSubjectivity)\n",
    "updated_dataset['TextBlob Polarity'] = updated_dataset['Cleaned Text'].apply(getPolarity)\n",
    "def getAnalysis(score):\n",
    "  if score < 0:\n",
    "    return 'Negative'\n",
    "  elif score == 0:\n",
    "    return 'Neutral'\n",
    "  else:\n",
    "    return 'Positive'\n",
    "\n",
    "updated_dataset['TextBlob Sentiment'] =  updated_dataset['TextBlob Polarity'].apply(getAnalysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive    2643\n",
       "Neutral     2276\n",
       "Negative     479\n",
       "Name: TextBlob Sentiment, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset['TextBlob Sentiment'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that TextBlob classified the majority of tweets to be Positive. To compare, we also decided to use Vader as well to observe if there was a major difference in classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaderSentiment) (2.28.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->vaderSentiment) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->vaderSentiment) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->vaderSentiment) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->vaderSentiment) (2022.6.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "def getSentiment(text):\n",
    "    return sentiment.polarity_scores(text)\n",
    "    \n",
    "updated_dataset['Vader Analysis'] = updated_dataset['Cleaned Text'].apply(getSentiment)\n",
    "\n",
    "def sentimentAnalysis(sentiment_dict):\n",
    "    if sentiment_dict['compound'] >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif sentiment_dict['compound'] <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "updated_dataset['Vader Sentiment'] = updated_dataset['Vader Analysis'].apply(sentimentAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vader Analysis</th>\n",
       "      <th>Vader Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.638, 'pos': 0.362, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.816, 'pos': 0.184, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.874, 'pos': 0.126, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.819, 'pos': 0.181, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21888</th>\n",
       "      <td>{'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21889</th>\n",
       "      <td>{'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21890</th>\n",
       "      <td>{'neg': 0.074, 'neu': 0.719, 'pos': 0.206, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21915</th>\n",
       "      <td>{'neg': 0.043, 'neu': 0.774, 'pos': 0.183, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21962</th>\n",
       "      <td>{'neg': 0.196, 'neu': 0.777, 'pos': 0.028, 'co...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5398 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Vader Analysis Vader Sentiment\n",
       "0      {'neg': 0.0, 'neu': 0.638, 'pos': 0.362, 'comp...        Positive\n",
       "1      {'neg': 0.0, 'neu': 0.816, 'pos': 0.184, 'comp...        Positive\n",
       "2      {'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...        Positive\n",
       "3      {'neg': 0.0, 'neu': 0.874, 'pos': 0.126, 'comp...        Positive\n",
       "4      {'neg': 0.0, 'neu': 0.819, 'pos': 0.181, 'comp...        Positive\n",
       "...                                                  ...             ...\n",
       "21888  {'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...        Positive\n",
       "21889  {'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...        Positive\n",
       "21890  {'neg': 0.074, 'neu': 0.719, 'pos': 0.206, 'co...        Positive\n",
       "21915  {'neg': 0.043, 'neu': 0.774, 'pos': 0.183, 'co...        Positive\n",
       "21962  {'neg': 0.196, 'neu': 0.777, 'pos': 0.028, 'co...        Negative\n",
       "\n",
       "[5398 rows x 2 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset[['Vader Analysis', 'Vader Sentiment']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive    3335\n",
       "Neutral     1691\n",
       "Negative     372\n",
       "Name: Vader Sentiment, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset['Vader Sentiment'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**changed approach → first data cleaning + tokenization then stemming OR lemmatization. (comparison)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Non-English Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    b'Joined UAE Minister &amp; President-Designat...\n",
       "1    b'Foreign Secretary  participated in a meeting...\n",
       "2    b'As the UAE marks National Environment Day, w...\n",
       "3    b'RT  Did you know that the UAE is developing ...\n",
       "4    b'RT   UAE lunar rover will test 1st artificia...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nonenglish(tweet):\n",
    "    tweet = re.sub(r'\\\\x[a-zA-Z0-9]+', '', tweet) # tweets in different language changed to hex code \\xhh\n",
    "    tweet = re.sub(r'\\\\n', '', tweet) # removing new line character as well\n",
    "    return tweet\n",
    "\n",
    "updated_dataset['Cleaned Text'] = updated_dataset['Cleaned Text'].apply(lambda tweet: nonenglish(tweet))\n",
    "updated_dataset['Cleaned Text'].head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    b'Joined UAE Minister &amp; President-Designat...\n",
       "1    b'Foreign Secretary  participated meeting foca...\n",
       "2    b'As UAE marks National Environment Day, remai...\n",
       "3    b'RT  Did know UAE developing Arabic ChatGPT u...\n",
       "4    b'RT   UAE lunar rover test 1st artificial int...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "# contains 318 stopwords, including and not limited to articles and prepositions\n",
    "stopwords = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "updated_dataset['Cleaned Text'] = updated_dataset['Cleaned Text'].apply(lambda tweet: ' '.join([text for text in tweet.split(' ') if text not in stopwords]))\n",
    "updated_dataset['Cleaned Text'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Symbols and Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    bJoined UAE Minister amp PresidentDesignate  H...\n",
       "1    bForeign Secretary  participated meeting focal...\n",
       "2    bAs UAE marks National Environment Day remain ...\n",
       "3    bRT  Did know UAE developing Arabic ChatGPT us...\n",
       "4    bRT   UAE lunar rover test 1st artificial inte...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "punctuations = string.punctuation\n",
    "print(punctuations)\n",
    "\n",
    "updated_dataset['Cleaned Text'] = updated_dataset['Cleaned Text'].apply(lambda tweet: tweet.translate({ord(punc): None for punc in punctuations}))\n",
    "updated_dataset['Cleaned Text'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    bJoined UAE Minister amp PresidentDesignate  H...\n",
       "1    bForeign Secretary  participated meeting focal...\n",
       "2    bAs UAE marks National Environment Day remain ...\n",
       "3    bRT  Did know UAE developing Arabic ChatGPT us...\n",
       "4    bRT   UAE lunar rover test st artificial intel...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset['Cleaned Text'] = updated_dataset['Cleaned Text'].apply(lambda tweet: re.sub('[0-9]+', '', tweet))\n",
    "updated_dataset['Cleaned Text'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing 'b' and 'RT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Joined UAE Minister amp PresidentDesignate  HE...\n",
       "1    Foreign Secretary  participated meeting focal ...\n",
       "2    As UAE marks National Environment Day remain c...\n",
       "3      Did know UAE developing Araic ChatGPT using ...\n",
       "4       UAE lunar rover test st artificial intellig...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset['Cleaned Text'] = updated_dataset['Cleaned Text'].apply(lambda tweet: re.sub('bRT|b', '', tweet))\n",
    "updated_dataset['Cleaned Text'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Joined, UAE, Minister, amp, PresidentDesignat...\n",
       "1    [Foreign, Secretary, participated, meeting, fo...\n",
       "2    [As, UAE, marks, National, Environment, Day, r...\n",
       "3    [Did, know, UAE, developing, Araic, ChatGPT, u...\n",
       "4    [UAE, lunar, rover, test, st, artificial, inte...\n",
       "Name: Tokenized Text, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as data has been completely cleaned, we can simply tokenize by converting strings to list\n",
    "updated_dataset['Tokenized Text'] = updated_dataset['Cleaned Text'].apply(lambda tweet: tweet.split())\n",
    "updated_dataset['Tokenized Text'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bhavika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [join, uae, minist, amp, presidentdesign, he, ...\n",
       "1    [foreign, secretari, particip, meet, focal, po...\n",
       "2    [as, uae, mark, nation, environ, day, remain, ...\n",
       "3    [did, know, uae, develop, araic, chatgpt, use,...\n",
       "4    [uae, lunar, rover, test, st, artifici, intell...\n",
       "Name: Stemmed Text, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming = nltk.stem.PorterStemmer()\n",
    "\n",
    "updated_dataset['Stemmed Text'] = updated_dataset['Tokenized Text'].apply(lambda tweet: [stemming.stem(text) for text in tweet])\n",
    "updated_dataset['Stemmed Text'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Joined, UAE, Minister, amp, PresidentDesignat...\n",
       "1    [Foreign, Secretary, participated, meeting, fo...\n",
       "2    [As, UAE, mark, National, Environment, Day, re...\n",
       "3    [Did, know, UAE, developing, Araic, ChatGPT, u...\n",
       "4    [UAE, lunar, rover, test, st, artificial, inte...\n",
       "Name: Lemmatized Text, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizing = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "updated_dataset['Lemmatized Text'] = updated_dataset['Tokenized Text'].apply(lambda tweet: [lemmatizing.lemmatize(text) for text in tweet])\n",
    "updated_dataset['Lemmatized Text'].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "48e2f1a5c55ea795dcf139e8514e387d8556933def71ce5b25b026d48c40902d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

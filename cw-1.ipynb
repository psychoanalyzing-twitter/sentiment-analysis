{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Distribution\n",
    "| Member | ID | Tasks |\n",
    "|---|---|---|\n",
    "|Bhavika| - | - | \n",
    "|Alora| - | - |\n",
    "|Andrea| - | - |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.12.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tweepy) (2.28.0)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\bhavika\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (1.26.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keys for authentication\n",
    "consumer_key = \"ONLQ0vLSAXM2tZDnckominEcK\"\n",
    "consumer_secret = \"MpP7XurdGn1wpufX3rrfQgrs4AROnaw9ZiiZzKN6exr3zEDlN6\"\n",
    "access_token = \"1274112117738737664-J2DjjoqqzN11CNp8bpnYzVp1dUYKM4\"\n",
    "access_token_secret = \"3coYFJVCqHnOmD2m9Qt6zs7j1I24lhlPTU71eUOq4Zqg9\"\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAALJslQEAAAAA7EWdkTwrVDM44LIuewNascHjvoY%3DeXmhQyuXgo8zXJiSTMfqtmPA393HHO2W7nSrk0bK6b67SuJeb0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate\n",
    "auth = tweepy.OAuth1UserHandler(\n",
    "    consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tweets(query: str, amount: int) -> list:\n",
    "    \"\"\"Scrapes a specified number of tweets based on given query and location.\n",
    "\n",
    "    Method from https://www.sahilfruitwala.com/guide-to-extract-tweets-using-tweepy#how-to-retrieve-specific-number-of-tweets-using-tweepy \n",
    "    \n",
    "    Args:\n",
    "        query (str): The query\n",
    "    Returns:\n",
    "        list (str): List of contents of tweets.\n",
    "    \"\"\"\n",
    "    extracted_tweets = []\n",
    "    for tweet in tweepy.Cursor(api.search_tweets, query, count=100, tweet_mode=\"extended\", result_type=\"recent\").items(amount):\n",
    "        extracted_tweets.append(tweet.full_text)\n",
    "    return extracted_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(tweets: list, csv_name: str) -> None:\n",
    "    \"\"\"Appends the list of tweets to csv.\n",
    "    \n",
    "    Largely copied from https://gist.github.com/anku255/0cebd75cce675f2b56de1ef48ec06575.\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Writing {0} tweets to {1}\".format(len(tweets), csv_name))\n",
    "    tweets_for_csv = [[tweet.encode(\"utf-8\")] for tweet in tweets]\n",
    "    with open(csv_name, \"a+\") as file:\n",
    "        writer = csv.writer(file, delimiter=\",\")\n",
    "        writer.writerows(tweets_for_csv)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The queries for scraping tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "queries = [\n",
    "    [\"(metaverse OR meta verse OR #metaverse OR #meta #verse) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(innovation OR #innovation OR innovate OR innovative OR #innovate OR #innovative) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(sustainability OR #sustainability OR sustainable) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(technology footprint OR technology OR #technologyfootprint) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(artificial intelligence OR ai OR #ai OR #artificialintelligence) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(ethics OR #ethics OR ethical OR #ethical) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(inflation OR #inflation) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cop 28 OR cop28 OR #cop28 OR #cop #28) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(museum of the future OR @museumofthefuture OR #museumofthefuture OR #museum #of #the #future) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(bit coin OR bitcoin OR #bitcoin OR #crypto OR crpyto OR cryptocurrency) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cyber attack OR #cyberattack OR #cyber OR #cyberattacks OR #cyber #attack) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(robot OR robots OR #robots OR #robot) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(drone OR drones OR #drone OR #drones) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(hacker OR hacking OR #hacker OR #hacking OR #hack) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(chatgpt OR #chatgpt OR #chat #gpt OR chat gpt) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cybersecurity OR #cybersecurityOR #cybersec OR cybersec OR cyber security OR #cyber #security) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(sustainabletech OR sustainable ai OR sustainable technology OR #sustainabletech OR #sustainableai OR #sustainabletechnology) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(computing OR quantum computing OR #quantum #computing OR #computing) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(automation OR #automation) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(distributed cloud OR #distributedcloud OR #distributed #cloud) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(big data OR #big #data OR bigdata OR #bigdata) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(ar OR #ar OR #augmentedreality OR #augmented #reality) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(data mining OR #data #mining OR #datamining) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(software OR #software) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(tech OR technology OR #tech OR #technology) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(digital OR digital transformation OR #digital #transformation OR #digitaltransformation) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(blockchain OR #blockchain) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(coders_hq OR coders hq OR #coders OR #coders #hq OR @coders_hq) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(nft OR non-fungible token OR non fungible token OR nfts OR #nfts OR #nonfungibletoken OR #nft) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(binance OR @binance OR #binance) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(uae hackathon OR hackathon OR #hackathon) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(internet of things OR IoT OR #iot OR #internet #of #things OR #internetofthings) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(software engineering OR #softwareengineering) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(#coding OR coding) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(@uaeai OR #uaeai Or #uae #ai) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(technology OR tech OT #tech Or #technology) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(nft OR #nft OR #nfts OR nfts) AND (place_country:AE OR uae) lang:en -filter:retweets\"]   ,\n",
    "    [\"(UAE codes OR #UAE_codes OR #UAEcodes) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(Microsoft Hololens OR Hololens OR #Hololens) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(GITEX or #gitex) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(information OR #information) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(internet OR #internet) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(computer OR science OR computer science OR #computerscience OR #computer OR #science) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(technological OR #technological) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(engineering OR #engineering) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(computing OR #computing) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(systems OR #systems) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(electronics OR #electronics) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(application OR #application OR app OR #app) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(robotics OR #robotics) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(high tech OR high technology OR #hightech OR #hightechnology OR #high #technology OR #high #tech) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(nanotech OR nano tech OR nanotechnology OR nano technology OR #nanotech OR #nano #tech OR #nano #technology OR #nanotechnology) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(biotech OR bio tech OR biotechnology OR bio technology OR #biotech OR #bio #tech OR #bio #technology OR #biotechnology) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(code OR #code) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(automation OR #automation OR automate OR #automate) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(virtual OR #virtual OR online OR #online) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(technological advancements OR #technological #advancements OR #advancements OR #techadvancements) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(bionics OR #bionics) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cyberscience OR #cyberscience) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(google OR #google OR @google) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(oracle OR #oracle OR @oracle) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(ibm OR #ibm OR @ibm) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(bionics OR #bionics) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(sap OR #sap) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(global knowledge OR #globalknowledge OR #global #knowledge) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(amazon OR #amazon OR @amazon) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(hewlett-packard OR Hewlett Packard OR HP OR #hewlett-packard OR #Hewlett #Packard OR #HP) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cisco OR #cisco OR @cisco) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(dell OR #dell OR @dell) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cisco OR #cisco OR @cisco) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(@WorldGovSummit OR #WGS OR #WGS2023 OR World Goverment Summit) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(masdar OR #masdar OR @masdar) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(renewable energy OR clean energy OR #renewableenergy OR #cleanenergy) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(machine learning OR #machinelearning OR ML OR #ML) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(recommender systems OR #RS OR #recommender #systems OR recommender engine) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(deep learning OR #deeplearning) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(research OR #research) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commented out below to prevent re-running code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Scraping tweets and writing them to dataset.csv\"\"\"\n",
    "# csv_name=\"dataset.csv\"\n",
    "# total = 0\n",
    "# for query in queries:\n",
    "#     tweets = scrape_tweets(query=query, amount=500)\n",
    "#     total += len(tweets)\n",
    "#     print(\"Found {0} tweets related to the query {1}\".format(len(tweets), query))\n",
    "#     write_to_csv(tweets=tweets,csv_name=csv_name)\n",
    "# print(\"Done. {0} rows.\".format(total))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section cleans the scraped tweets dataset of duplicate tweets, URLs, @'s and #s."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "- Do not modify original csv `dataset.csv`. The dataset with no duplicates is stored in `updated_dataset.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates\n",
    "dataset = pd.read_csv(\"dataset.csv\", header=None)\n",
    "dataset.rename(columns={0: 'Text'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'Joined UAE Minister &amp;amp; President-Designat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Foreign Secretary @AmbVMKwatra participated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'As the UAE marks National Environment Day, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'RT @KhalifaAlgaz: Did you know that the UAE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'RT @tawhidChtioui: \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21974</th>\n",
       "      <td>b'Send our special gifts to your loved ones!\\x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21975</th>\n",
       "      <td>b'Trainer code: 2735 9457 4031\\nfrom \\xf0\\x9f\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21976</th>\n",
       "      <td>b\"We are looking for ICY SNOW , SANDSTORM .\\nI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21977</th>\n",
       "      <td>b'Make #MONEY from #home through PC #APP \\nMak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21978</th>\n",
       "      <td>b'#Earn Money from #home through PC #APP\\nwith...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21979 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text\n",
       "0      b'Joined UAE Minister &amp; President-Designat...\n",
       "1      b'Foreign Secretary @AmbVMKwatra participated ...\n",
       "2      b'As the UAE marks National Environment Day, w...\n",
       "3      b'RT @KhalifaAlgaz: Did you know that the UAE ...\n",
       "4      b'RT @tawhidChtioui: \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\...\n",
       "...                                                  ...\n",
       "21974  b'Send our special gifts to your loved ones!\\x...\n",
       "21975  b'Trainer code: 2735 9457 4031\\nfrom \\xf0\\x9f\\...\n",
       "21976  b\"We are looking for ICY SNOW , SANDSTORM .\\nI...\n",
       "21977  b'Make #MONEY from #home through PC #APP \\nMak...\n",
       "21978  b'#Earn Money from #home through PC #APP\\nwith...\n",
       "\n",
       "[21979 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'Joined UAE Minister &amp;amp; President-Designat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Foreign Secretary @AmbVMKwatra participated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'As the UAE marks National Environment Day, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'RT @KhalifaAlgaz: Did you know that the UAE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'RT @tawhidChtioui: \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21888</th>\n",
       "      <td>b'@m_ut67 @modgovae @Forsan_UAE Dear sir pleas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21889</th>\n",
       "      <td>b'@UAE_BARQ Dear sir please my help you sir my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21890</th>\n",
       "      <td>b'@UAE_BARQ Dear sir please my help you sir my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21915</th>\n",
       "      <td>b\"#PhonePe launched a service on Tuesday that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21962</th>\n",
       "      <td>b'@WeLoveIndia7 Saudi k log bhikhari h ? \\nIn ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5878 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text\n",
       "0      b'Joined UAE Minister &amp; President-Designat...\n",
       "1      b'Foreign Secretary @AmbVMKwatra participated ...\n",
       "2      b'As the UAE marks National Environment Day, w...\n",
       "3      b'RT @KhalifaAlgaz: Did you know that the UAE ...\n",
       "4      b'RT @tawhidChtioui: \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\...\n",
       "...                                                  ...\n",
       "21888  b'@m_ut67 @modgovae @Forsan_UAE Dear sir pleas...\n",
       "21889  b'@UAE_BARQ Dear sir please my help you sir my...\n",
       "21890  b'@UAE_BARQ Dear sir please my help you sir my...\n",
       "21915  b\"#PhonePe launched a service on Tuesday that ...\n",
       "21962  b'@WeLoveIndia7 Saudi k log bhikhari h ? \\nIn ...\n",
       "\n",
       "[5878 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset = dataset.drop_duplicates().copy()\n",
    "updated_dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing URLs, @'s and hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    b'Joined UAE Minister &amp; President-Designat...\n",
       "1    b'Foreign Secretary  participated in a meeting...\n",
       "2    b'As the UAE marks National Environment Day, w...\n",
       "3    b'RT  Did you know that the UAE is developing ...\n",
       "4    b'RT  \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\x8f UAE lunar r...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = updated_dataset['Text'].copy()\n",
    "updated_dataset['Cleaned Text'] = tweets.str.replace(r'@[^\\s]+|#[^\\s]+|https?:\\/\\/\\S+|www\\.\\S+', '',regex=True)\n",
    "updated_dataset['Cleaned Text'].head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicates again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5398, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset = updated_dataset.drop('Text', axis=1)\n",
    "updated_dataset = updated_dataset.drop_duplicates()\n",
    "updated_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dataset.to_csv('updated-dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cleaned Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'Joined UAE Minister &amp;amp; President-Designat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Foreign Secretary  participated in a meeting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'As the UAE marks National Environment Day, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'RT  Did you know that the UAE is developing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'RT  \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\x8f UAE lunar r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21888</th>\n",
       "      <td>b'   Dear sir please my help you sir my two ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21889</th>\n",
       "      <td>b' Dear sir please my help you sir my two kidn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21890</th>\n",
       "      <td>b' Dear sir please my help you sir my two kidn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21915</th>\n",
       "      <td>b\" launched a service on Tuesday that will all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21962</th>\n",
       "      <td>b' Saudi k log bhikhari h ? \\nIn Middle East, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5398 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Cleaned Text\n",
       "0      b'Joined UAE Minister &amp; President-Designat...\n",
       "1      b'Foreign Secretary  participated in a meeting...\n",
       "2      b'As the UAE marks National Environment Day, w...\n",
       "3      b'RT  Did you know that the UAE is developing ...\n",
       "4      b'RT  \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\x8f UAE lunar r...\n",
       "...                                                  ...\n",
       "21888  b'   Dear sir please my help you sir my two ki...\n",
       "21889  b' Dear sir please my help you sir my two kidn...\n",
       "21890  b' Dear sir please my help you sir my two kidn...\n",
       "21915  b\" launched a service on Tuesday that will all...\n",
       "21962  b' Saudi k log bhikhari h ? \\nIn Middle East, ...\n",
       "\n",
       "[5398 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset = pd.read_csv('updated-dataset.csv', index_col=0)\n",
    "updated_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Labelling\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data labelling, we decided to use first use TextBlob. TextBlob calculates the subjectivity and polarity of a text to classify the text as 'Positive', 'Negative' or 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get subjectivity\n",
    "def getSubjectivity(text):\n",
    "   return TextBlob(text).sentiment.subjectivity\n",
    "  \n",
    " \n",
    "#Function to get the polarity\n",
    "def getPolarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "   \n",
    "#Create two new columns ‘Subjectivity’ & ‘Polarity’\n",
    "updated_dataset['TextBlob Subjectivity'] =    updated_dataset['Cleaned Text'].apply(getSubjectivity)\n",
    "updated_dataset['TextBlob Polarity'] = updated_dataset['Cleaned Text'].apply(getPolarity)\n",
    "def getAnalysis(score):\n",
    "  if score < 0:\n",
    "    return 'Negative'\n",
    "  elif score == 0:\n",
    "    return 'Neutral'\n",
    "  else:\n",
    "    return 'Positive'\n",
    "\n",
    "updated_dataset['TextBlob Sentiment'] =  updated_dataset['TextBlob Polarity'].apply(getAnalysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive    2643\n",
       "Neutral     2276\n",
       "Negative     479\n",
       "Name: TextBlob Sentiment, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset['TextBlob Sentiment'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that TextBlob classified the majority of tweets to be Positive. To compare, we also decided to use Vader as well to observe if there was a major difference in classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "def getSentiment(text):\n",
    "    return sentiment.polarity_scores(text)\n",
    "    \n",
    "updated_dataset['Vader Analysis'] = updated_dataset['Cleaned Text'].apply(getSentiment)\n",
    "\n",
    "def sentimentAnalysis(sentiment_dict):\n",
    "    if sentiment_dict['compound'] >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif sentiment_dict['compound'] <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "updated_dataset['Vader Sentiment'] = updated_dataset['Vader Analysis'].apply(sentimentAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vader Analysis</th>\n",
       "      <th>Vader Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.638, 'pos': 0.362, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.816, 'pos': 0.184, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.874, 'pos': 0.126, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.819, 'pos': 0.181, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21888</th>\n",
       "      <td>{'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21889</th>\n",
       "      <td>{'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21890</th>\n",
       "      <td>{'neg': 0.074, 'neu': 0.719, 'pos': 0.206, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21915</th>\n",
       "      <td>{'neg': 0.043, 'neu': 0.774, 'pos': 0.183, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21962</th>\n",
       "      <td>{'neg': 0.196, 'neu': 0.777, 'pos': 0.028, 'co...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5398 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Vader Analysis Vader Sentiment\n",
       "0      {'neg': 0.0, 'neu': 0.638, 'pos': 0.362, 'comp...        Positive\n",
       "1      {'neg': 0.0, 'neu': 0.816, 'pos': 0.184, 'comp...        Positive\n",
       "2      {'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...        Positive\n",
       "3      {'neg': 0.0, 'neu': 0.874, 'pos': 0.126, 'comp...        Positive\n",
       "4      {'neg': 0.0, 'neu': 0.819, 'pos': 0.181, 'comp...        Positive\n",
       "...                                                  ...             ...\n",
       "21888  {'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...        Positive\n",
       "21889  {'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...        Positive\n",
       "21890  {'neg': 0.074, 'neu': 0.719, 'pos': 0.206, 'co...        Positive\n",
       "21915  {'neg': 0.043, 'neu': 0.774, 'pos': 0.183, 'co...        Positive\n",
       "21962  {'neg': 0.196, 'neu': 0.777, 'pos': 0.028, 'co...        Negative\n",
       "\n",
       "[5398 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset[['Vader Analysis', 'Vader Sentiment']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive    3335\n",
       "Neutral     1691\n",
       "Negative     372\n",
       "Name: Vader Sentiment, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset['Vader Sentiment'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis between Vader and TextBlob\n",
    "\n",
    "\n",
    "| Data Labelling Method      | Positive Label | Negative Label | Neutral Label |\n",
    "| ----------- | ----------- | ----------- | ----------- |\n",
    "| TextBlob      | 2643   (49%)   |479  (8.9%)| 2276 (42.1%)|\n",
    "| Vader   | 3335  (61.8%)      | 372 (6.9%) | 1691 (31.3%)|\n",
    "\n",
    "From the summary table, we can observe that the negatively labelled tweets stay within a 2% difference of each other, regardless of the data labelling method. However, the positive and neutral label differ by 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a csv of tweets with conflicting sentiments\n",
    "unclearTweets = updated_dataset.loc[(updated_dataset['Vader Sentiment'] != updated_dataset['TextBlob Sentiment'])]\n",
    "unclearTweets.to_csv('ConflictingLabels.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After manually reviewing the differences in labels, it seems that the TextBlob sentiment labelling is more accurate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**approach → first data cleaning + tokenization + lemmatization**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Non-English Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    b'Joined UAE Minister &amp; President-Designat...\n",
       "1    b'Foreign Secretary  participated in a meeting...\n",
       "2    b'As the UAE marks National Environment Day, w...\n",
       "3    b'RT  Did you know that the UAE is developing ...\n",
       "4    b'RT   UAE lunar rover will test 1st artificia...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nonenglish(tweet):\n",
    "    tweet = re.sub(r'\\\\x[a-zA-Z0-9]+', '', tweet) # tweets in different language changed to hex code \\xhh\n",
    "    tweet = re.sub(r'\\\\n', '', tweet) # removing new line character as well\n",
    "    return tweet\n",
    "\n",
    "updated_dataset['Cleaned Text'] = updated_dataset['Cleaned Text'].apply(lambda tweet: nonenglish(tweet))\n",
    "updated_dataset['Cleaned Text'].head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    b'Joined UAE Minister &amp; President-Designat...\n",
       "1    b'Foreign Secretary  participated meeting foca...\n",
       "2    b'As UAE marks National Environment Day, remai...\n",
       "3    b'RT  Did know UAE developing Arabic ChatGPT u...\n",
       "4    b'RT   UAE lunar rover test 1st artificial int...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "# contains 318 stopwords, including but not limited to articles and prepositions\n",
    "stopwords = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "updated_dataset['Cleaned Text'] = updated_dataset['Cleaned Text'].apply(lambda tweet: ' '.join([text for text in tweet.split(' ') if text not in stopwords]))\n",
    "updated_dataset['Cleaned Text'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Symbols and Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    bJoined UAE Minister amp PresidentDesignate  H...\n",
       "1    bForeign Secretary  participated meeting focal...\n",
       "2    bAs UAE marks National Environment Day remain ...\n",
       "3    bRT  Did know UAE developing Arabic ChatGPT us...\n",
       "4    bRT   UAE lunar rover test 1st artificial inte...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "punctuations = string.punctuation\n",
    "print(punctuations)\n",
    "\n",
    "updated_dataset['Cleaned Text'] = updated_dataset['Cleaned Text'].apply(lambda tweet: tweet.translate({ord(punc): None for punc in punctuations}))\n",
    "updated_dataset['Cleaned Text'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    bJoined UAE Minister amp PresidentDesignate  H...\n",
       "1    bForeign Secretary  participated meeting focal...\n",
       "2    bAs UAE marks National Environment Day remain ...\n",
       "3    bRT  Did know UAE developing Arabic ChatGPT us...\n",
       "4    bRT   UAE lunar rover test st artificial intel...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset['Cleaned Text'] = updated_dataset['Cleaned Text'].apply(lambda tweet: re.sub('[0-9]+', '', tweet))\n",
    "updated_dataset['Cleaned Text'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing 'b' and 'RT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Joined UAE Minister amp PresidentDesignate  HE...\n",
       "1    Foreign Secretary  participated meeting focal ...\n",
       "2    As UAE marks National Environment Day remain c...\n",
       "3      Did know UAE developing Araic ChatGPT using ...\n",
       "4       UAE lunar rover test st artificial intellig...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset['Cleaned Text'] = updated_dataset['Cleaned Text'].apply(lambda tweet: re.sub('bRT|b', '', tweet))\n",
    "updated_dataset['Cleaned Text'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Joined, UAE, Minister, amp, PresidentDesignat...\n",
       "1    [Foreign, Secretary, participated, meeting, fo...\n",
       "2    [As, UAE, marks, National, Environment, Day, r...\n",
       "3    [Did, know, UAE, developing, Araic, ChatGPT, u...\n",
       "4    [UAE, lunar, rover, test, st, artificial, inte...\n",
       "Name: Tokenized Text, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as data has been completely cleaned, we can simply tokenize by converting strings to lists\n",
    "updated_dataset['Tokenized Text'] = updated_dataset['Cleaned Text'].apply(lambda tweet: tweet.split())\n",
    "updated_dataset['Tokenized Text'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [joined, uae, minister, amp, presidentdesignat...\n",
       "1    [foreign, secretary, participated, meeting, fo...\n",
       "2    [as, uae, mark, national, environment, day, re...\n",
       "3    [did, know, uae, developing, araic, chatgpt, u...\n",
       "4    [uae, lunar, rover, test, st, artificial, inte...\n",
       "Name: Lemmatized Text, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "lemmatizing = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "updated_dataset['Lemmatized Text'] = updated_dataset['Tokenized Text'].apply(lambda tweet: [lemmatizing.lemmatize(text).lower() for text in tweet])\n",
    "updated_dataset['Lemmatized Text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode TextBlob Sentiments into numbers\n",
    "updated_dataset['Encoded_TextBlob_Sentiment'] = updated_dataset['TextBlob Sentiment'].apply(lambda x: 1 if x=='Positive' else -1 if x=='Negative' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dataset.reset_index(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Representation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section experiments with different document representations, namely:\n",
    "- Bag of Words\n",
    "- N-grams\n",
    "- TF-IDF\n",
    "- CBOW\n",
    "- Skip-gram\n",
    "- Pre-trained Word2Vec model by Google\n",
    "\n",
    "\n",
    "Each of the representations are testing using Naive Bayes, and the high performance models are used in the next stage of the pipeline.\n",
    "\n",
    "Note: Arbitrarily using lemmatization and textblob sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = updated_dataset['Lemmatized Text']\n",
    "y = updated_dataset['Encoded_TextBlob_Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    updated_dataset['Lemmatized Text'], \n",
    "    updated_dataset['Encoded_TextBlob_Sentiment'],\n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=updated_dataset['Encoded_TextBlob_Sentiment']\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bow = CountVectorizer(\n",
    "    ngram_range= (1,1), # unigram\n",
    "    preprocessor= lambda x: x, # override preprocessing\n",
    "    tokenizer= lambda x: x, # override tokenization\n",
    ")\n",
    "\n",
    "nb_bow = Pipeline([\n",
    "    ('bow', bow),\n",
    "    ('naive_bayes', MultinomialNB())\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.71      0.28      0.40        96\n",
      "           0       0.85      0.58      0.69       455\n",
      "           1       0.67      0.92      0.77       529\n",
      "\n",
      "    accuracy                           0.72      1080\n",
      "   macro avg       0.74      0.60      0.62      1080\n",
      "weighted avg       0.75      0.72      0.71      1080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_bow.fit(X_train, y_train)\n",
    "y_pred_bow = nb_bow.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_bow))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams\n",
    "How to find best n-gram number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.77      0.35      0.49        96\n",
      "           0       0.87      0.62      0.72       455\n",
      "           1       0.69      0.93      0.79       529\n",
      "\n",
      "    accuracy                           0.75      1080\n",
      "   macro avg       0.78      0.63      0.67      1080\n",
      "weighted avg       0.77      0.75      0.74      1080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngram = CountVectorizer(\n",
    "    ngram_range= (1,3), # trigram\n",
    "    preprocessor= lambda x: x, # override preprocessing\n",
    "    tokenizer= lambda x: x, # override tokenization\n",
    ")\n",
    "\n",
    "ngram_nb = Pipeline([\n",
    "    ('ngram', ngram),\n",
    "    ('naive_bayes', MultinomialNB())\n",
    "])\n",
    "\n",
    "ngram_nb.fit(X_train, y_train)\n",
    "ngram_nb_y_pred = ngram_nb.predict(X_test)\n",
    "print(classification_report(y_test, ngram_nb_y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.09      0.17        96\n",
      "           0       0.88      0.56      0.68       455\n",
      "           1       0.65      0.95      0.77       529\n",
      "\n",
      "    accuracy                           0.71      1080\n",
      "   macro avg       0.84      0.53      0.54      1080\n",
      "weighted avg       0.77      0.71      0.68      1080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    preprocessor= lambda x: x, # override preprocessing\n",
    "    tokenizer= lambda x: x, # override tokenization\n",
    ")\n",
    "\n",
    "tfidf_nb = Pipeline([\n",
    "    ('tfidf', tfidf),\n",
    "    ('naive_bayes', MultinomialNB())\n",
    "])\n",
    "\n",
    "tfidf_nb.fit(X_train, y_train)\n",
    "tfidf_nb_y_pred = tfidf_nb.predict(X_test)\n",
    "print(classification_report(y_test, tfidf_nb_y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\alora\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\alora\\anaconda3\\lib\\site-packages (from gensim) (1.21.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\alora\\anaconda3\\lib\\site-packages (from gensim) (1.7.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\alora\\anaconda3\\lib\\site-packages (from gensim) (6.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "\n",
    "#%pip install gensim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible parameters to tweak is window, cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = Word2Vec(\n",
    "    sentences=X,\n",
    "    window=10, # how many context words before and after target word to consider\n",
    "    min_count=2,\n",
    "    workers=4, # number of threads\n",
    "    sg=0, # 0 to use cbow, 1 for skipgram\n",
    "    cbow_mean=0, # 0 uses sum of context word vectors, 1 uses mean\n",
    ")\n",
    "cbow.train(X, total_examples=cbow.corpus_count,epochs=cbow.epochs)\n",
    "# After training, KeyedVector object is used\n",
    "cbow_model = cbow.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def normalize_vectors(input_vectors):\n",
    "    \"\"\"Normalizes vectors to be in range [0,1)\n",
    "    \n",
    "    Args:\n",
    "        input_vectors (np.ndarray): List of vectors, each vector is np.ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    return MinMaxScaler().fit(input_vectors).transform(input_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence(sentence, wv_model):\n",
    "    \"\"\"Convert a sentence to a vector by summing word vectors in the sentence.\n",
    "\n",
    "    Another option could be to average word vectors rather than just sum.\n",
    "    \n",
    "    Arg: \n",
    "        sentence (list): Tokenized sentence\n",
    "    Returns:\n",
    "        (np.ndarray): Vector representation of sentence\n",
    "    \"\"\"\n",
    "    vector_size = wv_model.vector_size\n",
    "    wv_res = np.zeros(vector_size)\n",
    "    count = 0\n",
    "    for w in sentence:\n",
    "        if w in wv_model:\n",
    "            # count += 1\n",
    "            wv_res += wv_model[w]\n",
    "    # wv_res = wv_res/count\n",
    "    return wv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00        96\n",
      "           0       0.62      0.47      0.53       455\n",
      "           1       0.58      0.81      0.67       529\n",
      "\n",
      "    accuracy                           0.59      1080\n",
      "   macro avg       0.40      0.42      0.40      1080\n",
      "weighted avg       0.54      0.59      0.55      1080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the embeddings of train and test set\n",
    "X_train_cbow = normalize_vectors(np.array([vectorize_sentence(x,cbow_model) for x in X_train]))\n",
    "X_test_cbow = normalize_vectors(np.array([vectorize_sentence(x,cbow_model) for x in X_test]))\n",
    "\n",
    "#test NB\n",
    "nb_cbow = MultinomialNB()\n",
    "nb_cbow.fit(X_train_cbow, y_train)\n",
    "y_pred_cbow = nb_cbow.predict(X_test_cbow)\n",
    "print(classification_report(y_test, y_pred_cbow, zero_division=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram = Word2Vec(\n",
    "    sentences=X,\n",
    "    window=10,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    sg=1, # 0 to use cbow, 1 for skipgram\n",
    ")\n",
    "# Train the model\n",
    "skipgram.train(X, total_examples=skipgram.corpus_count,epochs=skipgram.epochs)\n",
    "\n",
    "# After training, KeyedVector object is used\n",
    "skipgram_model = skipgram.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00        96\n",
      "           0       0.66      0.46      0.55       455\n",
      "           1       0.58      0.83      0.68       529\n",
      "\n",
      "    accuracy                           0.60      1080\n",
      "   macro avg       0.41      0.43      0.41      1080\n",
      "weighted avg       0.56      0.60      0.56      1080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the embeddings of train and test set\n",
    "X_train_sg = normalize_vectors(np.array([vectorize_sentence(x,skipgram_model) for x in X_train]))\n",
    "X_test_sg = normalize_vectors(np.array([vectorize_sentence(x,skipgram_model) for x in X_test]))\n",
    "#test on NB\n",
    "nb_sg = MultinomialNB()\n",
    "nb_sg.fit(X_train_sg, y_train)\n",
    "y_pred_sg = nb_sg.predict(X_test_sg)\n",
    "print(classification_report(y_test, y_pred_sg, zero_division=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pre-trained Word2Vec Model\n",
    "Using pre-trained word2vec model from Google, which has 3 million words. Trained on 100 billion words from the google news dataset. Unsure if it uses CBOW or SkipGram or both\n",
    "\n",
    "From https://thinkingneuron.com/how-to-classify-text-using-word2vec/\n",
    "Google Model can be downloaded from https://thinkingneuron.com/how-to-classify-text-using-word2vec/#:~:text=download%20link%3A%20https%3A%2F%2Fdrive.google.com%2Ffile%2Fd%2F0b7xkcwpi5kdynlnuttlss21pqmm%2Fedit%3Fusp%3Dsharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alora\\Academic\\F20AA\\cw1\\sentiment-analysis\\cw-1.ipynb Cell 84\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alora/Academic/F20AA/cw1/sentiment-analysis/cw-1.ipynb#Y146sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# load the word vectors from Google Model (1GB)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alora/Academic/F20AA/cw1/sentiment-analysis/cw-1.ipynb#Y146sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Will take time because 3 mil words. \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alora/Academic/F20AA/cw1/sentiment-analysis/cw-1.ipynb#Y146sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Download model from https://thinkingneuron.com/how-to-classify-text-using-word2vec/#:~:text=download%20link%3A%20https%3A%2F%2Fdrive.google.com%2Ffile%2Fd%2F0b7xkcwpi5kdynlnuttlss21pqmm%2Fedit%3Fusp%3Dsharing\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alora/Academic/F20AA/cw1/sentiment-analysis/cw-1.ipynb#Y146sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m GoogleModel \u001b[39m=\u001b[39m gensim\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mKeyedVectors\u001b[39m.\u001b[39;49mload_word2vec_format(\u001b[39m'\u001b[39;49m\u001b[39mGoogleNews-vectors-negative300.bin.gz\u001b[39;49m\u001b[39m'\u001b[39;49m, binary\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,)\n",
      "File \u001b[1;32mc:\\Users\\alora\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py:1629\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1582\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m   1583\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_word2vec_format\u001b[39m(\n\u001b[0;32m   1584\u001b[0m         \u001b[39mcls\u001b[39m, fname, fvocab\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m, unicode_errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1585\u001b[0m         limit\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, datatype\u001b[39m=\u001b[39mREAL, no_header\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1586\u001b[0m     ):\n\u001b[0;32m   1587\u001b[0m     \u001b[39m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[0;32m   1588\u001b[0m \n\u001b[0;32m   1589\u001b[0m \u001b[39m    Warnings\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1627\u001b[0m \n\u001b[0;32m   1628\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1629\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_word2vec_format(\n\u001b[0;32m   1630\u001b[0m         \u001b[39mcls\u001b[39;49m, fname, fvocab\u001b[39m=\u001b[39;49mfvocab, binary\u001b[39m=\u001b[39;49mbinary, encoding\u001b[39m=\u001b[39;49mencoding, unicode_errors\u001b[39m=\u001b[39;49municode_errors,\n\u001b[0;32m   1631\u001b[0m         limit\u001b[39m=\u001b[39;49mlimit, datatype\u001b[39m=\u001b[39;49mdatatype, no_header\u001b[39m=\u001b[39;49mno_header,\n\u001b[0;32m   1632\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\alora\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py:1955\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   1952\u001b[0m             counts[word] \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(count)\n\u001b[0;32m   1954\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mloading projection weights from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, fname)\n\u001b[1;32m-> 1955\u001b[0m \u001b[39mwith\u001b[39;00m utils\u001b[39m.\u001b[39;49mopen(fname, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m fin:\n\u001b[0;32m   1956\u001b[0m     \u001b[39mif\u001b[39;00m no_header:\n\u001b[0;32m   1957\u001b[0m         \u001b[39m# deduce both vocab_size & vector_size from 1st pass over file\u001b[39;00m\n\u001b[0;32m   1958\u001b[0m         \u001b[39mif\u001b[39;00m binary:\n",
      "File \u001b[1;32mc:\\Users\\alora\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:224\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m ve:\n\u001b[0;32m    222\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(ve\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m])\n\u001b[1;32m--> 224\u001b[0m binary \u001b[39m=\u001b[39m _open_binary_stream(uri, binary_mode, transport_params)\n\u001b[0;32m    225\u001b[0m decompressed \u001b[39m=\u001b[39m so_compression\u001b[39m.\u001b[39mcompression_wrapper(binary, binary_mode, compression)\n\u001b[0;32m    227\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode \u001b[39mor\u001b[39;00m explicit_encoding \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\alora\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:400\u001b[0m, in \u001b[0;36m_open_binary_stream\u001b[1;34m(uri, mode, transport_params)\u001b[0m\n\u001b[0;32m    398\u001b[0m scheme \u001b[39m=\u001b[39m _sniff_scheme(uri)\n\u001b[0;32m    399\u001b[0m submodule \u001b[39m=\u001b[39m transport\u001b[39m.\u001b[39mget_transport(scheme)\n\u001b[1;32m--> 400\u001b[0m fobj \u001b[39m=\u001b[39m submodule\u001b[39m.\u001b[39;49mopen_uri(uri, mode, transport_params)\n\u001b[0;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(fobj, \u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    402\u001b[0m     fobj\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m uri\n",
      "File \u001b[1;32mc:\\Users\\alora\\anaconda3\\lib\\site-packages\\smart_open\\local_file.py:34\u001b[0m, in \u001b[0;36mopen_uri\u001b[1;34m(uri_as_string, mode, transport_params)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopen_uri\u001b[39m(uri_as_string, mode, transport_params):\n\u001b[0;32m     33\u001b[0m     parsed_uri \u001b[39m=\u001b[39m parse_uri(uri_as_string)\n\u001b[1;32m---> 34\u001b[0m     fobj \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39;49mopen(parsed_uri[\u001b[39m'\u001b[39;49m\u001b[39muri_path\u001b[39;49m\u001b[39m'\u001b[39;49m], mode)\n\u001b[0;32m     35\u001b[0m     \u001b[39mreturn\u001b[39;00m fobj\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin.gz'"
     ]
    }
   ],
   "source": [
    "# load the word vectors from Google Model (1GB)\n",
    "# Will take time because 3 mil words. \n",
    "# Download model from https://thinkingneuron.com/how-to-classify-text-using-word2vec/#:~:text=download%20link%3A%20https%3A%2F%2Fdrive.google.com%2Ffile%2Fd%2F0b7xkcwpi5kdynlnuttlss21pqmm%2Fedit%3Fusp%3Dsharing\n",
    "GoogleModel = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence_2(sentences, wv_model):\n",
    "    \"\"\"Convert a list of sentences to a vectors.\n",
    "\n",
    "    By summing word vectors in the sentence.\n",
    "    Another option could be to average word vectors rather than just sum.\n",
    "    \n",
    "    Arg: \n",
    "        sentence (pd.Series): Series consisting of list of tokenized texts \n",
    "    Returns:\n",
    "        (np.ndarray): Vector representation of sentences\n",
    "    \"\"\"\n",
    "    vector_size = wv_model.vector_size\n",
    "    count = 0\n",
    "    vector_sentences = []\n",
    "\n",
    "    for i in range(len(sentences)): \n",
    "        sentence = np.zeros(vector_size)\n",
    "        for word in sentences[i]:\n",
    "            count+=1\n",
    "            if word in wv_model:\n",
    "                sentence += wv_model[word]\n",
    "        # sentence = sentence/count\n",
    "        vector_sentences.append(sentence)\n",
    "    vector_sentences_df = pd.DataFrame(vector_sentences)\n",
    "    \n",
    "    return vector_sentences_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert text data to W2V vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_data = vectorize_sentence_2(X, GoogleModel)\n",
    "w2v_data.reset_index(inplace=True, drop=True)\n",
    "w2v_data['sentiment']=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.132812</td>\n",
       "      <td>2.405243</td>\n",
       "      <td>0.206055</td>\n",
       "      <td>0.998474</td>\n",
       "      <td>0.067627</td>\n",
       "      <td>-0.279907</td>\n",
       "      <td>-1.275879</td>\n",
       "      <td>-4.573059</td>\n",
       "      <td>2.728638</td>\n",
       "      <td>1.771240</td>\n",
       "      <td>...</td>\n",
       "      <td>2.500275</td>\n",
       "      <td>-2.007019</td>\n",
       "      <td>1.403908</td>\n",
       "      <td>-3.793671</td>\n",
       "      <td>-1.133545</td>\n",
       "      <td>-1.440094</td>\n",
       "      <td>-0.750305</td>\n",
       "      <td>0.297852</td>\n",
       "      <td>4.358887</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.202881</td>\n",
       "      <td>1.526184</td>\n",
       "      <td>1.248779</td>\n",
       "      <td>0.022171</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.535217</td>\n",
       "      <td>1.304497</td>\n",
       "      <td>-1.866699</td>\n",
       "      <td>1.610382</td>\n",
       "      <td>0.834839</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.429688</td>\n",
       "      <td>0.471130</td>\n",
       "      <td>0.044685</td>\n",
       "      <td>-0.072174</td>\n",
       "      <td>0.426514</td>\n",
       "      <td>1.126831</td>\n",
       "      <td>-0.328308</td>\n",
       "      <td>0.306915</td>\n",
       "      <td>1.507935</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.514006</td>\n",
       "      <td>2.221619</td>\n",
       "      <td>1.219940</td>\n",
       "      <td>2.058777</td>\n",
       "      <td>-1.362762</td>\n",
       "      <td>-0.093506</td>\n",
       "      <td>2.393387</td>\n",
       "      <td>-2.217712</td>\n",
       "      <td>2.003166</td>\n",
       "      <td>1.495361</td>\n",
       "      <td>...</td>\n",
       "      <td>1.928314</td>\n",
       "      <td>-2.390686</td>\n",
       "      <td>0.371109</td>\n",
       "      <td>-0.576813</td>\n",
       "      <td>0.328003</td>\n",
       "      <td>-0.169029</td>\n",
       "      <td>-0.052216</td>\n",
       "      <td>1.480957</td>\n",
       "      <td>-0.169128</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.134628</td>\n",
       "      <td>0.555664</td>\n",
       "      <td>0.677246</td>\n",
       "      <td>-0.599525</td>\n",
       "      <td>0.236725</td>\n",
       "      <td>0.374146</td>\n",
       "      <td>-1.129639</td>\n",
       "      <td>0.325989</td>\n",
       "      <td>0.212769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.318176</td>\n",
       "      <td>-1.261230</td>\n",
       "      <td>0.185188</td>\n",
       "      <td>-0.322174</td>\n",
       "      <td>0.256348</td>\n",
       "      <td>0.832886</td>\n",
       "      <td>0.004272</td>\n",
       "      <td>0.159027</td>\n",
       "      <td>0.760498</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.098145</td>\n",
       "      <td>0.179138</td>\n",
       "      <td>-0.097778</td>\n",
       "      <td>1.079620</td>\n",
       "      <td>-0.130381</td>\n",
       "      <td>-0.615356</td>\n",
       "      <td>1.475098</td>\n",
       "      <td>-1.155151</td>\n",
       "      <td>0.913147</td>\n",
       "      <td>0.649658</td>\n",
       "      <td>...</td>\n",
       "      <td>2.099121</td>\n",
       "      <td>-0.911377</td>\n",
       "      <td>1.061813</td>\n",
       "      <td>-0.402344</td>\n",
       "      <td>0.636719</td>\n",
       "      <td>-1.323242</td>\n",
       "      <td>-1.282776</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>-0.279785</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5393</th>\n",
       "      <td>-0.848206</td>\n",
       "      <td>1.854889</td>\n",
       "      <td>0.927032</td>\n",
       "      <td>2.490479</td>\n",
       "      <td>-1.490753</td>\n",
       "      <td>0.884399</td>\n",
       "      <td>0.667587</td>\n",
       "      <td>0.053528</td>\n",
       "      <td>2.230713</td>\n",
       "      <td>1.627228</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.471985</td>\n",
       "      <td>-3.151123</td>\n",
       "      <td>-0.589844</td>\n",
       "      <td>1.288662</td>\n",
       "      <td>1.495911</td>\n",
       "      <td>-2.537903</td>\n",
       "      <td>-1.745911</td>\n",
       "      <td>0.368065</td>\n",
       "      <td>-0.410278</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5394</th>\n",
       "      <td>-0.848206</td>\n",
       "      <td>1.854889</td>\n",
       "      <td>0.927032</td>\n",
       "      <td>2.490479</td>\n",
       "      <td>-1.490753</td>\n",
       "      <td>0.884399</td>\n",
       "      <td>0.667587</td>\n",
       "      <td>0.053528</td>\n",
       "      <td>2.230713</td>\n",
       "      <td>1.627228</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.471985</td>\n",
       "      <td>-3.151123</td>\n",
       "      <td>-0.589844</td>\n",
       "      <td>1.288662</td>\n",
       "      <td>1.495911</td>\n",
       "      <td>-2.537903</td>\n",
       "      <td>-1.745911</td>\n",
       "      <td>0.368065</td>\n",
       "      <td>-0.410278</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5395</th>\n",
       "      <td>-0.848206</td>\n",
       "      <td>1.854889</td>\n",
       "      <td>0.927032</td>\n",
       "      <td>2.490479</td>\n",
       "      <td>-1.490753</td>\n",
       "      <td>0.884399</td>\n",
       "      <td>0.667587</td>\n",
       "      <td>0.053528</td>\n",
       "      <td>2.230713</td>\n",
       "      <td>1.627228</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.471985</td>\n",
       "      <td>-3.151123</td>\n",
       "      <td>-0.589844</td>\n",
       "      <td>1.288662</td>\n",
       "      <td>1.495911</td>\n",
       "      <td>-2.537903</td>\n",
       "      <td>-1.745911</td>\n",
       "      <td>0.368065</td>\n",
       "      <td>-0.410278</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5396</th>\n",
       "      <td>-0.170532</td>\n",
       "      <td>0.555901</td>\n",
       "      <td>0.524551</td>\n",
       "      <td>1.156067</td>\n",
       "      <td>-0.659790</td>\n",
       "      <td>0.719757</td>\n",
       "      <td>-0.405373</td>\n",
       "      <td>-0.833130</td>\n",
       "      <td>0.974854</td>\n",
       "      <td>0.811157</td>\n",
       "      <td>...</td>\n",
       "      <td>1.457672</td>\n",
       "      <td>0.223541</td>\n",
       "      <td>0.326981</td>\n",
       "      <td>0.550018</td>\n",
       "      <td>-0.178589</td>\n",
       "      <td>0.676849</td>\n",
       "      <td>-0.140881</td>\n",
       "      <td>0.202820</td>\n",
       "      <td>-0.080078</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5397</th>\n",
       "      <td>-0.742981</td>\n",
       "      <td>1.656494</td>\n",
       "      <td>1.001097</td>\n",
       "      <td>3.151978</td>\n",
       "      <td>-0.668518</td>\n",
       "      <td>-0.188110</td>\n",
       "      <td>-1.311646</td>\n",
       "      <td>-2.395264</td>\n",
       "      <td>0.448215</td>\n",
       "      <td>2.064152</td>\n",
       "      <td>...</td>\n",
       "      <td>2.906128</td>\n",
       "      <td>-0.707062</td>\n",
       "      <td>1.330200</td>\n",
       "      <td>-1.054474</td>\n",
       "      <td>-1.403137</td>\n",
       "      <td>-2.133179</td>\n",
       "      <td>-1.052002</td>\n",
       "      <td>0.674034</td>\n",
       "      <td>0.739746</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5398 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0    -1.132812  2.405243  0.206055  0.998474  0.067627 -0.279907 -1.275879   \n",
       "1    -2.202881  1.526184  1.248779  0.022171 -0.643616 -0.535217  1.304497   \n",
       "2    -1.514006  2.221619  1.219940  2.058777 -1.362762 -0.093506  2.393387   \n",
       "3     0.004150  0.134628  0.555664  0.677246 -0.599525  0.236725  0.374146   \n",
       "4    -1.098145  0.179138 -0.097778  1.079620 -0.130381 -0.615356  1.475098   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5393 -0.848206  1.854889  0.927032  2.490479 -1.490753  0.884399  0.667587   \n",
       "5394 -0.848206  1.854889  0.927032  2.490479 -1.490753  0.884399  0.667587   \n",
       "5395 -0.848206  1.854889  0.927032  2.490479 -1.490753  0.884399  0.667587   \n",
       "5396 -0.170532  0.555901  0.524551  1.156067 -0.659790  0.719757 -0.405373   \n",
       "5397 -0.742981  1.656494  1.001097  3.151978 -0.668518 -0.188110 -1.311646   \n",
       "\n",
       "             7         8         9  ...       291       292       293  \\\n",
       "0    -4.573059  2.728638  1.771240  ...  2.500275 -2.007019  1.403908   \n",
       "1    -1.866699  1.610382  0.834839  ... -0.429688  0.471130  0.044685   \n",
       "2    -2.217712  2.003166  1.495361  ...  1.928314 -2.390686  0.371109   \n",
       "3    -1.129639  0.325989  0.212769  ...  0.318176 -1.261230  0.185188   \n",
       "4    -1.155151  0.913147  0.649658  ...  2.099121 -0.911377  1.061813   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5393  0.053528  2.230713  1.627228  ... -0.471985 -3.151123 -0.589844   \n",
       "5394  0.053528  2.230713  1.627228  ... -0.471985 -3.151123 -0.589844   \n",
       "5395  0.053528  2.230713  1.627228  ... -0.471985 -3.151123 -0.589844   \n",
       "5396 -0.833130  0.974854  0.811157  ...  1.457672  0.223541  0.326981   \n",
       "5397 -2.395264  0.448215  2.064152  ...  2.906128 -0.707062  1.330200   \n",
       "\n",
       "           294       295       296       297       298       299  sentiment  \n",
       "0    -3.793671 -1.133545 -1.440094 -0.750305  0.297852  4.358887          1  \n",
       "1    -0.072174  0.426514  1.126831 -0.328308  0.306915  1.507935         -1  \n",
       "2    -0.576813  0.328003 -0.169029 -0.052216  1.480957 -0.169128          1  \n",
       "3    -0.322174  0.256348  0.832886  0.004272  0.159027  0.760498          0  \n",
       "4    -0.402344  0.636719 -1.323242 -1.282776  0.060547 -0.279785         -1  \n",
       "...        ...       ...       ...       ...       ...       ...        ...  \n",
       "5393  1.288662  1.495911 -2.537903 -1.745911  0.368065 -0.410278          0  \n",
       "5394  1.288662  1.495911 -2.537903 -1.745911  0.368065 -0.410278          0  \n",
       "5395  1.288662  1.495911 -2.537903 -1.745911  0.368065 -0.410278          0  \n",
       "5396  0.550018 -0.178589  0.676849 -0.140881  0.202820 -0.080078         -1  \n",
       "5397 -1.054474 -1.403137 -2.133179 -1.052002  0.674034  0.739746          1  \n",
       "\n",
       "[5398 rows x 301 columns]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and test values\n",
    "X_w2v = w2v_data.iloc[:,:-1].values\n",
    "y_w2v = w2v_data.iloc[:,-1].values\n",
    "\n",
    "X_w2v_scaled = normalize_vectors(X_w2v)\n",
    "\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(\n",
    "    X_w2v_scaled, y_w2v, test_size=0.2, random_state=42,\n",
    "    stratify= y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00        96\n",
      "           0       0.67      0.65      0.66       455\n",
      "           1       0.65      0.78      0.71       529\n",
      "\n",
      "    accuracy                           0.65      1080\n",
      "   macro avg       0.44      0.48      0.45      1080\n",
      "weighted avg       0.60      0.65      0.62      1080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_w2v, y_train_w2v)\n",
    "y_pred_w2v = nb.predict(X_test_w2v)\n",
    "print(classification_report(y_test_w2v, y_pred_w2v, zero_division=0))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Text Representation Models\n",
    "The different models are compared based on their weighted precision, recall, and f1 score.\n",
    "Values that are closer to one are optimal.\n",
    "\n",
    "|Model|Precision|Recall|F-1|\n",
    "|---|---|---|---|\n",
    "|Bag Of Words (Unigram)|0.75|0.73|0.71|\n",
    "|Trigrams|0.77|0.75|0.73|\n",
    "|TF-IDF|0.77|0.71|0.68|\n",
    "|CBOW (Trained on scraped tweets corpus)|0.58|0.57|0.5|\n",
    "|Skip-gram (Trained on scraped tweets corpus)|0.6|0.60|0.54|\n",
    "|Word2Vec model (Trained on Google News Corpus)|0.59|0.65|0.62|\n",
    "\n",
    "\n",
    "It was expected that the more sophisticated models - those which take into account the context of words - would perform better. However, based on the results the more primitive approaches performed better. This could be due to our small dataset.\n",
    "\n",
    "The tri-gram model was the best performer in terms of F-1 score, which is the harmonic mean of the precision and recall values.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression with Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.80      0.38      0.51        96\n",
      "           0       0.77      0.86      0.81       455\n",
      "           1       0.82      0.81      0.82       529\n",
      "\n",
      "    accuracy                           0.79      1080\n",
      "   macro avg       0.80      0.68      0.71      1080\n",
      "weighted avg       0.80      0.79      0.79      1080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "ngram = CountVectorizer(\n",
    "    ngram_range= (1, 3), # trigram\n",
    "    preprocessor= lambda x: x, # override preprocessing\n",
    "    tokenizer= lambda x: x, # override tokenization\n",
    ")\n",
    "\n",
    "ngram_lr = Pipeline([\n",
    "    ('ngram', ngram),\n",
    "    ('logistic_regression', LogisticRegression())\n",
    "])\n",
    "\n",
    "ngram_lr.fit(X_train, y_train)\n",
    "ngram_lr_y_pred = ngram_lr.predict(X_test)\n",
    "print(classification_report(y_test, ngram_lr_y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. FastText with Trigrams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastText is an open-source library created by Facebook AI Research lab. Setup instructions are here: https://fasttext.cc/docs/en/supervised-tutorial.html#installing-fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module fasttext.FastText in fasttext:\n",
      "\n",
      "NAME\n",
      "    fasttext.FastText\n",
      "\n",
      "DESCRIPTION\n",
      "    # Copyright (c) 2017-present, Facebook, Inc.\n",
      "    # All rights reserved.\n",
      "    #\n",
      "    # This source code is licensed under the MIT license found in the\n",
      "    # LICENSE file in the root directory of this source tree.\n",
      "\n",
      "FUNCTIONS\n",
      "    cbow(*kargs, **kwargs)\n",
      "    \n",
      "    eprint(*args, **kwargs)\n",
      "    \n",
      "    load_model(path)\n",
      "        Load a model given a filepath and return a model object.\n",
      "    \n",
      "    read_args(arg_list, arg_dict, arg_names, default_values)\n",
      "    \n",
      "    skipgram(*kargs, **kwargs)\n",
      "    \n",
      "    supervised(*kargs, **kwargs)\n",
      "    \n",
      "    tokenize(text)\n",
      "        Given a string of text, tokenize it and return a list of tokens\n",
      "    \n",
      "    train_supervised(*kargs, **kwargs)\n",
      "        Train a supervised model and return a model object.\n",
      "        \n",
      "        input must be a filepath. The input text does not need to be tokenized\n",
      "        as per the tokenize function, but it must be preprocessed and encoded\n",
      "        as UTF-8. You might want to consult standard preprocessing scripts such\n",
      "        as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html\n",
      "        \n",
      "        The input file must must contain at least one label per line. For an\n",
      "        example consult the example datasets which are part of the fastText\n",
      "        repository such as the dataset pulled by classification-example.sh.\n",
      "    \n",
      "    train_unsupervised(*kargs, **kwargs)\n",
      "        Train an unsupervised model and return a model object.\n",
      "        \n",
      "        input must be a filepath. The input text does not need to be tokenized\n",
      "        as per the tokenize function, but it must be preprocessed and encoded\n",
      "        as UTF-8. You might want to consult standard preprocessing scripts such\n",
      "        as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html\n",
      "        \n",
      "        The input field must not contain any labels or use the specified label prefix\n",
      "        unless it is ok for those words to be ignored. For an example consult the\n",
      "        dataset pulled by the example script word-vector-example.sh, which is\n",
      "        part of the fastText repository.\n",
      "\n",
      "DATA\n",
      "    BOW = '<'\n",
      "    EOS = '</s>'\n",
      "    EOW = '>'\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    displayed_errors = {}\n",
      "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 1310...\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
      "    unsupervised_default = {'autotuneDuration': 300, 'autotuneMetric': 'f1...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\alora\\anaconda3\\lib\\site-packages\\fasttext\\fasttext.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "help(fasttext.FastText)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformatting the dataset as FastText expects each row of data to be in the form:\n",
    "__label__sentiment tweet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5398, 2)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftDf= updated_dataset[['TextBlob Sentiment', 'Cleaned Text']].copy()\n",
    "ftDf.iloc[:, 0] = ftDf.iloc[:, 0].apply(lambda x: '__label__' + x)\n",
    "ftDf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting into test and train\n",
    "training = ftDf.sample(frac = 0.7)\n",
    "testing = ftDf.drop(training.index)\n",
    "\n",
    "training.to_csv('train.txt', \n",
    "                index = False, \n",
    "                sep = ' ',\n",
    "                header = None, \n",
    "                quoting = csv.QUOTE_NONE, \n",
    "                quotechar = \"\", \n",
    "                escapechar = \" \")\n",
    "\n",
    "testing.to_csv('test.txt', \n",
    "                index = False, \n",
    "                sep = ' ',\n",
    "                header = None, \n",
    "                quoting = csv.QUOTE_NONE, \n",
    "                quotechar = \"\", \n",
    "                escapechar = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftModel = fasttext.train_supervised('train.txt', wordNgrams = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1619, 0.44224830142063004, 0.44224830142063004)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftModel.test('test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate further, we can test it on unseen samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__Neutral',), array([0.46955368]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftModel.predict('I love the UAE for its innovation with artificial intelligence.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the test set, fastText model had a precision score of 0.44 ad a recall score of 0.44. It had also misclassified an unseen sample. However, FastText model parameters can be adjusted, so we will try adjusting the epoch and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1619, 0.7794935145151328, 0.7794935145151328)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftModel2 = fasttext.train_supervised('train.txt', wordNgrams = 3, epoch = 6,lr=0.25)\n",
    "ftModel2.test('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__Positive',), array([0.68560129]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftModel2.predict('I love the UAE for its innovation with artificial intelligence.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, increasing the epoch and learning rate corrected a misclassification that happened with the previous model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4d3d5844ec835cb994d9b961c91f9f9a1dfb3461f5792c6e96deb8bb5afec0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

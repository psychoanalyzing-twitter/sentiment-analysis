{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Distribution\n",
    "| Member | ID | Tasks |\n",
    "|---|---|---|\n",
    "|Bhavika| - | - | \n",
    "|Alora| - | - |\n",
    "|Andrea| - | - |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tweepy in /home/andrea/.local/lib/python3.10/site-packages (4.12.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/lib/python3/dist-packages (from tweepy) (3.2.0)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in /home/andrea/.local/lib/python3.10/site-packages (from tweepy) (2.28.1)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /home/andrea/.local/lib/python3.10/site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/andrea/.local/lib/python3.10/site-packages (from requests<3,>=2.27.0->tweepy) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/andrea/.local/lib/python3.10/site-packages (from requests<3,>=2.27.0->tweepy) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/andrea/.local/lib/python3.10/site-packages (from requests<3,>=2.27.0->tweepy) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/andrea/.local/lib/python3.10/site-packages (from requests<3,>=2.27.0->tweepy) (3.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tweepy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keys for authentication\n",
    "consumer_key = \"ONLQ0vLSAXM2tZDnckominEcK\"\n",
    "consumer_secret = \"MpP7XurdGn1wpufX3rrfQgrs4AROnaw9ZiiZzKN6exr3zEDlN6\"\n",
    "access_token = \"1274112117738737664-J2DjjoqqzN11CNp8bpnYzVp1dUYKM4\"\n",
    "access_token_secret = \"3coYFJVCqHnOmD2m9Qt6zs7j1I24lhlPTU71eUOq4Zqg9\"\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAALJslQEAAAAA7EWdkTwrVDM44LIuewNascHjvoY%3DeXmhQyuXgo8zXJiSTMfqtmPA393HHO2W7nSrk0bK6b67SuJeb0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate\n",
    "auth = tweepy.OAuth1UserHandler(\n",
    "    consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tweets(query: str, amount: int) -> list:\n",
    "    \"\"\"Scrapes a specified number of tweets based on given query and location.\n",
    "\n",
    "    Method from https://www.sahilfruitwala.com/guide-to-extract-tweets-using-tweepy#how-to-retrieve-specific-number-of-tweets-using-tweepy \n",
    "\n",
    "    Args:\n",
    "        query (str): The query\n",
    "    Returns:\n",
    "        list (str): List of contents of tweets.\n",
    "    \"\"\"\n",
    "    extracted_tweets = []\n",
    "    for tweet in tweepy.Cursor(api.search_tweets, query, count=100, tweet_mode=\"extended\", result_type=\"recent\").items(amount):\n",
    "        extracted_tweets.append(tweet.full_text)\n",
    "    return extracted_tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(tweets: list, csv_name: str) -> None:\n",
    "    \"\"\"Appends the list of tweets to csv.\n",
    "\n",
    "    Largely copied from https://gist.github.com/anku255/0cebd75cce675f2b56de1ef48ec06575.\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Writing {0} tweets to {1}\".format(len(tweets), csv_name))\n",
    "    tweets_for_csv = [[tweet.encode(\"utf-8\")] for tweet in tweets]\n",
    "    with open(csv_name, \"a+\") as file:\n",
    "        writer = csv.writer(file, delimiter=\",\")\n",
    "        writer.writerows(tweets_for_csv)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The queries for scraping tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "queries = [\n",
    "    [\"(metaverse OR meta verse OR #metaverse OR #meta #verse) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(innovation OR #innovation OR innovate OR innovative OR #innovate OR #innovative) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(sustainability OR #sustainability OR sustainable) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(technology footprint OR technology OR #technologyfootprint) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(artificial intelligence OR ai OR #ai OR #artificialintelligence) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(ethics OR #ethics OR ethical OR #ethical) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(inflation OR #inflation) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cop 28 OR cop28 OR #cop28 OR #cop #28) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(museum of the future OR @museumofthefuture OR #museumofthefuture OR #museum #of #the #future) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(bit coin OR bitcoin OR #bitcoin OR #crypto OR crpyto OR cryptocurrency) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cyber attack OR #cyberattack OR #cyber OR #cyberattacks OR #cyber #attack) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(robot OR robots OR #robots OR #robot) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(drone OR drones OR #drone OR #drones) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(hacker OR hacking OR #hacker OR #hacking OR #hack) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(chatgpt OR #chatgpt OR #chat #gpt OR chat gpt) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cybersecurity OR #cybersecurityOR #cybersec OR cybersec OR cyber security OR #cyber #security) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(sustainabletech OR sustainable ai OR sustainable technology OR #sustainabletech OR #sustainableai OR #sustainabletechnology) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(computing OR quantum computing OR #quantum #computing OR #computing) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(automation OR #automation) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(distributed cloud OR #distributedcloud OR #distributed #cloud) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(big data OR #big #data OR bigdata OR #bigdata) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(ar OR #ar OR #augmentedreality OR #augmented #reality) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(data mining OR #data #mining OR #datamining) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(software OR #software) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(tech OR technology OR #tech OR #technology) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(digital OR digital transformation OR #digital #transformation OR #digitaltransformation) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(blockchain OR #blockchain) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(coders_hq OR coders hq OR #coders OR #coders #hq OR @coders_hq) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(nft OR non-fungible token OR non fungible token OR nfts OR #nfts OR #nonfungibletoken OR #nft) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(binance OR @binance OR #binance) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(uae hackathon OR hackathon OR #hackathon) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(internet of things OR IoT OR #iot OR #internet #of #things OR #internetofthings) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(software engineering OR #softwareengineering) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(#coding OR coding) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(@uaeai OR #uaeai Or #uae #ai) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(technology OR tech OT #tech Or #technology) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(nft OR #nft OR #nfts OR nfts) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(UAE codes OR #UAE_codes OR #UAEcodes) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(Microsoft Hololens OR Hololens OR #Hololens) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(GITEX or #gitex) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(information OR #information) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(internet OR #internet) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(computer OR science OR computer science OR #computerscience OR #computer OR #science) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(technological OR #technological) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(engineering OR #engineering) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(computing OR #computing) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(systems OR #systems) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(electronics OR #electronics) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(application OR #application OR app OR #app) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(robotics OR #robotics) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(high tech OR high technology OR #hightech OR #hightechnology OR #high #technology OR #high #tech) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(nanotech OR nano tech OR nanotechnology OR nano technology OR #nanotech OR #nano #tech OR #nano #technology OR #nanotechnology) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(biotech OR bio tech OR biotechnology OR bio technology OR #biotech OR #bio #tech OR #bio #technology OR #biotechnology) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(code OR #code) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(automation OR #automation OR automate OR #automate) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(virtual OR #virtual OR online OR #online) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(technological advancements OR #technological #advancements OR #advancements OR #techadvancements) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(bionics OR #bionics) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cyberscience OR #cyberscience) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(google OR #google OR @google) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(oracle OR #oracle OR @oracle) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(ibm OR #ibm OR @ibm) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(bionics OR #bionics) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(sap OR #sap) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(global knowledge OR #globalknowledge OR #global #knowledge) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(amazon OR #amazon OR @amazon) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(hewlett-packard OR Hewlett Packard OR HP OR #hewlett-packard OR #Hewlett #Packard OR #HP) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cisco OR #cisco OR @cisco) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(dell OR #dell OR @dell) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cisco OR #cisco OR @cisco) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(@WorldGovSummit OR #WGS OR #WGS2023 OR World Goverment Summit) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(masdar OR #masdar OR @masdar) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(renewable energy OR clean energy OR #renewableenergy OR #cleanenergy) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(machine learning OR #machinelearning OR ML OR #ML) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(recommender systems OR #RS OR #recommender #systems OR recommender engine) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(deep learning OR #deeplearning) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(research OR #research) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commented out below to prevent re-running code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Scraping tweets and writing them to dataset.csv\"\"\"\n",
    "# csv_name=\"dataset.csv\"\n",
    "# total = 0\n",
    "# for query in queries:\n",
    "#     tweets = scrape_tweets(query=query, amount=500)\n",
    "#     total += len(tweets)\n",
    "#     print(\"Found {0} tweets related to the query {1}\".format(len(tweets), query))\n",
    "#     write_to_csv(tweets=tweets,csv_name=csv_name)\n",
    "# print(\"Done. {0} rows.\".format(total))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section cleans the scraped tweets dataset of duplicate tweets, URLs, @'s and #s."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "- Do not modify original csv `dataset.csv`. The dataset with no duplicates is stored in `updated_dataset.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates\n",
    "dataset = pd.read_csv(\"dataset.csv\", header=None)\n",
    "dataset.rename(columns={0: 'Text'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'Joined UAE Minister &amp;amp; President-Designat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Foreign Secretary @AmbVMKwatra participated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'As the UAE marks National Environment Day, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'RT @KhalifaAlgaz: Did you know that the UAE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'RT @tawhidChtioui: \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21974</th>\n",
       "      <td>b'Send our special gifts to your loved ones!\\x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21975</th>\n",
       "      <td>b'Trainer code: 2735 9457 4031\\nfrom \\xf0\\x9f\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21976</th>\n",
       "      <td>b\"We are looking for ICY SNOW , SANDSTORM .\\nI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21977</th>\n",
       "      <td>b'Make #MONEY from #home through PC #APP \\nMak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21978</th>\n",
       "      <td>b'#Earn Money from #home through PC #APP\\nwith...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21979 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text\n",
       "0      b'Joined UAE Minister &amp; President-Designat...\n",
       "1      b'Foreign Secretary @AmbVMKwatra participated ...\n",
       "2      b'As the UAE marks National Environment Day, w...\n",
       "3      b'RT @KhalifaAlgaz: Did you know that the UAE ...\n",
       "4      b'RT @tawhidChtioui: \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\...\n",
       "...                                                  ...\n",
       "21974  b'Send our special gifts to your loved ones!\\x...\n",
       "21975  b'Trainer code: 2735 9457 4031\\nfrom \\xf0\\x9f\\...\n",
       "21976  b\"We are looking for ICY SNOW , SANDSTORM .\\nI...\n",
       "21977  b'Make #MONEY from #home through PC #APP \\nMak...\n",
       "21978  b'#Earn Money from #home through PC #APP\\nwith...\n",
       "\n",
       "[21979 rows x 1 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'Joined UAE Minister &amp;amp; President-Designat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Foreign Secretary @AmbVMKwatra participated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'As the UAE marks National Environment Day, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'RT @KhalifaAlgaz: Did you know that the UAE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'RT @tawhidChtioui: \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21888</th>\n",
       "      <td>b'@m_ut67 @modgovae @Forsan_UAE Dear sir pleas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21889</th>\n",
       "      <td>b'@UAE_BARQ Dear sir please my help you sir my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21890</th>\n",
       "      <td>b'@UAE_BARQ Dear sir please my help you sir my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21915</th>\n",
       "      <td>b\"#PhonePe launched a service on Tuesday that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21962</th>\n",
       "      <td>b'@WeLoveIndia7 Saudi k log bhikhari h ? \\nIn ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5878 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text\n",
       "0      b'Joined UAE Minister &amp; President-Designat...\n",
       "1      b'Foreign Secretary @AmbVMKwatra participated ...\n",
       "2      b'As the UAE marks National Environment Day, w...\n",
       "3      b'RT @KhalifaAlgaz: Did you know that the UAE ...\n",
       "4      b'RT @tawhidChtioui: \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\...\n",
       "...                                                  ...\n",
       "21888  b'@m_ut67 @modgovae @Forsan_UAE Dear sir pleas...\n",
       "21889  b'@UAE_BARQ Dear sir please my help you sir my...\n",
       "21890  b'@UAE_BARQ Dear sir please my help you sir my...\n",
       "21915  b\"#PhonePe launched a service on Tuesday that ...\n",
       "21962  b'@WeLoveIndia7 Saudi k log bhikhari h ? \\nIn ...\n",
       "\n",
       "[5878 rows x 1 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset = dataset.drop_duplicates().copy()\n",
    "updated_dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing URLs, @'s and hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    b'Joined UAE Minister &amp; President-Designat...\n",
       "1    b'Foreign Secretary  participated in a meeting...\n",
       "2    b'As the UAE marks National Environment Day, w...\n",
       "3    b'RT  Did you know that the UAE is developing ...\n",
       "4    b'RT  \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\x8f UAE lunar r...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = updated_dataset['Text'].copy()\n",
    "updated_dataset['Cleaned Text'] = tweets.str.replace(\n",
    "    r'@[^\\s]+|#[^\\s]+|https?:\\/\\/\\S+|www\\.\\S+', '', regex=True)\n",
    "updated_dataset['Cleaned Text'].head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicates again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5398, 1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset = updated_dataset.drop('Text', axis=1)\n",
    "updated_dataset = updated_dataset.drop_duplicates()\n",
    "updated_dataset.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dataset.to_csv('updated-dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cleaned Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'Joined UAE Minister &amp;amp; President-Designat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Foreign Secretary  participated in a meeting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'As the UAE marks National Environment Day, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'RT  Did you know that the UAE is developing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'RT  \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\x8f UAE lunar r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21888</th>\n",
       "      <td>b'   Dear sir please my help you sir my two ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21889</th>\n",
       "      <td>b' Dear sir please my help you sir my two kidn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21890</th>\n",
       "      <td>b' Dear sir please my help you sir my two kidn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21915</th>\n",
       "      <td>b\" launched a service on Tuesday that will all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21962</th>\n",
       "      <td>b' Saudi k log bhikhari h ? \\nIn Middle East, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5398 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Cleaned Text\n",
       "0      b'Joined UAE Minister &amp; President-Designat...\n",
       "1      b'Foreign Secretary  participated in a meeting...\n",
       "2      b'As the UAE marks National Environment Day, w...\n",
       "3      b'RT  Did you know that the UAE is developing ...\n",
       "4      b'RT  \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\x8f UAE lunar r...\n",
       "...                                                  ...\n",
       "21888  b'   Dear sir please my help you sir my two ki...\n",
       "21889  b' Dear sir please my help you sir my two kidn...\n",
       "21890  b' Dear sir please my help you sir my two kidn...\n",
       "21915  b\" launched a service on Tuesday that will all...\n",
       "21962  b' Saudi k log bhikhari h ? \\nIn Middle East, ...\n",
       "\n",
       "[5398 rows x 1 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset = pd.read_csv('updated-dataset.csv', index_col=0)\n",
    "updated_dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Labelling\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data labelling, we decided to use first use TextBlob. TextBlob calculates the subjectivity and polarity of a text to classify the text as 'Positive', 'Negative' or 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get subjectivity\n",
    "def getSubjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "\n",
    "# Function to get the polarity\n",
    "def getPolarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "\n",
    "# Create two new columns ‘Subjectivity’ & ‘Polarity’\n",
    "updated_dataset['TextBlob Subjectivity'] = updated_dataset['Cleaned Text'].apply(\n",
    "    getSubjectivity)\n",
    "updated_dataset['TextBlob Polarity'] = updated_dataset['Cleaned Text'].apply(\n",
    "    getPolarity)\n",
    "\n",
    "\n",
    "def getAnalysis(score):\n",
    "    if score < 0:\n",
    "        return 'Negative'\n",
    "    elif score == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "\n",
    "updated_dataset['TextBlob Sentiment'] = updated_dataset['TextBlob Polarity'].apply(\n",
    "    getAnalysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive    2643\n",
       "Neutral     2276\n",
       "Negative     479\n",
       "Name: TextBlob Sentiment, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset['TextBlob Sentiment'].value_counts()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that TextBlob classified the majority of tweets to be Positive. To compare, we also decided to use Vader as well to observe if there was a major difference in classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install vaderSentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "def getSentiment(text):\n",
    "    return sentiment.polarity_scores(text)\n",
    "\n",
    "\n",
    "updated_dataset['Vader Analysis'] = updated_dataset['Cleaned Text'].apply(\n",
    "    getSentiment)\n",
    "\n",
    "\n",
    "def sentimentAnalysis(sentiment_dict):\n",
    "    if sentiment_dict['compound'] >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif sentiment_dict['compound'] <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "\n",
    "updated_dataset['Vader Sentiment'] = updated_dataset['Vader Analysis'].apply(\n",
    "    sentimentAnalysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vader Analysis</th>\n",
       "      <th>Vader Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.638, 'pos': 0.362, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.816, 'pos': 0.184, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.874, 'pos': 0.126, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.819, 'pos': 0.181, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21888</th>\n",
       "      <td>{'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21889</th>\n",
       "      <td>{'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21890</th>\n",
       "      <td>{'neg': 0.074, 'neu': 0.719, 'pos': 0.206, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21915</th>\n",
       "      <td>{'neg': 0.043, 'neu': 0.774, 'pos': 0.183, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21962</th>\n",
       "      <td>{'neg': 0.196, 'neu': 0.777, 'pos': 0.028, 'co...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5398 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Vader Analysis Vader Sentiment\n",
       "0      {'neg': 0.0, 'neu': 0.638, 'pos': 0.362, 'comp...        Positive\n",
       "1      {'neg': 0.0, 'neu': 0.816, 'pos': 0.184, 'comp...        Positive\n",
       "2      {'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...        Positive\n",
       "3      {'neg': 0.0, 'neu': 0.874, 'pos': 0.126, 'comp...        Positive\n",
       "4      {'neg': 0.0, 'neu': 0.819, 'pos': 0.181, 'comp...        Positive\n",
       "...                                                  ...             ...\n",
       "21888  {'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...        Positive\n",
       "21889  {'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...        Positive\n",
       "21890  {'neg': 0.074, 'neu': 0.719, 'pos': 0.206, 'co...        Positive\n",
       "21915  {'neg': 0.043, 'neu': 0.774, 'pos': 0.183, 'co...        Positive\n",
       "21962  {'neg': 0.196, 'neu': 0.777, 'pos': 0.028, 'co...        Negative\n",
       "\n",
       "[5398 rows x 2 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset[['Vader Analysis', 'Vader Sentiment']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive    3335\n",
       "Neutral     1691\n",
       "Negative     372\n",
       "Name: Vader Sentiment, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset['Vader Sentiment'].value_counts()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis between Vader and TextBlob\n",
    "\n",
    "\n",
    "| Data Labelling Method      | Positive Label | Negative Label | Neutral Label |\n",
    "| ----------- | ----------- | ----------- | ----------- |\n",
    "| TextBlob      | 2643   (49%)   |479  (8.9%)| 2276 (42.1%)|\n",
    "| Vader   | 3335  (61.8%)      | 372 (6.9%) | 1691 (31.3%)|\n",
    "\n",
    "From the summary table, we can observe that the negatively labelled tweets stay within a 2% difference of each other, regardless of the data labelling method. However, the positive and neutral label differ by 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a csv of tweets with conflicting sentiments\n",
    "unclearTweets = updated_dataset.loc[(\n",
    "    updated_dataset['Vader Sentiment'] != updated_dataset['TextBlob Sentiment'])]\n",
    "unclearTweets.to_csv('ConflictingLabels.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cleaned Text</th>\n",
       "      <th>TextBlob Subjectivity</th>\n",
       "      <th>TextBlob Polarity</th>\n",
       "      <th>TextBlob Sentiment</th>\n",
       "      <th>Vader Analysis</th>\n",
       "      <th>Vader Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Foreign Secretary  participated in a meeting...</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>-0.062500</td>\n",
       "      <td>Negative</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.816, 'pos': 0.184, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'RT  Did you know that the UAE is developing ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.874, 'pos': 0.126, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'RT  \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\x8f UAE lunar r...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>Negative</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.819, 'pos': 0.181, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>b'RT  UK BUSINESS SECRETARY PRAISES UK-UAE GRA...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.484, 'pos': 0.516, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>b'RT  As host of COP28, UAE is committed to le...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.817, 'pos': 0.183, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21888</th>\n",
       "      <td>b'   Dear sir please my help you sir my two ki...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>{'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21889</th>\n",
       "      <td>b' Dear sir please my help you sir my two kidn...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>{'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21890</th>\n",
       "      <td>b' Dear sir please my help you sir my two kidn...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>{'neg': 0.074, 'neu': 0.719, 'pos': 0.206, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21915</th>\n",
       "      <td>b\" launched a service on Tuesday that will all...</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>-0.062500</td>\n",
       "      <td>Negative</td>\n",
       "      <td>{'neg': 0.043, 'neu': 0.774, 'pos': 0.183, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21962</th>\n",
       "      <td>b' Saudi k log bhikhari h ? \\nIn Middle East, ...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>Positive</td>\n",
       "      <td>{'neg': 0.196, 'neu': 0.777, 'pos': 0.028, 'co...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2257 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Cleaned Text  \\\n",
       "1      b'Foreign Secretary  participated in a meeting...   \n",
       "3      b'RT  Did you know that the UAE is developing ...   \n",
       "4      b'RT  \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\x8f UAE lunar r...   \n",
       "10     b'RT  UK BUSINESS SECRETARY PRAISES UK-UAE GRA...   \n",
       "14     b'RT  As host of COP28, UAE is committed to le...   \n",
       "...                                                  ...   \n",
       "21888  b'   Dear sir please my help you sir my two ki...   \n",
       "21889  b' Dear sir please my help you sir my two kidn...   \n",
       "21890  b' Dear sir please my help you sir my two kidn...   \n",
       "21915  b\" launched a service on Tuesday that will all...   \n",
       "21962  b' Saudi k log bhikhari h ? \\nIn Middle East, ...   \n",
       "\n",
       "       TextBlob Subjectivity  TextBlob Polarity TextBlob Sentiment  \\\n",
       "1                   0.562500          -0.062500           Negative   \n",
       "3                   0.000000           0.000000            Neutral   \n",
       "4                   1.000000          -0.600000           Negative   \n",
       "10                  0.000000           0.000000            Neutral   \n",
       "14                  0.125000           0.000000            Neutral   \n",
       "...                      ...                ...                ...   \n",
       "21888               0.100000           0.000000            Neutral   \n",
       "21889               0.100000           0.000000            Neutral   \n",
       "21890               0.100000           0.000000            Neutral   \n",
       "21915               0.062500          -0.062500           Negative   \n",
       "21962               0.333333           0.333333           Positive   \n",
       "\n",
       "                                          Vader Analysis Vader Sentiment  \n",
       "1      {'neg': 0.0, 'neu': 0.816, 'pos': 0.184, 'comp...        Positive  \n",
       "3      {'neg': 0.0, 'neu': 0.874, 'pos': 0.126, 'comp...        Positive  \n",
       "4      {'neg': 0.0, 'neu': 0.819, 'pos': 0.181, 'comp...        Positive  \n",
       "10     {'neg': 0.0, 'neu': 0.484, 'pos': 0.516, 'comp...        Positive  \n",
       "14     {'neg': 0.0, 'neu': 0.817, 'pos': 0.183, 'comp...        Positive  \n",
       "...                                                  ...             ...  \n",
       "21888  {'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...        Positive  \n",
       "21889  {'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...        Positive  \n",
       "21890  {'neg': 0.074, 'neu': 0.719, 'pos': 0.206, 'co...        Positive  \n",
       "21915  {'neg': 0.043, 'neu': 0.774, 'pos': 0.183, 'co...        Positive  \n",
       "21962  {'neg': 0.196, 'neu': 0.777, 'pos': 0.028, 'co...        Negative  \n",
       "\n",
       "[2257 rows x 6 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unclearTweets\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After manually reviewing the differences in labels, it seems that the TextBlob sentiment labelling is more accurate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**approach → first data cleaning + tokenization + lemmatization**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Non-English Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    b'Joined UAE Minister &amp; President-Designat...\n",
       "1    b'Foreign Secretary  participated in a meeting...\n",
       "2    b'As the UAE marks National Environment Day, w...\n",
       "3    b'RT  Did you know that the UAE is developing ...\n",
       "4    b'RT   UAE lunar rover will test 1st artificia...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nonenglish(tweet):\n",
    "    # tweets in different language changed to hex code \\xhh\n",
    "    tweet = re.sub(r'\\\\x[a-zA-Z0-9]+', '', tweet)\n",
    "    tweet = re.sub(r'\\\\n', '', tweet)  # removing new line character as well\n",
    "    return tweet\n",
    "\n",
    "\n",
    "updated_dataset['Cleaned Text'] = updated_dataset['Cleaned Text'].apply(\n",
    "    lambda tweet: nonenglish(tweet))\n",
    "updated_dataset['Cleaned Text'].head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    b'Joined UAE Minister &amp; President-Designat...\n",
       "1    b'Foreign Secretary  participated meeting foca...\n",
       "2    b'As UAE marks National Environment Day, remai...\n",
       "3    b'RT  Did know UAE developing Arabic ChatGPT u...\n",
       "4    b'RT   UAE lunar rover test 1st artificial int...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "# contains 318 stopwords, including but not limited to articles and prepositions\n",
    "stopwords = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "updated_dataset['Cleaned Text'] = updated_dataset['Cleaned Text'].apply(\n",
    "    lambda tweet: ' '.join([text for text in tweet.split(' ') if text not in stopwords]))\n",
    "updated_dataset['Cleaned Text'].head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Symbols and Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    bJoined UAE Minister amp PresidentDesignate  H...\n",
       "1    bForeign Secretary  participated meeting focal...\n",
       "2    bAs UAE marks National Environment Day remain ...\n",
       "3    bRT  Did know UAE developing Arabic ChatGPT us...\n",
       "4    bRT   UAE lunar rover test 1st artificial inte...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "punctuations = string.punctuation\n",
    "print(punctuations)\n",
    "\n",
    "updated_dataset['Cleaned Text'] = updated_dataset['Cleaned Text'].apply(\n",
    "    lambda tweet: tweet.translate({ord(punc): None for punc in punctuations}))\n",
    "updated_dataset['Cleaned Text'].head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    bJoined UAE Minister amp PresidentDesignate  H...\n",
       "1    bForeign Secretary  participated meeting focal...\n",
       "2    bAs UAE marks National Environment Day remain ...\n",
       "3    bRT  Did know UAE developing Arabic ChatGPT us...\n",
       "4    bRT   UAE lunar rover test st artificial intel...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset['Cleaned Text'] = updated_dataset['Cleaned Text'].apply(\n",
    "    lambda tweet: re.sub('[0-9]+', '', tweet))\n",
    "updated_dataset['Cleaned Text'].head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing 'b' and 'RT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Joined UAE Minister amp PresidentDesignate  HE...\n",
       "1    Foreign Secretary  participated meeting focal ...\n",
       "2    As UAE marks National Environment Day remain c...\n",
       "3      Did know UAE developing Araic ChatGPT using ...\n",
       "4       UAE lunar rover test st artificial intellig...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset['Cleaned Text'] = updated_dataset['Cleaned Text'].apply(\n",
    "    lambda tweet: re.sub('bRT|b', '', tweet))\n",
    "updated_dataset['Cleaned Text'].head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Joined, UAE, Minister, amp, PresidentDesignat...\n",
       "1    [Foreign, Secretary, participated, meeting, fo...\n",
       "2    [As, UAE, marks, National, Environment, Day, r...\n",
       "3    [Did, know, UAE, developing, Araic, ChatGPT, u...\n",
       "4    [UAE, lunar, rover, test, st, artificial, inte...\n",
       "Name: Tokenized Text, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as data has been completely cleaned, we can simply tokenize by converting strings to lists\n",
    "updated_dataset['Tokenized Text'] = updated_dataset['Cleaned Text'].apply(\n",
    "    lambda tweet: tweet.split())\n",
    "updated_dataset['Tokenized Text'].head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [joined, uae, minister, amp, presidentdesignat...\n",
       "1    [foreign, secretary, participated, meeting, fo...\n",
       "2    [as, uae, mark, national, environment, day, re...\n",
       "3    [did, know, uae, developing, araic, chatgpt, u...\n",
       "4    [uae, lunar, rover, test, st, artificial, inte...\n",
       "Name: Lemmatized Text, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "lemmatizing = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "updated_dataset['Lemmatized Text'] = updated_dataset['Tokenized Text'].apply(\n",
    "    lambda tweet: [lemmatizing.lemmatize(text).lower() for text in tweet])\n",
    "updated_dataset['Lemmatized Text'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode TextBlob Sentiments into numbers\n",
    "updated_dataset['Encoded_TextBlob_Sentiment'] = updated_dataset['TextBlob Sentiment'].apply(\n",
    "    lambda x: 1 if x == 'Positive' else -1 if x == 'Negative' else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cleaned Text</th>\n",
       "      <th>TextBlob Subjectivity</th>\n",
       "      <th>TextBlob Polarity</th>\n",
       "      <th>TextBlob Sentiment</th>\n",
       "      <th>Vader Analysis</th>\n",
       "      <th>Vader Sentiment</th>\n",
       "      <th>Tokenized Text</th>\n",
       "      <th>Lemmatized Text</th>\n",
       "      <th>Encoded_TextBlob_Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joined UAE Minister amp PresidentDesignate  HE...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>Positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.638, 'pos': 0.362, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[Joined, UAE, Minister, amp, PresidentDesignat...</td>\n",
       "      <td>[joined, uae, minister, amp, presidentdesignat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Foreign Secretary  participated meeting focal ...</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>-0.062500</td>\n",
       "      <td>Negative</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.816, 'pos': 0.184, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[Foreign, Secretary, participated, meeting, fo...</td>\n",
       "      <td>[foreign, secretary, participated, meeting, fo...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As UAE marks National Environment Day remain c...</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>Positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[As, UAE, marks, National, Environment, Day, r...</td>\n",
       "      <td>[as, uae, mark, national, environment, day, re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Did know UAE developing Araic ChatGPT using ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.874, 'pos': 0.126, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[Did, know, UAE, developing, Araic, ChatGPT, u...</td>\n",
       "      <td>[did, know, uae, developing, araic, chatgpt, u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UAE lunar rover test st artificial intellig...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>Negative</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.819, 'pos': 0.181, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[UAE, lunar, rover, test, st, artificial, inte...</td>\n",
       "      <td>[uae, lunar, rover, test, st, artificial, inte...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5393</th>\n",
       "      <td>Dear sir help sir kidney Damage Doctor disc...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>{'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[Dear, sir, help, sir, kidney, Damage, Doctor,...</td>\n",
       "      <td>[dear, sir, help, sir, kidney, damage, doctor,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5394</th>\n",
       "      <td>Dear sir help sir kidney Damage Doctor discus...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>{'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[Dear, sir, help, sir, kidney, Damage, Doctor,...</td>\n",
       "      <td>[dear, sir, help, sir, kidney, damage, doctor,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5395</th>\n",
       "      <td>Dear sir help sir kidney Damage Doctor discus...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>{'neg': 0.074, 'neu': 0.719, 'pos': 0.206, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[Dear, sir, help, sir, kidney, Damage, Doctor,...</td>\n",
       "      <td>[dear, sir, help, sir, kidney, damage, doctor,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5396</th>\n",
       "      <td>launched service Tuesday allow Indian users t...</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>-0.062500</td>\n",
       "      <td>Negative</td>\n",
       "      <td>{'neg': 0.043, 'neu': 0.774, 'pos': 0.183, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[launched, service, Tuesday, allow, Indian, us...</td>\n",
       "      <td>[launched, service, tuesday, allow, indian, us...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5397</th>\n",
       "      <td>Saudi k log hikhari h  In Middle East UAE num...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>Positive</td>\n",
       "      <td>{'neg': 0.196, 'neu': 0.777, 'pos': 0.028, 'co...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[Saudi, k, log, hikhari, h, In, Middle, East, ...</td>\n",
       "      <td>[saudi, k, log, hikhari, h, in, middle, east, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5398 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Cleaned Text  \\\n",
       "0     Joined UAE Minister amp PresidentDesignate  HE...   \n",
       "1     Foreign Secretary  participated meeting focal ...   \n",
       "2     As UAE marks National Environment Day remain c...   \n",
       "3       Did know UAE developing Araic ChatGPT using ...   \n",
       "4        UAE lunar rover test st artificial intellig...   \n",
       "...                                                 ...   \n",
       "5393     Dear sir help sir kidney Damage Doctor disc...   \n",
       "5394   Dear sir help sir kidney Damage Doctor discus...   \n",
       "5395   Dear sir help sir kidney Damage Doctor discus...   \n",
       "5396   launched service Tuesday allow Indian users t...   \n",
       "5397   Saudi k log hikhari h  In Middle East UAE num...   \n",
       "\n",
       "      TextBlob Subjectivity  TextBlob Polarity TextBlob Sentiment  \\\n",
       "0                  0.200000           0.133333           Positive   \n",
       "1                  0.562500          -0.062500           Negative   \n",
       "2                  0.525000           0.166667           Positive   \n",
       "3                  0.000000           0.000000            Neutral   \n",
       "4                  1.000000          -0.600000           Negative   \n",
       "...                     ...                ...                ...   \n",
       "5393               0.100000           0.000000            Neutral   \n",
       "5394               0.100000           0.000000            Neutral   \n",
       "5395               0.100000           0.000000            Neutral   \n",
       "5396               0.062500          -0.062500           Negative   \n",
       "5397               0.333333           0.333333           Positive   \n",
       "\n",
       "                                         Vader Analysis Vader Sentiment  \\\n",
       "0     {'neg': 0.0, 'neu': 0.638, 'pos': 0.362, 'comp...        Positive   \n",
       "1     {'neg': 0.0, 'neu': 0.816, 'pos': 0.184, 'comp...        Positive   \n",
       "2     {'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...        Positive   \n",
       "3     {'neg': 0.0, 'neu': 0.874, 'pos': 0.126, 'comp...        Positive   \n",
       "4     {'neg': 0.0, 'neu': 0.819, 'pos': 0.181, 'comp...        Positive   \n",
       "...                                                 ...             ...   \n",
       "5393  {'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...        Positive   \n",
       "5394  {'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...        Positive   \n",
       "5395  {'neg': 0.074, 'neu': 0.719, 'pos': 0.206, 'co...        Positive   \n",
       "5396  {'neg': 0.043, 'neu': 0.774, 'pos': 0.183, 'co...        Positive   \n",
       "5397  {'neg': 0.196, 'neu': 0.777, 'pos': 0.028, 'co...        Negative   \n",
       "\n",
       "                                         Tokenized Text  \\\n",
       "0     [Joined, UAE, Minister, amp, PresidentDesignat...   \n",
       "1     [Foreign, Secretary, participated, meeting, fo...   \n",
       "2     [As, UAE, marks, National, Environment, Day, r...   \n",
       "3     [Did, know, UAE, developing, Araic, ChatGPT, u...   \n",
       "4     [UAE, lunar, rover, test, st, artificial, inte...   \n",
       "...                                                 ...   \n",
       "5393  [Dear, sir, help, sir, kidney, Damage, Doctor,...   \n",
       "5394  [Dear, sir, help, sir, kidney, Damage, Doctor,...   \n",
       "5395  [Dear, sir, help, sir, kidney, Damage, Doctor,...   \n",
       "5396  [launched, service, Tuesday, allow, Indian, us...   \n",
       "5397  [Saudi, k, log, hikhari, h, In, Middle, East, ...   \n",
       "\n",
       "                                        Lemmatized Text  \\\n",
       "0     [joined, uae, minister, amp, presidentdesignat...   \n",
       "1     [foreign, secretary, participated, meeting, fo...   \n",
       "2     [as, uae, mark, national, environment, day, re...   \n",
       "3     [did, know, uae, developing, araic, chatgpt, u...   \n",
       "4     [uae, lunar, rover, test, st, artificial, inte...   \n",
       "...                                                 ...   \n",
       "5393  [dear, sir, help, sir, kidney, damage, doctor,...   \n",
       "5394  [dear, sir, help, sir, kidney, damage, doctor,...   \n",
       "5395  [dear, sir, help, sir, kidney, damage, doctor,...   \n",
       "5396  [launched, service, tuesday, allow, indian, us...   \n",
       "5397  [saudi, k, log, hikhari, h, in, middle, east, ...   \n",
       "\n",
       "      Encoded_TextBlob_Sentiment  \n",
       "0                              1  \n",
       "1                             -1  \n",
       "2                              1  \n",
       "3                              0  \n",
       "4                             -1  \n",
       "...                          ...  \n",
       "5393                           0  \n",
       "5394                           0  \n",
       "5395                           0  \n",
       "5396                          -1  \n",
       "5397                           1  \n",
       "\n",
       "[5398 rows x 9 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resets the index numbers of the dataframe\n",
    "updated_dataset.reset_index(inplace=True, drop=True)\n",
    "updated_dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Representation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section experiments with different document representations, namely:\n",
    "- Bag of Words\n",
    "- N-grams\n",
    "- TF-IDF\n",
    "- CBOW\n",
    "- Skip-gram\n",
    "- Pre-trained Word2Vec model by Google\n",
    "\n",
    "\n",
    "Each of the representations are tested using Naive Bayes, and the high performance models are used in the next stage of the pipeline.\n",
    "\n",
    "Note: Arbitrarily using lemmatization and textblob sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting train and test sets ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = updated_dataset['Lemmatized Text']\n",
    "y = updated_dataset['Encoded_TextBlob_Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    updated_dataset['Lemmatized Text'],\n",
    "    updated_dataset['Encoded_TextBlob_Sentiment'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=updated_dataset['Encoded_TextBlob_Sentiment']\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.71      0.28      0.40        96\n",
      "           0       0.85      0.58      0.69       455\n",
      "           1       0.67      0.92      0.77       529\n",
      "\n",
      "    accuracy                           0.72      1080\n",
      "   macro avg       0.74      0.60      0.62      1080\n",
      "weighted avg       0.75      0.72      0.71      1080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bow = CountVectorizer(\n",
    "    ngram_range=(1, 1),  # unigram\n",
    "    preprocessor=lambda x: x,  # override preprocessing cause already done\n",
    "    tokenizer=lambda x: x,  # override tokenization cause already done\n",
    ")\n",
    "\n",
    "nb_bow = Pipeline([\n",
    "    ('bow', bow),\n",
    "    ('naive_bayes', MultinomialNB())\n",
    "])\n",
    "\n",
    "nb_bow.fit(X_train, y_train)\n",
    "y_pred_bow = nb_bow.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_bow))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "The n-grams tested are $(a,b)$, which denotes a vector representation where a-grams up until b-grams are used.\n",
    "E.g. When the n-gram range is (1,3), the vector representation of the document includes unigrams, bigrams, and trigrams. \n",
    "\n",
    "If the range is $(a,a)$, only a-grams are used. E.g. (1,1) denotes unigrams.\n",
    "\n",
    "Possible n-gram values that are tested are:\n",
    "$\\text{n-gram values}=\\{(a,b) | a \\in \\{1,2,...,9\\} \\cap b \\in \\{1,2,...,9\\} \\cap (a <= b)\\}$\n",
    "\n",
    "It is observed that the n-gram range (1,6) provides the best macro-f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "A = np.arange(1, 10)\n",
    "B = np.arange(1, 10)\n",
    "\n",
    "# Generate possible n-gram pairs\n",
    "ngram_pairs = [(a, b) for a in A for b in B if a <= b]\n",
    "\n",
    "results_ngram = {}\n",
    "for ngram_range in ngram_pairs:\n",
    "    ngram = CountVectorizer(\n",
    "        ngram_range=ngram_range,\n",
    "        preprocessor=lambda x: x,  # override preprocessing\n",
    "        tokenizer=lambda x: x,  # override tokenization\n",
    "    )\n",
    "\n",
    "    ngram_nb = Pipeline([\n",
    "        ('ngram', ngram),\n",
    "        ('naive_bayes', MultinomialNB())\n",
    "    ])\n",
    "\n",
    "    ngram_nb.fit(X_train, y_train)\n",
    "    ngram_nb_y_pred = ngram_nb.predict(X_test)\n",
    "\n",
    "    prec, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_test, ngram_nb_y_pred, beta=1.0, average='macro')\n",
    "    results_ngram[ngram_range] = {\n",
    "        'precision': prec, 'recall': recall, 'f1-score': f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision    (1, 3)\n",
       "recall       (1, 6)\n",
       "f1-score     (1, 6)\n",
       "dtype: object"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_results = pd.DataFrame.from_dict(results_ngram, orient='index', columns=[\n",
    "                                       'precision', 'recall', 'f1-score'])\n",
    "ngram_results.index.names = ['min_ngram', 'max_ngram']\n",
    "# get n-grams corresponding to max f1 score\n",
    "ngram_results.idxmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_ngram</th>\n",
       "      <th>max_ngram</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.744011</td>\n",
       "      <td>0.595388</td>\n",
       "      <td>0.623261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.776213</td>\n",
       "      <td>0.624163</td>\n",
       "      <td>0.656949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.778171</td>\n",
       "      <td>0.633833</td>\n",
       "      <td>0.666656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.768264</td>\n",
       "      <td>0.636147</td>\n",
       "      <td>0.667612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.760271</td>\n",
       "      <td>0.637612</td>\n",
       "      <td>0.667404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.761317</td>\n",
       "      <td>0.638243</td>\n",
       "      <td>0.668040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.761317</td>\n",
       "      <td>0.638243</td>\n",
       "      <td>0.668040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.761317</td>\n",
       "      <td>0.638243</td>\n",
       "      <td>0.668040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.761317</td>\n",
       "      <td>0.638243</td>\n",
       "      <td>0.668040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">2</th>\n",
       "      <th>2</th>\n",
       "      <td>0.703705</td>\n",
       "      <td>0.616992</td>\n",
       "      <td>0.643597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.695476</td>\n",
       "      <td>0.616362</td>\n",
       "      <td>0.641273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.695689</td>\n",
       "      <td>0.616362</td>\n",
       "      <td>0.641320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.699136</td>\n",
       "      <td>0.616362</td>\n",
       "      <td>0.642112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.699891</td>\n",
       "      <td>0.616992</td>\n",
       "      <td>0.642742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.699891</td>\n",
       "      <td>0.616992</td>\n",
       "      <td>0.642742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.699891</td>\n",
       "      <td>0.616992</td>\n",
       "      <td>0.642742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.699891</td>\n",
       "      <td>0.616992</td>\n",
       "      <td>0.642742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">3</th>\n",
       "      <th>3</th>\n",
       "      <td>0.716251</td>\n",
       "      <td>0.607867</td>\n",
       "      <td>0.634091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.716762</td>\n",
       "      <td>0.608600</td>\n",
       "      <td>0.634875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.719255</td>\n",
       "      <td>0.607969</td>\n",
       "      <td>0.635058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.719255</td>\n",
       "      <td>0.607969</td>\n",
       "      <td>0.635058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.719255</td>\n",
       "      <td>0.607969</td>\n",
       "      <td>0.635058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.718739</td>\n",
       "      <td>0.607237</td>\n",
       "      <td>0.634275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.718739</td>\n",
       "      <td>0.607237</td>\n",
       "      <td>0.634275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">4</th>\n",
       "      <th>4</th>\n",
       "      <td>0.718298</td>\n",
       "      <td>0.589320</td>\n",
       "      <td>0.614002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.721500</td>\n",
       "      <td>0.592792</td>\n",
       "      <td>0.617818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.721500</td>\n",
       "      <td>0.592792</td>\n",
       "      <td>0.617818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.721974</td>\n",
       "      <td>0.593525</td>\n",
       "      <td>0.618640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.721974</td>\n",
       "      <td>0.593525</td>\n",
       "      <td>0.618640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.721500</td>\n",
       "      <td>0.592792</td>\n",
       "      <td>0.617818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">5</th>\n",
       "      <th>5</th>\n",
       "      <td>0.732348</td>\n",
       "      <td>0.574538</td>\n",
       "      <td>0.598222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.732792</td>\n",
       "      <td>0.575270</td>\n",
       "      <td>0.599076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.732348</td>\n",
       "      <td>0.574538</td>\n",
       "      <td>0.598222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.732348</td>\n",
       "      <td>0.574538</td>\n",
       "      <td>0.598222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.732348</td>\n",
       "      <td>0.574538</td>\n",
       "      <td>0.598222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">6</th>\n",
       "      <th>6</th>\n",
       "      <td>0.731875</td>\n",
       "      <td>0.557748</td>\n",
       "      <td>0.578741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.731875</td>\n",
       "      <td>0.557748</td>\n",
       "      <td>0.578741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.731875</td>\n",
       "      <td>0.557748</td>\n",
       "      <td>0.578741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.730415</td>\n",
       "      <td>0.557118</td>\n",
       "      <td>0.578145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">7</th>\n",
       "      <th>7</th>\n",
       "      <td>0.720098</td>\n",
       "      <td>0.539068</td>\n",
       "      <td>0.555661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.718542</td>\n",
       "      <td>0.538438</td>\n",
       "      <td>0.555073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.718542</td>\n",
       "      <td>0.538438</td>\n",
       "      <td>0.555073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">8</th>\n",
       "      <th>8</th>\n",
       "      <td>0.708012</td>\n",
       "      <td>0.506673</td>\n",
       "      <td>0.515449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.708012</td>\n",
       "      <td>0.506673</td>\n",
       "      <td>0.515449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <th>9</th>\n",
       "      <td>0.699506</td>\n",
       "      <td>0.479202</td>\n",
       "      <td>0.478806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     precision    recall  f1-score\n",
       "min_ngram max_ngram                               \n",
       "1         1           0.744011  0.595388  0.623261\n",
       "          2           0.776213  0.624163  0.656949\n",
       "          3           0.778171  0.633833  0.666656\n",
       "          4           0.768264  0.636147  0.667612\n",
       "          5           0.760271  0.637612  0.667404\n",
       "          6           0.761317  0.638243  0.668040\n",
       "          7           0.761317  0.638243  0.668040\n",
       "          8           0.761317  0.638243  0.668040\n",
       "          9           0.761317  0.638243  0.668040\n",
       "2         2           0.703705  0.616992  0.643597\n",
       "          3           0.695476  0.616362  0.641273\n",
       "          4           0.695689  0.616362  0.641320\n",
       "          5           0.699136  0.616362  0.642112\n",
       "          6           0.699891  0.616992  0.642742\n",
       "          7           0.699891  0.616992  0.642742\n",
       "          8           0.699891  0.616992  0.642742\n",
       "          9           0.699891  0.616992  0.642742\n",
       "3         3           0.716251  0.607867  0.634091\n",
       "          4           0.716762  0.608600  0.634875\n",
       "          5           0.719255  0.607969  0.635058\n",
       "          6           0.719255  0.607969  0.635058\n",
       "          7           0.719255  0.607969  0.635058\n",
       "          8           0.718739  0.607237  0.634275\n",
       "          9           0.718739  0.607237  0.634275\n",
       "4         4           0.718298  0.589320  0.614002\n",
       "          5           0.721500  0.592792  0.617818\n",
       "          6           0.721500  0.592792  0.617818\n",
       "          7           0.721974  0.593525  0.618640\n",
       "          8           0.721974  0.593525  0.618640\n",
       "          9           0.721500  0.592792  0.617818\n",
       "5         5           0.732348  0.574538  0.598222\n",
       "          6           0.732792  0.575270  0.599076\n",
       "          7           0.732348  0.574538  0.598222\n",
       "          8           0.732348  0.574538  0.598222\n",
       "          9           0.732348  0.574538  0.598222\n",
       "6         6           0.731875  0.557748  0.578741\n",
       "          7           0.731875  0.557748  0.578741\n",
       "          8           0.731875  0.557748  0.578741\n",
       "          9           0.730415  0.557118  0.578145\n",
       "7         7           0.720098  0.539068  0.555661\n",
       "          8           0.718542  0.538438  0.555073\n",
       "          9           0.718542  0.538438  0.555073\n",
       "8         8           0.708012  0.506673  0.515449\n",
       "          9           0.708012  0.506673  0.515449\n",
       "9         9           0.699506  0.479202  0.478806"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_results\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "The TF-IDF vectorizor is tested over several n-gram ranges. The ranges are: $\\text{n-gram values}=\\{(a,b) | a \\in \\{1,2,...,9\\} \\cap b \\in \\{1,2,...,9\\} \\cap (a <= b)\\}$.\n",
    "\n",
    "It is observed that the tfidf vector with n-gram range (2,3) provides the best macro-f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "A = np.arange(1, 10)\n",
    "B = np.arange(1, 10)\n",
    "\n",
    "# Generate possible n-gram pairs\n",
    "ngram_pairs = [(a, b) for a in A for b in B if a <= b]\n",
    "\n",
    "results_tfidf = {}\n",
    "for ngram_range in ngram_pairs:\n",
    "    tfidf = TfidfVectorizer(\n",
    "        preprocessor=lambda x: x,  # override preprocessing\n",
    "        tokenizer=lambda x: x,  # override tokenization\n",
    "        ngram_range=ngram_range\n",
    "    )\n",
    "\n",
    "    tfidf_nb = Pipeline([\n",
    "        ('tfidf', tfidf),\n",
    "        ('naive_bayes', MultinomialNB())\n",
    "    ])\n",
    "\n",
    "    tfidf_nb.fit(X_train, y_train)\n",
    "    tfidf_nb_y_pred = tfidf_nb.predict(X_test)\n",
    "    prec, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_test, tfidf_nb_y_pred, beta=1.0, average='macro')\n",
    "    results_tfidf[ngram_range] = {\n",
    "        'precision': prec, 'recall': recall, 'f1-score': f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision    (1, 4)\n",
       "recall       (1, 4)\n",
       "f1-score     (2, 3)\n",
       "dtype: object"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_results = pd.DataFrame.from_dict(results_tfidf, orient='index', columns=[\n",
    "                                       'precision', 'recall', 'f1-score'])\n",
    "tfidf_results.index.names = ['min_ngram', 'max_ngram']\n",
    "# get n-grams corresponding to max f1 score\n",
    "tfidf_results.idxmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_ngram</th>\n",
       "      <th>max_ngram</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.840404</td>\n",
       "      <td>0.534808</td>\n",
       "      <td>0.540647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.848358</td>\n",
       "      <td>0.544962</td>\n",
       "      <td>0.551621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.848872</td>\n",
       "      <td>0.551467</td>\n",
       "      <td>0.560676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.851383</td>\n",
       "      <td>0.560170</td>\n",
       "      <td>0.571980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.848805</td>\n",
       "      <td>0.558178</td>\n",
       "      <td>0.569967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.848156</td>\n",
       "      <td>0.558280</td>\n",
       "      <td>0.570094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.844364</td>\n",
       "      <td>0.556492</td>\n",
       "      <td>0.568337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.843167</td>\n",
       "      <td>0.556697</td>\n",
       "      <td>0.568583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.843468</td>\n",
       "      <td>0.558265</td>\n",
       "      <td>0.570198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">2</th>\n",
       "      <th>2</th>\n",
       "      <td>0.815706</td>\n",
       "      <td>0.555756</td>\n",
       "      <td>0.572937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.794665</td>\n",
       "      <td>0.556064</td>\n",
       "      <td>0.573077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.795223</td>\n",
       "      <td>0.555331</td>\n",
       "      <td>0.572561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.795223</td>\n",
       "      <td>0.555331</td>\n",
       "      <td>0.572561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.817078</td>\n",
       "      <td>0.555229</td>\n",
       "      <td>0.572929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.818145</td>\n",
       "      <td>0.549545</td>\n",
       "      <td>0.562975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.818489</td>\n",
       "      <td>0.549442</td>\n",
       "      <td>0.562859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.817971</td>\n",
       "      <td>0.548709</td>\n",
       "      <td>0.562124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">3</th>\n",
       "      <th>3</th>\n",
       "      <td>0.815290</td>\n",
       "      <td>0.533663</td>\n",
       "      <td>0.546693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.795003</td>\n",
       "      <td>0.538923</td>\n",
       "      <td>0.553380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.792883</td>\n",
       "      <td>0.535451</td>\n",
       "      <td>0.547788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.791359</td>\n",
       "      <td>0.533150</td>\n",
       "      <td>0.545197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.820266</td>\n",
       "      <td>0.535773</td>\n",
       "      <td>0.548283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.819241</td>\n",
       "      <td>0.535143</td>\n",
       "      <td>0.547675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.818469</td>\n",
       "      <td>0.530938</td>\n",
       "      <td>0.541159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">4</th>\n",
       "      <th>4</th>\n",
       "      <td>0.817540</td>\n",
       "      <td>0.523642</td>\n",
       "      <td>0.535187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.816789</td>\n",
       "      <td>0.521532</td>\n",
       "      <td>0.530587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.816789</td>\n",
       "      <td>0.521532</td>\n",
       "      <td>0.530587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.816789</td>\n",
       "      <td>0.521532</td>\n",
       "      <td>0.530587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.817902</td>\n",
       "      <td>0.522162</td>\n",
       "      <td>0.531187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.820148</td>\n",
       "      <td>0.523423</td>\n",
       "      <td>0.532389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">5</th>\n",
       "      <th>5</th>\n",
       "      <td>0.819089</td>\n",
       "      <td>0.513284</td>\n",
       "      <td>0.520406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.819089</td>\n",
       "      <td>0.513284</td>\n",
       "      <td>0.520406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.818834</td>\n",
       "      <td>0.509812</td>\n",
       "      <td>0.514577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.819616</td>\n",
       "      <td>0.509709</td>\n",
       "      <td>0.514328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.819363</td>\n",
       "      <td>0.506237</td>\n",
       "      <td>0.508392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">6</th>\n",
       "      <th>6</th>\n",
       "      <td>0.817659</td>\n",
       "      <td>0.499336</td>\n",
       "      <td>0.499927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.817659</td>\n",
       "      <td>0.499336</td>\n",
       "      <td>0.499927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.817659</td>\n",
       "      <td>0.499336</td>\n",
       "      <td>0.499927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.817415</td>\n",
       "      <td>0.495864</td>\n",
       "      <td>0.493884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">7</th>\n",
       "      <th>7</th>\n",
       "      <td>0.815303</td>\n",
       "      <td>0.477081</td>\n",
       "      <td>0.464636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.477184</td>\n",
       "      <td>0.464853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.812094</td>\n",
       "      <td>0.473711</td>\n",
       "      <td>0.458244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">8</th>\n",
       "      <th>8</th>\n",
       "      <td>0.805167</td>\n",
       "      <td>0.451529</td>\n",
       "      <td>0.430413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.805167</td>\n",
       "      <td>0.451529</td>\n",
       "      <td>0.430413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <th>9</th>\n",
       "      <td>0.795174</td>\n",
       "      <td>0.431002</td>\n",
       "      <td>0.400267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     precision    recall  f1-score\n",
       "min_ngram max_ngram                               \n",
       "1         1           0.840404  0.534808  0.540647\n",
       "          2           0.848358  0.544962  0.551621\n",
       "          3           0.848872  0.551467  0.560676\n",
       "          4           0.851383  0.560170  0.571980\n",
       "          5           0.848805  0.558178  0.569967\n",
       "          6           0.848156  0.558280  0.570094\n",
       "          7           0.844364  0.556492  0.568337\n",
       "          8           0.843167  0.556697  0.568583\n",
       "          9           0.843468  0.558265  0.570198\n",
       "2         2           0.815706  0.555756  0.572937\n",
       "          3           0.794665  0.556064  0.573077\n",
       "          4           0.795223  0.555331  0.572561\n",
       "          5           0.795223  0.555331  0.572561\n",
       "          6           0.817078  0.555229  0.572929\n",
       "          7           0.818145  0.549545  0.562975\n",
       "          8           0.818489  0.549442  0.562859\n",
       "          9           0.817971  0.548709  0.562124\n",
       "3         3           0.815290  0.533663  0.546693\n",
       "          4           0.795003  0.538923  0.553380\n",
       "          5           0.792883  0.535451  0.547788\n",
       "          6           0.791359  0.533150  0.545197\n",
       "          7           0.820266  0.535773  0.548283\n",
       "          8           0.819241  0.535143  0.547675\n",
       "          9           0.818469  0.530938  0.541159\n",
       "4         4           0.817540  0.523642  0.535187\n",
       "          5           0.816789  0.521532  0.530587\n",
       "          6           0.816789  0.521532  0.530587\n",
       "          7           0.816789  0.521532  0.530587\n",
       "          8           0.817902  0.522162  0.531187\n",
       "          9           0.820148  0.523423  0.532389\n",
       "5         5           0.819089  0.513284  0.520406\n",
       "          6           0.819089  0.513284  0.520406\n",
       "          7           0.818834  0.509812  0.514577\n",
       "          8           0.819616  0.509709  0.514328\n",
       "          9           0.819363  0.506237  0.508392\n",
       "6         6           0.817659  0.499336  0.499927\n",
       "          7           0.817659  0.499336  0.499927\n",
       "          8           0.817659  0.499336  0.499927\n",
       "          9           0.817415  0.495864  0.493884\n",
       "7         7           0.815303  0.477081  0.464636\n",
       "          8           0.813333  0.477184  0.464853\n",
       "          9           0.812094  0.473711  0.458244\n",
       "8         8           0.805167  0.451529  0.430413\n",
       "          9           0.805167  0.451529  0.430413\n",
       "9         9           0.795174  0.431002  0.400267"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_results\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "\n",
    "# %pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def normalize_vectors(input_vectors):\n",
    "    \"\"\"Normalizes vectors to be in range [0,1)\n",
    "\n",
    "    Args:\n",
    "        input_vectors (np.ndarray): List of vectors, each vector is np.ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    return MinMaxScaler().fit(input_vectors).transform(input_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence(sentence, wv_model, mode=0):\n",
    "    \"\"\"Convert a sentence to a vector by summing word vectors in the sentence.\n",
    "\n",
    "    Another option could be to average word vectors rather than just sum.\n",
    "\n",
    "    Arg: \n",
    "        sentence (list): Tokenized sentence\n",
    "        mode (int, 1 or 0): 0 for summing word vectors, 1 or averaging word vectors.\n",
    "    Returns:\n",
    "        (np.ndarray): Vector representation of sentence\n",
    "    \"\"\"\n",
    "    if mode not in [0, 1]:\n",
    "        raise Exception(\"Mode parameter should be either 1 or 0\")\n",
    "\n",
    "    vector_size = wv_model.vector_size\n",
    "    wv_res = np.zeros(vector_size)\n",
    "    count = 1\n",
    "    for w in sentence:\n",
    "        if w in wv_model:\n",
    "            wv_res += wv_model[w]\n",
    "            if mode:\n",
    "                count += 1\n",
    "    if mode:\n",
    "        wv_res = wv_res/count\n",
    "    return wv_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence_2(sentences, wv_model, mode=0):\n",
    "    \"\"\"Convert a list of sentences to a vectors.\n",
    "\n",
    "    By summing word vectors in the sentence.\n",
    "    Another option could be to average word vectors rather than just sum.\n",
    "\n",
    "    Arg: \n",
    "        sentence (pd.Series): Series consisting of list of tokenized texts \n",
    "        mode (int, 1 or 0): 0 for summing word vectors, 1 or averaging word vectors.\n",
    "    Returns:\n",
    "        (np.ndarray): Vector representation of sentences\n",
    "    \"\"\"\n",
    "    if mode not in [0, 1]:\n",
    "        raise Exception(\"Mode parameter should be either 1 or 0\")\n",
    "\n",
    "    vector_size = wv_model.vector_size\n",
    "    count = 1\n",
    "    vector_sentences = []\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = np.zeros(vector_size)\n",
    "        for word in sentences[i]:\n",
    "            if word in wv_model:\n",
    "                count += 1\n",
    "                sentence += wv_model[word]\n",
    "        if mode:  # if mode set to 1 (average word vectors)\n",
    "            sentence = sentence/count\n",
    "        vector_sentences.append(sentence)\n",
    "    vector_sentences_df = pd.DataFrame(vector_sentences)\n",
    "    return vector_sentences_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW\n",
    "The parameters tested are window, cbow_modes, and vec_mode.\n",
    "- window determines how many context words around the target word to use.\n",
    "- cbow_mode is either 1 or 0, and indicates whether the context word vectors are summed(0) or averaged(1).\n",
    "- vec_mode is either 1 or 0, and indicates whether the document vector should be the sum(0) or average(1) of its word vectors.  \n",
    "\n",
    "\n",
    "It is observed that the highest macro-f1 score is obtained when the window size is 1, cbow_mode is 1, and the vec_mode is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter values\n",
    "windows = np.arange(1, 6)\n",
    "cbow_modes = [1, 0]\n",
    "vectorization_modes = [0, 1]\n",
    "\n",
    "# create the cartesian product of possible combinaiton of parameters\n",
    "parameters = [(window, cbow_mode, vec_mode)\n",
    "              for window in windows for cbow_mode in cbow_modes for vec_mode in vectorization_modes]\n",
    "\n",
    "results_cbow = {}\n",
    "for window, cbow_mode, vec_mode in parameters:\n",
    "    cbow = Word2Vec(\n",
    "        sentences=X,\n",
    "        window=window,  # how many context words before and after target word to consider\n",
    "        min_count=1,  # ignore words with frequency lower than this\n",
    "        sg=0,  # 0 to use cbow, 1 for skipgram\n",
    "        cbow_mean=cbow_mode,  # 0 uses sum of context word vectors, 1 uses mean\n",
    "    )\n",
    "    cbow.train(X, total_examples=cbow.corpus_count, epochs=cbow.epochs)\n",
    "    # After training, KeyedVector object is used\n",
    "    cbow_model = cbow.wv\n",
    "\n",
    "    # Get the embeddings of train and test set\n",
    "    X_train_cbow = normalize_vectors(\n",
    "        np.array([vectorize_sentence(x, cbow_model, vec_mode) for x in X_train]))\n",
    "    X_test_cbow = normalize_vectors(\n",
    "        np.array([vectorize_sentence(x, cbow_model, vec_mode) for x in X_test]))\n",
    "\n",
    "    # Test NB\n",
    "    nb_cbow = MultinomialNB()\n",
    "    nb_cbow.fit(X_train_cbow, y_train)\n",
    "    y_pred_cbow = nb_cbow.predict(X_test_cbow)\n",
    "\n",
    "    prec, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_test, y_pred_cbow, beta=1.0, average='macro', zero_division=0)\n",
    "    results_cbow[(window, cbow_mode, vec_mode)] = {\n",
    "        'precision': prec, 'recall': recall, 'f1-score': f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision    (1, 0, 1)\n",
       "recall       (1, 1, 0)\n",
       "f1-score     (1, 1, 0)\n",
       "dtype: object"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cbow_results = pd.DataFrame.from_dict(results_cbow, orient='index', columns=[\n",
    "                                      'precision', 'recall', 'f1-score'])\n",
    "cbow_results.index.names = ['window size',\n",
    "                            'cbow mean value', 'document vectorization mode']\n",
    "# get n-grams corresponding to max f1 score\n",
    "cbow_results.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>window size</th>\n",
       "      <th>cbow mean value</th>\n",
       "      <th>document vectorization mode</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0.406385</td>\n",
       "      <td>0.431903</td>\n",
       "      <td>0.410402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.408865</td>\n",
       "      <td>0.383704</td>\n",
       "      <td>0.334447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.394161</td>\n",
       "      <td>0.425044</td>\n",
       "      <td>0.404993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.410892</td>\n",
       "      <td>0.383807</td>\n",
       "      <td>0.335113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">2</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0.375491</td>\n",
       "      <td>0.410916</td>\n",
       "      <td>0.392110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.399158</td>\n",
       "      <td>0.381696</td>\n",
       "      <td>0.335466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.399660</td>\n",
       "      <td>0.428210</td>\n",
       "      <td>0.407584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.403268</td>\n",
       "      <td>0.382854</td>\n",
       "      <td>0.335870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">3</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0.373843</td>\n",
       "      <td>0.410460</td>\n",
       "      <td>0.391289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.398996</td>\n",
       "      <td>0.380861</td>\n",
       "      <td>0.333822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.386608</td>\n",
       "      <td>0.419050</td>\n",
       "      <td>0.399493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.408466</td>\n",
       "      <td>0.384744</td>\n",
       "      <td>0.337342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">4</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0.365628</td>\n",
       "      <td>0.401946</td>\n",
       "      <td>0.382712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.391754</td>\n",
       "      <td>0.382106</td>\n",
       "      <td>0.337584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.391583</td>\n",
       "      <td>0.422216</td>\n",
       "      <td>0.402164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.393045</td>\n",
       "      <td>0.381578</td>\n",
       "      <td>0.337736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">5</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0.362904</td>\n",
       "      <td>0.399205</td>\n",
       "      <td>0.379704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.385898</td>\n",
       "      <td>0.379381</td>\n",
       "      <td>0.334482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.388525</td>\n",
       "      <td>0.420838</td>\n",
       "      <td>0.401080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.397530</td>\n",
       "      <td>0.381066</td>\n",
       "      <td>0.334975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         precision    recall  \\\n",
       "window size cbow mean value document vectorization mode                        \n",
       "1           1               0                             0.406385  0.431903   \n",
       "                            1                             0.408865  0.383704   \n",
       "            0               0                             0.394161  0.425044   \n",
       "                            1                             0.410892  0.383807   \n",
       "2           1               0                             0.375491  0.410916   \n",
       "                            1                             0.399158  0.381696   \n",
       "            0               0                             0.399660  0.428210   \n",
       "                            1                             0.403268  0.382854   \n",
       "3           1               0                             0.373843  0.410460   \n",
       "                            1                             0.398996  0.380861   \n",
       "            0               0                             0.386608  0.419050   \n",
       "                            1                             0.408466  0.384744   \n",
       "4           1               0                             0.365628  0.401946   \n",
       "                            1                             0.391754  0.382106   \n",
       "            0               0                             0.391583  0.422216   \n",
       "                            1                             0.393045  0.381578   \n",
       "5           1               0                             0.362904  0.399205   \n",
       "                            1                             0.385898  0.379381   \n",
       "            0               0                             0.388525  0.420838   \n",
       "                            1                             0.397530  0.381066   \n",
       "\n",
       "                                                         f1-score  \n",
       "window size cbow mean value document vectorization mode            \n",
       "1           1               0                            0.410402  \n",
       "                            1                            0.334447  \n",
       "            0               0                            0.404993  \n",
       "                            1                            0.335113  \n",
       "2           1               0                            0.392110  \n",
       "                            1                            0.335466  \n",
       "            0               0                            0.407584  \n",
       "                            1                            0.335870  \n",
       "3           1               0                            0.391289  \n",
       "                            1                            0.333822  \n",
       "            0               0                            0.399493  \n",
       "                            1                            0.337342  \n",
       "4           1               0                            0.382712  \n",
       "                            1                            0.337584  \n",
       "            0               0                            0.402164  \n",
       "                            1                            0.337736  \n",
       "5           1               0                            0.379704  \n",
       "                            1                            0.334482  \n",
       "            0               0                            0.401080  \n",
       "                            1                            0.334975  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip Gram\n",
    "\n",
    "The parameters tested are window, and vec_mode.\n",
    "- window determines how many context words around the target word to use.\n",
    "- vec_mode is either 1 or 0, and indicates whether the document vector should be the sum(0) or average(1) of its word vectors.  \n",
    "\n",
    "\n",
    "It is observed that the highest macro-f1 score is obtained when the window size is 4 and the vec_mode is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modes: 1, 0\n",
      "modes: 1, 1\n",
      "modes: 2, 0\n",
      "modes: 2, 1\n",
      "modes: 3, 0\n",
      "modes: 3, 1\n",
      "modes: 4, 0\n",
      "modes: 4, 1\n",
      "modes: 5, 0\n",
      "modes: 5, 1\n"
     ]
    }
   ],
   "source": [
    "windows = np.arange(1, 6)\n",
    "vectorization_modes = [0, 1]\n",
    "\n",
    "skipgram_parameters = [(window, vec_mode)\n",
    "                       for window in windows for vec_mode in vectorization_modes]\n",
    "\n",
    "results_sg = {}\n",
    "for window, vec_mode in skipgram_parameters:\n",
    "    skipgram = Word2Vec(\n",
    "        sentences=X,\n",
    "        window=window,  # how many context words before and after target word to consider\n",
    "        min_count=1,  # ignore words with frequency lower than this\n",
    "        sg=1,  # 0 to use cbow, 1 for skipgram\n",
    "    )\n",
    "    skipgram.train(X, total_examples=skipgram.corpus_count,\n",
    "                   epochs=skipgram.epochs)\n",
    "    # After training, KeyedVector object is used\n",
    "    skipgram_model = skipgram.wv\n",
    "\n",
    "    # Get the embeddings of train and test set\n",
    "    X_train_sg = normalize_vectors(\n",
    "        np.array([vectorize_sentence(x, skipgram_model, vec_mode) for x in X_train]))\n",
    "    X_test_sg = normalize_vectors(\n",
    "        np.array([vectorize_sentence(x, skipgram_model, vec_mode) for x in X_test]))\n",
    "\n",
    "    # Test NB\n",
    "    nb_sg = MultinomialNB()\n",
    "    nb_sg.fit(X_train_sg, y_train)\n",
    "    y_pred_sg = nb_sg.predict(X_test_sg)\n",
    "\n",
    "    prec, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_test, y_pred_sg, beta=1.0, average='macro', zero_division=0)\n",
    "    results_sg[(window, vec_mode)] = {\n",
    "        'precision': prec, 'recall': recall, 'f1-score': f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision    (2, 1)\n",
       "recall       (4, 0)\n",
       "f1-score     (4, 0)\n",
       "dtype: object"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram_results = pd.DataFrame.from_dict(results_sg, orient='index', columns=[\n",
    "                                          'precision', 'recall', 'f1-score'])\n",
    "skipgram_results.index.names = ['window size', 'document vectorization mode']\n",
    "# get n-grams corresponding to max f1 score\n",
    "skipgram_results.idxmax()\n",
    "# skipgram_results.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>window size</th>\n",
       "      <th>document vectorization mode</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0.395672</td>\n",
       "      <td>0.424854</td>\n",
       "      <td>0.404232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.420314</td>\n",
       "      <td>0.387485</td>\n",
       "      <td>0.337363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>0</th>\n",
       "      <td>0.390294</td>\n",
       "      <td>0.425101</td>\n",
       "      <td>0.405688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.757977</td>\n",
       "      <td>0.393155</td>\n",
       "      <td>0.347706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>0</th>\n",
       "      <td>0.724105</td>\n",
       "      <td>0.427021</td>\n",
       "      <td>0.411113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.750041</td>\n",
       "      <td>0.400612</td>\n",
       "      <td>0.364189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>0</th>\n",
       "      <td>0.728356</td>\n",
       "      <td>0.429556</td>\n",
       "      <td>0.413292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.757443</td>\n",
       "      <td>0.404597</td>\n",
       "      <td>0.368270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5</th>\n",
       "      <th>0</th>\n",
       "      <td>0.395744</td>\n",
       "      <td>0.425264</td>\n",
       "      <td>0.405005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.756287</td>\n",
       "      <td>0.403235</td>\n",
       "      <td>0.366747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         precision    recall  f1-score\n",
       "window size document vectorization mode                               \n",
       "1           0                             0.395672  0.424854  0.404232\n",
       "            1                             0.420314  0.387485  0.337363\n",
       "2           0                             0.390294  0.425101  0.405688\n",
       "            1                             0.757977  0.393155  0.347706\n",
       "3           0                             0.724105  0.427021  0.411113\n",
       "            1                             0.750041  0.400612  0.364189\n",
       "4           0                             0.728356  0.429556  0.413292\n",
       "            1                             0.757443  0.404597  0.368270\n",
       "5           0                             0.395744  0.425264  0.405005\n",
       "            1                             0.756287  0.403235  0.366747"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pre-trained Word2Vec Model\n",
    "Using pre-trained word2vec model from Google, which has 3 million words. Trained on 100 billion words from the google news dataset. Unsure if it uses CBOW or SkipGram or both\n",
    "\n",
    "From https://thinkingneuron.com/how-to-classify-text-using-word2vec/\n",
    "Google Model can be downloaded from https://thinkingneuron.com/how-to-classify-text-using-word2vec/#:~:text=download%20link%3A%20https%3A%2F%2Fdrive.google.com%2Ffile%2Fd%2F0b7xkcwpi5kdynlnuttlss21pqmm%2Fedit%3Fusp%3Dsharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the word vectors from Google Model (1GB)\n",
    "# Will take time because 3 mil words.\n",
    "# Download model from https://thinkingneuron.com/how-to-classify-text-using-word2vec/#:~:text=download%20link%3A%20https%3A%2F%2Fdrive.google.com%2Ffile%2Fd%2F0b7xkcwpi5kdynlnuttlss21pqmm%2Fedit%3Fusp%3Dsharing\n",
    "GoogleModel = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    '/mnt/d/Andrea/Heriot-Watt/year-4/f20aa/f20aa-coursework-1/GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2 ways to vectorize documents using the Word2Vec model is either by summing (mode=1) or averaging (mode=1) each of the word vectors in the document.\n",
    "\n",
    "It is obeserved that the highest macro-f1 score is obtained when the word vectors are summed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "modes = np.arange(0,2)\n",
    "w2v_data_mode0 = 0\n",
    "w2v_data_mode1 = 0\n",
    "\n",
    "results_google_model = {}\n",
    "for mode in modes:\n",
    "    print(mode)\n",
    "    w2v_data = vectorize_sentence_2(X, GoogleModel, mode=mode)\n",
    "    \n",
    "    # Get train and test values\n",
    "    X_w2v = w2v_data.values\n",
    "    y_w2v = y.values\n",
    "\n",
    "    # scale because NB doesnt accept negative values\n",
    "    X_w2v_scaled = normalize_vectors(X_w2v)\n",
    "\n",
    "    X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(\n",
    "        X_w2v_scaled, y_w2v, test_size=0.2, random_state=42,\n",
    "        stratify=y)\n",
    "\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_w2v, y_train_w2v)\n",
    "    y_pred_w2v = nb.predict(X_test_w2v)\n",
    "\n",
    "    prec, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_test_w2v, y_pred_w2v, beta=1.0, average='macro', zero_division=0)\n",
    "    results_google_model[mode] = {\n",
    "        'precision': prec, 'recall': recall, 'f1-score': f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision    0\n",
       "recall       0\n",
       "f1-score     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_model_results = pd.DataFrame.from_dict(\n",
    "    results_google_model, orient='index', columns=['precision', 'recall', 'f1-score'])\n",
    "google_model_results.index.names = ['document vectorization mode']\n",
    "# get n-grams corresponding to max f1 score\n",
    "google_model_results.idxmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document vectorization mode</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.437536</td>\n",
       "      <td>0.475932</td>\n",
       "      <td>0.454780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.163114</td>\n",
       "      <td>0.332703</td>\n",
       "      <td>0.218905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             precision    recall  f1-score\n",
       "document vectorization mode                               \n",
       "0                             0.437536  0.475932  0.454780\n",
       "1                             0.163114  0.332703  0.218905"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_model_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Text Representation Models\n",
    "The different models are compared based on their macroaveraged precision, recall, and f1 score.\n",
    "Values that are closer to one are optimal.\n",
    "\n",
    "|Model|Precision|Recall|F-1|\n",
    "|---|---|---|---|\n",
    "|Bag Of Words (Unigram)|0.74|0.60|0.62|\n",
    "|N-gram range (1,6)|0.76|0.64|0.67|\n",
    "|TF-IDF on n-gram range (2,3)|0.79|0.56|0.57|\n",
    "|CBOW (Trained on scraped tweets corpus)|0.41|0.43|0.41|\n",
    "|Skip-gram (Trained on scraped tweets corpus)|0.73|0.43|0.41|\n",
    "|Word2Vec model (Trained on Google News Corpus)|0.437536|0.48|0.45|\n",
    "\n",
    "\n",
    "It was expected that the more sophisticated models - those which take into account the context of words - would perform better. However, based on the results the more primitive approaches performed better. This could be due to our small dataset.\n",
    "\n",
    "The n-gram model with range (1,6) was the best performer in terms of macro F-1 score, which is the unweighted harmonic mean of the precision and recall values.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression with N-grams range (1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.78      0.30      0.44        96\n",
      "           0       0.74      0.86      0.80       455\n",
      "           1       0.82      0.79      0.80       529\n",
      "\n",
      "    accuracy                           0.78      1080\n",
      "   macro avg       0.78      0.65      0.68      1080\n",
      "weighted avg       0.78      0.78      0.77      1080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "ngram = CountVectorizer(\n",
    "    ngram_range=(1, 6),  # ngrams where n is 1 through 6\n",
    "    preprocessor=lambda x: x,  # override preprocessing\n",
    "    tokenizer=lambda x: x,  # override tokenization\n",
    ")\n",
    "\n",
    "ngram_lr = Pipeline([\n",
    "    ('ngram', ngram),\n",
    "    ('logistic_regression', LogisticRegression())\n",
    "])\n",
    "\n",
    "ngram_lr.fit(X_train, y_train)\n",
    "ngram_lr_y_pred = ngram_lr.predict(X_test)\n",
    "print(classification_report(y_test, ngram_lr_y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. FastText with Trigrams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastText is an open-source library created by Facebook AI Research lab. Setup instructions are here: https://fasttext.cc/docs/en/supervised-tutorial.html#installing-fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting fasttext\n",
      "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 KB\u001b[0m \u001b[31m346.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/andrea/.local/lib/python3.10/site-packages (from fasttext) (1.23.4)\n",
      "Collecting pybind11>=2.2\n",
      "  Using cached pybind11-2.10.3-py3-none-any.whl (222 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /usr/lib/python3/dist-packages (from fasttext) (59.6.0)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4133920 sha256=8c4d33402fd131b4474303dfc57f9111c382ffdebb43d9b08384b28533ff1da8\n",
      "  Stored in directory: /home/andrea/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
      "Successfully built fasttext\n",
      "Installing collected packages: pybind11, fasttext\n",
      "Successfully installed fasttext-0.9.2 pybind11-2.10.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fasttext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module fasttext.FastText in fasttext:\n",
      "\n",
      "NAME\n",
      "    fasttext.FastText\n",
      "\n",
      "DESCRIPTION\n",
      "    # Copyright (c) 2017-present, Facebook, Inc.\n",
      "    # All rights reserved.\n",
      "    #\n",
      "    # This source code is licensed under the MIT license found in the\n",
      "    # LICENSE file in the root directory of this source tree.\n",
      "\n",
      "FUNCTIONS\n",
      "    cbow(*kargs, **kwargs)\n",
      "    \n",
      "    eprint(*args, **kwargs)\n",
      "    \n",
      "    load_model(path)\n",
      "        Load a model given a filepath and return a model object.\n",
      "    \n",
      "    read_args(arg_list, arg_dict, arg_names, default_values)\n",
      "    \n",
      "    skipgram(*kargs, **kwargs)\n",
      "    \n",
      "    supervised(*kargs, **kwargs)\n",
      "    \n",
      "    tokenize(text)\n",
      "        Given a string of text, tokenize it and return a list of tokens\n",
      "    \n",
      "    train_supervised(*kargs, **kwargs)\n",
      "        Train a supervised model and return a model object.\n",
      "        \n",
      "        input must be a filepath. The input text does not need to be tokenized\n",
      "        as per the tokenize function, but it must be preprocessed and encoded\n",
      "        as UTF-8. You might want to consult standard preprocessing scripts such\n",
      "        as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html\n",
      "        \n",
      "        The input file must must contain at least one label per line. For an\n",
      "        example consult the example datasets which are part of the fastText\n",
      "        repository such as the dataset pulled by classification-example.sh.\n",
      "    \n",
      "    train_unsupervised(*kargs, **kwargs)\n",
      "        Train an unsupervised model and return a model object.\n",
      "        \n",
      "        input must be a filepath. The input text does not need to be tokenized\n",
      "        as per the tokenize function, but it must be preprocessed and encoded\n",
      "        as UTF-8. You might want to consult standard preprocessing scripts such\n",
      "        as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html\n",
      "        \n",
      "        The input field must not contain any labels or use the specified label prefix\n",
      "        unless it is ok for those words to be ignored. For an example consult the\n",
      "        dataset pulled by the example script word-vector-example.sh, which is\n",
      "        part of the fastText repository.\n",
      "\n",
      "DATA\n",
      "    BOW = '<'\n",
      "    EOS = '</s>'\n",
      "    EOW = '>'\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    displayed_errors = {}\n",
      "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 1310...\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
      "    unsupervised_default = {'autotuneDuration': 300, 'autotuneMetric': 'f1...\n",
      "\n",
      "FILE\n",
      "    /home/andrea/.local/lib/python3.10/site-packages/fasttext/FastText.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "help(fasttext.FastText)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformatting the dataset as FastText expects each row of data to be in the form:\n",
    "__label__sentiment tweet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5398, 2)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftDf = updated_dataset[['TextBlob Sentiment', 'Cleaned Text']].copy()\n",
    "ftDf.iloc[:, 0] = ftDf.iloc[:, 0].apply(lambda x: '__label__' + x)\n",
    "ftDf.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into test and train\n",
    "training = ftDf.sample(frac=0.7)\n",
    "testing = ftDf.drop(training.index)\n",
    "\n",
    "training.to_csv('train.txt',\n",
    "                index=False,\n",
    "                sep=' ',\n",
    "                header=None,\n",
    "                quoting=csv.QUOTE_NONE,\n",
    "                quotechar=\"\",\n",
    "                escapechar=\" \")\n",
    "\n",
    "testing.to_csv('test.txt',\n",
    "               index=False,\n",
    "               sep=' ',\n",
    "               header=None,\n",
    "               quoting=csv.QUOTE_NONE,\n",
    "               quotechar=\"\",\n",
    "               escapechar=\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftModel = fasttext.train_supervised('train.txt', wordNgrams=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1619, 0.44224830142063004, 0.44224830142063004)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftModel.test('test.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate further, we can test it on unseen samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__Neutral',), array([0.46955368]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftModel.predict(\n",
    "    'I love the UAE for its innovation with artificial intelligence.')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the test set, fastText model had a precision score of 0.44 ad a recall score of 0.44. It had also misclassified an unseen sample. However, FastText model parameters can be adjusted, so we will try adjusting the epoch and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1619, 0.7794935145151328, 0.7794935145151328)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftModel2 = fasttext.train_supervised(\n",
    "    'train.txt', wordNgrams=3, epoch=6, lr=0.25)\n",
    "ftModel2.test('test.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__Positive',), array([0.68560129]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftModel2.predict(\n",
    "    'I love the UAE for its innovation with artificial intelligence.')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, increasing the epoch and learning rate corrected a misclassification that happened with the previous model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4d3d5844ec835cb994d9b961c91f9f9a1dfb3461f5792c6e96deb8bb5afec0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Distribution\n",
    "| Member | ID | Tasks |\n",
    "|---|---|---|\n",
    "|Bhavika| - | - | \n",
    "|Alora| - | - |\n",
    "|Andrea| - | - |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tweepy in /home/andrea/.local/lib/python3.10/site-packages (4.12.1)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in /home/andrea/.local/lib/python3.10/site-packages (from tweepy) (2.28.1)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /home/andrea/.local/lib/python3.10/site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/lib/python3/dist-packages (from tweepy) (3.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/andrea/.local/lib/python3.10/site-packages (from requests<3,>=2.27.0->tweepy) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/andrea/.local/lib/python3.10/site-packages (from requests<3,>=2.27.0->tweepy) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/andrea/.local/lib/python3.10/site-packages (from requests<3,>=2.27.0->tweepy) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/andrea/.local/lib/python3.10/site-packages (from requests<3,>=2.27.0->tweepy) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keys for authentication\n",
    "consumer_key = \"ONLQ0vLSAXM2tZDnckominEcK\"\n",
    "consumer_secret = \"MpP7XurdGn1wpufX3rrfQgrs4AROnaw9ZiiZzKN6exr3zEDlN6\"\n",
    "access_token = \"1274112117738737664-J2DjjoqqzN11CNp8bpnYzVp1dUYKM4\"\n",
    "access_token_secret = \"3coYFJVCqHnOmD2m9Qt6zs7j1I24lhlPTU71eUOq4Zqg9\"\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAALJslQEAAAAA7EWdkTwrVDM44LIuewNascHjvoY%3DeXmhQyuXgo8zXJiSTMfqtmPA393HHO2W7nSrk0bK6b67SuJeb0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate\n",
    "auth = tweepy.OAuth1UserHandler(\n",
    "    consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tweets(query: str, amount: int) -> list:\n",
    "    \"\"\"Scrapes a specified number of tweets based on given query and location.\n",
    "\n",
    "    Method from https://www.sahilfruitwala.com/guide-to-extract-tweets-using-tweepy#how-to-retrieve-specific-number-of-tweets-using-tweepy \n",
    "    \n",
    "    Args:\n",
    "        query (str): The query\n",
    "    Returns:\n",
    "        list (str): List of contents of tweets.\n",
    "    \"\"\"\n",
    "    extracted_tweets = []\n",
    "    for tweet in tweepy.Cursor(api.search_tweets, query, count=100, tweet_mode=\"extended\", result_type=\"recent\").items(amount):\n",
    "        extracted_tweets.append(tweet.full_text)\n",
    "    return extracted_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(tweets: list, csv_name: str) -> None:\n",
    "    \"\"\"Appends the list of tweets to csv.\n",
    "    \n",
    "    Largely copied from https://gist.github.com/anku255/0cebd75cce675f2b56de1ef48ec06575.\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Writing {0} tweets to {1}\".format(len(tweets), csv_name))\n",
    "    tweets_for_csv = [[tweet.encode(\"utf-8\")] for tweet in tweets]\n",
    "    with open(csv_name, \"a+\") as file:\n",
    "        writer = csv.writer(file, delimiter=\",\")\n",
    "        writer.writerows(tweets_for_csv)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The queries for scraping tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "queries = [\n",
    "    [\"(metaverse OR meta verse OR #metaverse OR #meta #verse) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(innovation OR #innovation OR innovate OR innovative OR #innovate OR #innovative) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(sustainability OR #sustainability OR sustainable) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(technology footprint OR technology OR #technologyfootprint) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(artificial intelligence OR ai OR #ai OR #artificialintelligence) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(ethics OR #ethics OR ethical OR #ethical) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(inflation OR #inflation) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cop 28 OR cop28 OR #cop28 OR #cop #28) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(museum of the future OR @museumofthefuture OR #museumofthefuture OR #museum #of #the #future) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(bit coin OR bitcoin OR #bitcoin OR #crypto OR crpyto OR cryptocurrency) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cyber attack OR #cyberattack OR #cyber OR #cyberattacks OR #cyber #attack) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(robot OR robots OR #robots OR #robot) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(drone OR drones OR #drone OR #drones) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(hacker OR hacking OR #hacker OR #hacking OR #hack) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(chatgpt OR #chatgpt OR #chat #gpt OR chat gpt) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cybersecurity OR #cybersecurityOR #cybersec OR cybersec OR cyber security OR #cyber #security) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(sustainabletech OR sustainable ai OR sustainable technology OR #sustainabletech OR #sustainableai OR #sustainabletechnology) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(computing OR quantum computing OR #quantum #computing OR #computing) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(automation OR #automation) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(distributed cloud OR #distributedcloud OR #distributed #cloud) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(big data OR #big #data OR bigdata OR #bigdata) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(ar OR #ar OR #augmentedreality OR #augmented #reality) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(data mining OR #data #mining OR #datamining) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(software OR #software) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(tech OR technology OR #tech OR #technology) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(digital OR digital transformation OR #digital #transformation OR #digitaltransformation) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(blockchain OR #blockchain) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(coders_hq OR coders hq OR #coders OR #coders #hq OR @coders_hq) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(nft OR non-fungible token OR non fungible token OR nfts OR #nfts OR #nonfungibletoken OR #nft) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(binance OR @binance OR #binance) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(uae hackathon OR hackathon OR #hackathon) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(internet of things OR IoT OR #iot OR #internet #of #things OR #internetofthings) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(software engineering OR #softwareengineering) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(#coding OR coding) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(@uaeai OR #uaeai Or #uae #ai) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(technology OR tech OT #tech Or #technology) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(nft OR #nft OR #nfts OR nfts) AND (place_country:AE OR uae) lang:en -filter:retweets\"]   ,\n",
    "    [\"(UAE codes OR #UAE_codes OR #UAEcodes) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(Microsoft Hololens OR Hololens OR #Hololens) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(GITEX or #gitex) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(information OR #information) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(internet OR #internet) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(computer OR science OR computer science OR #computerscience OR #computer OR #science) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(technological OR #technological) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(engineering OR #engineering) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(computing OR #computing) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(systems OR #systems) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(electronics OR #electronics) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(application OR #application OR app OR #app) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(robotics OR #robotics) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(high tech OR high technology OR #hightech OR #hightechnology OR #high #technology OR #high #tech) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(nanotech OR nano tech OR nanotechnology OR nano technology OR #nanotech OR #nano #tech OR #nano #technology OR #nanotechnology) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(biotech OR bio tech OR biotechnology OR bio technology OR #biotech OR #bio #tech OR #bio #technology OR #biotechnology) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(code OR #code) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(automation OR #automation OR automate OR #automate) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(virtual OR #virtual OR online OR #online) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(technological advancements OR #technological #advancements OR #advancements OR #techadvancements) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(bionics OR #bionics) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cyberscience OR #cyberscience) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(google OR #google OR @google) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(oracle OR #oracle OR @oracle) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(ibm OR #ibm OR @ibm) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(bionics OR #bionics) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(sap OR #sap) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(global knowledge OR #globalknowledge OR #global #knowledge) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(amazon OR #amazon OR @amazon) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(hewlett-packard OR Hewlett Packard OR HP OR #hewlett-packard OR #Hewlett #Packard OR #HP) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cisco OR #cisco OR @cisco) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(dell OR #dell OR @dell) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(cisco OR #cisco OR @cisco) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(@WorldGovSummit OR #WGS OR #WGS2023 OR World Goverment Summit) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(masdar OR #masdar OR @masdar) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(renewable energy OR clean energy OR #renewableenergy OR #cleanenergy) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(machine learning OR #machinelearning OR ML OR #ML) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(recommender systems OR #RS OR #recommender #systems OR recommender engine) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(deep learning OR #deeplearning) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "    [\"(research OR #research) AND (place_country:AE OR uae) lang:en -filter:retweets\"],\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commented out below to prevent re-running code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Scraping tweets and writing them to dataset.csv\"\"\"\n",
    "# csv_name=\"dataset.csv\"\n",
    "# total = 0\n",
    "# for query in queries:\n",
    "#     tweets = scrape_tweets(query=query, amount=500)\n",
    "#     total += len(tweets)\n",
    "#     print(\"Found {0} tweets related to the query {1}\".format(len(tweets), query))\n",
    "#     write_to_csv(tweets=tweets,csv_name=csv_name)\n",
    "# print(\"Done. {0} rows.\".format(total))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section cleans the scraped tweets dataset of duplicate tweets, URLs, @'s and #s."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "- Do not modify original csv `dataset.csv`. The dataset with no duplicates is stored in `updated_dataset.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates\n",
    "dataset = pd.read_csv(\"dataset.csv\", header=None)\n",
    "dataset.rename(columns={0: 'Text'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'Joined UAE Minister &amp;amp; President-Designat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Foreign Secretary @AmbVMKwatra participated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'As the UAE marks National Environment Day, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'RT @KhalifaAlgaz: Did you know that the UAE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'RT @tawhidChtioui: \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21974</th>\n",
       "      <td>b'Send our special gifts to your loved ones!\\x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21975</th>\n",
       "      <td>b'Trainer code: 2735 9457 4031\\nfrom \\xf0\\x9f\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21976</th>\n",
       "      <td>b\"We are looking for ICY SNOW , SANDSTORM .\\nI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21977</th>\n",
       "      <td>b'Make #MONEY from #home through PC #APP \\nMak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21978</th>\n",
       "      <td>b'#Earn Money from #home through PC #APP\\nwith...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21979 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text\n",
       "0      b'Joined UAE Minister &amp; President-Designat...\n",
       "1      b'Foreign Secretary @AmbVMKwatra participated ...\n",
       "2      b'As the UAE marks National Environment Day, w...\n",
       "3      b'RT @KhalifaAlgaz: Did you know that the UAE ...\n",
       "4      b'RT @tawhidChtioui: \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\...\n",
       "...                                                  ...\n",
       "21974  b'Send our special gifts to your loved ones!\\x...\n",
       "21975  b'Trainer code: 2735 9457 4031\\nfrom \\xf0\\x9f\\...\n",
       "21976  b\"We are looking for ICY SNOW , SANDSTORM .\\nI...\n",
       "21977  b'Make #MONEY from #home through PC #APP \\nMak...\n",
       "21978  b'#Earn Money from #home through PC #APP\\nwith...\n",
       "\n",
       "[21979 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'Joined UAE Minister &amp;amp; President-Designat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Foreign Secretary @AmbVMKwatra participated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'As the UAE marks National Environment Day, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'RT @KhalifaAlgaz: Did you know that the UAE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'RT @tawhidChtioui: \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21888</th>\n",
       "      <td>b'@m_ut67 @modgovae @Forsan_UAE Dear sir pleas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21889</th>\n",
       "      <td>b'@UAE_BARQ Dear sir please my help you sir my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21890</th>\n",
       "      <td>b'@UAE_BARQ Dear sir please my help you sir my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21915</th>\n",
       "      <td>b\"#PhonePe launched a service on Tuesday that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21962</th>\n",
       "      <td>b'@WeLoveIndia7 Saudi k log bhikhari h ? \\nIn ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5878 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text\n",
       "0      b'Joined UAE Minister &amp; President-Designat...\n",
       "1      b'Foreign Secretary @AmbVMKwatra participated ...\n",
       "2      b'As the UAE marks National Environment Day, w...\n",
       "3      b'RT @KhalifaAlgaz: Did you know that the UAE ...\n",
       "4      b'RT @tawhidChtioui: \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\...\n",
       "...                                                  ...\n",
       "21888  b'@m_ut67 @modgovae @Forsan_UAE Dear sir pleas...\n",
       "21889  b'@UAE_BARQ Dear sir please my help you sir my...\n",
       "21890  b'@UAE_BARQ Dear sir please my help you sir my...\n",
       "21915  b\"#PhonePe launched a service on Tuesday that ...\n",
       "21962  b'@WeLoveIndia7 Saudi k log bhikhari h ? \\nIn ...\n",
       "\n",
       "[5878 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset = dataset.drop_duplicates().copy()\n",
    "updated_dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing URLs, @'s and hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = updated_dataset['Text'].copy()\n",
    "updated_dataset['Cleaned Text'] = tweets.str.replace(r'@[^\\s]+|#[^\\s]+|https?:\\/\\/\\S+|www\\.\\S+', '',regex=True)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Duplicates again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5398, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset = updated_dataset.drop('Text', axis=1)\n",
    "updated_dataset = updated_dataset.drop_duplicates()\n",
    "updated_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dataset.to_csv('updated-dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cleaned Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'Joined UAE Minister &amp;amp; President-Designat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Foreign Secretary  participated in a meeting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'As the UAE marks National Environment Day, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'RT  Did you know that the UAE is developing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'RT  \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\x8f UAE lunar r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21888</th>\n",
       "      <td>b'   Dear sir please my help you sir my two ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21889</th>\n",
       "      <td>b' Dear sir please my help you sir my two kidn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21890</th>\n",
       "      <td>b' Dear sir please my help you sir my two kidn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21915</th>\n",
       "      <td>b\" launched a service on Tuesday that will all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21962</th>\n",
       "      <td>b' Saudi k log bhikhari h ? \\nIn Middle East, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5398 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Cleaned Text\n",
       "0      b'Joined UAE Minister &amp; President-Designat...\n",
       "1      b'Foreign Secretary  participated in a meeting...\n",
       "2      b'As the UAE marks National Environment Day, w...\n",
       "3      b'RT  Did you know that the UAE is developing ...\n",
       "4      b'RT  \\xf0\\x9f\\x9b\\xb0\\xef\\xb8\\x8f UAE lunar r...\n",
       "...                                                  ...\n",
       "21888  b'   Dear sir please my help you sir my two ki...\n",
       "21889  b' Dear sir please my help you sir my two kidn...\n",
       "21890  b' Dear sir please my help you sir my two kidn...\n",
       "21915  b\" launched a service on Tuesday that will all...\n",
       "21962  b' Saudi k log bhikhari h ? \\nIn Middle East, ...\n",
       "\n",
       "[5398 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset = pd.read_csv('updated-dataset.csv', index_col=0)\n",
    "updated_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Labelling\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data labelling, we decided to use first use TextBlob. TextBlob calculates the subjectivity and polarity of a text to classify the text as 'Positive', 'Negative' or 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: TextBlob in /home/andrea/.local/lib/python3.10/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in /home/andrea/.local/lib/python3.10/site-packages (from TextBlob) (3.8.1)\n",
      "Requirement already satisfied: click in /home/andrea/.local/lib/python3.10/site-packages (from nltk>=3.1->TextBlob) (8.1.3)\n",
      "Requirement already satisfied: joblib in /home/andrea/.local/lib/python3.10/site-packages (from nltk>=3.1->TextBlob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/andrea/.local/lib/python3.10/site-packages (from nltk>=3.1->TextBlob) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /home/andrea/.local/lib/python3.10/site-packages (from nltk>=3.1->TextBlob) (4.64.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get subjectivity\n",
    "def getSubjectivity(text):\n",
    "   return TextBlob(text).sentiment.subjectivity\n",
    "  \n",
    " \n",
    "#Function to get the polarity\n",
    "def getPolarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "   \n",
    "#Create two new columns ‘Subjectivity’ & ‘Polarity’\n",
    "updated_dataset['TextBlob Subjectivity'] =    updated_dataset['Cleaned Text'].apply(getSubjectivity)\n",
    "updated_dataset['TextBlob Polarity'] = updated_dataset['Cleaned Text'].apply(getPolarity)\n",
    "def getAnalysis(score):\n",
    "  if score < 0:\n",
    "    return 'Negative'\n",
    "  elif score == 0:\n",
    "    return 'Neutral'\n",
    "  else:\n",
    "    return 'Positive'\n",
    "\n",
    "updated_dataset['TextBlob Sentiment'] =  updated_dataset['TextBlob Polarity'].apply(getAnalysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive    2643\n",
       "Neutral     2276\n",
       "Negative     479\n",
       "Name: TextBlob Sentiment, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset['TextBlob Sentiment'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that TextBlob classified the majority of tweets to be Positive. To compare, we also decided to use Vader as well to observe if there was a major difference in classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: vaderSentiment in /home/andrea/.local/lib/python3.10/site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in /home/andrea/.local/lib/python3.10/site-packages (from vaderSentiment) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/andrea/.local/lib/python3.10/site-packages (from requests->vaderSentiment) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/andrea/.local/lib/python3.10/site-packages (from requests->vaderSentiment) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/andrea/.local/lib/python3.10/site-packages (from requests->vaderSentiment) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/andrea/.local/lib/python3.10/site-packages (from requests->vaderSentiment) (2022.12.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "def getSentiment(text):\n",
    "    return sentiment.polarity_scores(text)\n",
    "    \n",
    "updated_dataset['Vader Analysis'] = updated_dataset['Cleaned Text'].apply(getSentiment)\n",
    "\n",
    "def sentimentAnalysis(sentiment_dict):\n",
    "    if sentiment_dict['compound'] >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif sentiment_dict['compound'] <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "updated_dataset['Vader Sentiment'] = updated_dataset['Vader Analysis'].apply(sentimentAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vader Analysis</th>\n",
       "      <th>Vader Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.638, 'pos': 0.362, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.816, 'pos': 0.184, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.874, 'pos': 0.126, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'neg': 0.0, 'neu': 0.819, 'pos': 0.181, 'comp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21888</th>\n",
       "      <td>{'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21889</th>\n",
       "      <td>{'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21890</th>\n",
       "      <td>{'neg': 0.074, 'neu': 0.719, 'pos': 0.206, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21915</th>\n",
       "      <td>{'neg': 0.043, 'neu': 0.774, 'pos': 0.183, 'co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21962</th>\n",
       "      <td>{'neg': 0.196, 'neu': 0.777, 'pos': 0.028, 'co...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5398 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Vader Analysis Vader Sentiment\n",
       "0      {'neg': 0.0, 'neu': 0.638, 'pos': 0.362, 'comp...        Positive\n",
       "1      {'neg': 0.0, 'neu': 0.816, 'pos': 0.184, 'comp...        Positive\n",
       "2      {'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'comp...        Positive\n",
       "3      {'neg': 0.0, 'neu': 0.874, 'pos': 0.126, 'comp...        Positive\n",
       "4      {'neg': 0.0, 'neu': 0.819, 'pos': 0.181, 'comp...        Positive\n",
       "...                                                  ...             ...\n",
       "21888  {'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...        Positive\n",
       "21889  {'neg': 0.073, 'neu': 0.726, 'pos': 0.202, 'co...        Positive\n",
       "21890  {'neg': 0.074, 'neu': 0.719, 'pos': 0.206, 'co...        Positive\n",
       "21915  {'neg': 0.043, 'neu': 0.774, 'pos': 0.183, 'co...        Positive\n",
       "21962  {'neg': 0.196, 'neu': 0.777, 'pos': 0.028, 'co...        Negative\n",
       "\n",
       "[5398 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset[['Vader Analysis', 'Vader Sentiment']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive    3335\n",
       "Neutral     1691\n",
       "Negative     372\n",
       "Name: Vader Sentiment, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset['Vader Sentiment'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**changed approach → first data cleaning + tokenization then stemming OR lemmatization. (comparison)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Non-English Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    b'joined uae minister &amp; president-designat...\n",
       "1    b'foreign secretary  participated in a meeting...\n",
       "2    b'as the uae marks national environment day, w...\n",
       "3    b'rt  did you know that the uae is developing ...\n",
       "4    b'rt   uae lunar rover will test 1st artificia...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nonenglish(tweet):\n",
    "    tweet = re.sub(r'\\\\x[a-zA-Z0-9]+', '', tweet) # tweets in different language changed to hex code \\xhh\n",
    "    tweet = re.sub(r'\\\\n', '', tweet) # removing new line character as well\n",
    "    return tweet\n",
    "\n",
    "updated_dataset['Cleaned Text'] = updated_dataset['Cleaned Text'].apply(lambda tweet: nonenglish(tweet).lower())\n",
    "updated_dataset['Cleaned Text'].head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    b'joined uae minister &amp; president-designat...\n",
       "1    b'foreign secretary  participated meeting foca...\n",
       "2    b'as uae marks national environment day, remai...\n",
       "3    b'rt  did know uae developing arabic chatgpt u...\n",
       "4    b'rt   uae lunar rover test 1st artificial int...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "# contains 318 stopwords, including but not limited to articles and prepositions\n",
    "stopwords = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "updated_dataset['Cleaned Text'] = updated_dataset['Cleaned Text'].apply(lambda tweet: ' '.join([text for text in tweet.split(' ') if text not in stopwords]))\n",
    "updated_dataset['Cleaned Text'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Symbols and Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    bjoined uae minister amp presidentdesignate  d...\n",
       "1    bforeign secretary  participated meeting focal...\n",
       "2    bas uae marks national environment day remain ...\n",
       "3    brt  did know uae developing arabic chatgpt us...\n",
       "4    brt   uae lunar rover test 1st artificial inte...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "punctuations = string.punctuation\n",
    "print(punctuations)\n",
    "\n",
    "updated_dataset['Cleaned Text'] = updated_dataset['Cleaned Text'].apply(lambda tweet: tweet.translate({ord(punc): None for punc in punctuations}))\n",
    "updated_dataset['Cleaned Text'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    bjoined uae minister amp presidentdesignate  d...\n",
       "1    bforeign secretary  participated meeting focal...\n",
       "2    bas uae marks national environment day remain ...\n",
       "3    brt  did know uae developing arabic chatgpt us...\n",
       "4    brt   uae lunar rover test st artificial intel...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset['Cleaned Text'] = updated_dataset['Cleaned Text'].apply(lambda tweet: re.sub('[0-9]+', '', tweet))\n",
    "updated_dataset['Cleaned Text'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing 'b' and 'RT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    joined uae minister amp presidentdesignate  dr...\n",
       "1    foreign secretary  participated meeting focal ...\n",
       "2    as uae marks national environment day remain c...\n",
       "3    rt  did know uae developing araic chatgpt usin...\n",
       "4    rt   uae lunar rover test st artificial intell...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset['Cleaned Text'] = updated_dataset['Cleaned Text'].apply(lambda tweet: re.sub('bRT|b', '', tweet))\n",
    "updated_dataset['Cleaned Text'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [joined, uae, minister, amp, presidentdesignat...\n",
       "1    [foreign, secretary, participated, meeting, fo...\n",
       "2    [as, uae, marks, national, environment, day, r...\n",
       "3    [rt, did, know, uae, developing, araic, chatgp...\n",
       "4    [rt, uae, lunar, rover, test, st, artificial, ...\n",
       "Name: Tokenized Text, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as data has been completely cleaned, we can simply tokenize by converting strings to lists\n",
    "updated_dataset['Tokenized Text'] = updated_dataset['Cleaned Text'].apply(lambda tweet: tweet.split())\n",
    "updated_dataset['Tokenized Text'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/andrea/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [join, uae, minist, amp, presidentdesign, dr, ...\n",
       "1    [foreign, secretari, particip, meet, focal, po...\n",
       "2    [as, uae, mark, nation, environ, day, remain, ...\n",
       "3    [rt, did, know, uae, develop, araic, chatgpt, ...\n",
       "4    [rt, uae, lunar, rover, test, st, artifici, in...\n",
       "Name: Stemmed Text, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming = nltk.stem.PorterStemmer()\n",
    "\n",
    "updated_dataset['Stemmed Text'] = updated_dataset['Tokenized Text'].apply(lambda tweet: [stemming.stem(text) for text in tweet])\n",
    "updated_dataset['Stemmed Text'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [joined, uae, minister, amp, presidentdesignat...\n",
       "1    [foreign, secretary, participated, meeting, fo...\n",
       "2    [a, uae, mark, national, environment, day, rem...\n",
       "3    [rt, did, know, uae, developing, araic, chatgp...\n",
       "4    [rt, uae, lunar, rover, test, st, artificial, ...\n",
       "Name: Lemmatized Text, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizing = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "updated_dataset['Lemmatized Text'] = updated_dataset['Tokenized Text'].apply(lambda tweet: [lemmatizing.lemmatize(text) for text in tweet])\n",
    "updated_dataset['Lemmatized Text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode TextBlob Sentiments into numbers\n",
    "updated_dataset['Encoded_TextBlob_Sentiment'] = updated_dataset['TextBlob Sentiment'].apply(lambda x: 1 if x=='Positive' else -1 if x=='Negative' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dataset.reset_index(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Representation\n",
    "\n",
    "This section experiments with different document representations, namely:\n",
    "- Bag of Words\n",
    "- N-grams\n",
    "- TF-IDF\n",
    "- CBOW\n",
    "- Skip-gram\n",
    "- Pre-trained Word2Vec model by Google\n",
    "\n",
    "\n",
    "Each of the representations are testing using Naive Bayes, and the high performance models are used in the next stage of the pipeline.\n",
    "\n",
    "Note: Arbitrarily using lemmatization and textblob sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = updated_dataset['Lemmatized Text']\n",
    "y = updated_dataset['Encoded_TextBlob_Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    updated_dataset['Lemmatized Text'], \n",
    "    updated_dataset['Encoded_TextBlob_Sentiment'],\n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=updated_dataset['Encoded_TextBlob_Sentiment']\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bow = CountVectorizer(\n",
    "    ngram_range= (1,1), # unigram\n",
    "    preprocessor= lambda x: x, # override preprocessing\n",
    "    tokenizer= lambda x: x, # override tokenization\n",
    ")\n",
    "\n",
    "nb_bow = Pipeline([\n",
    "    ('bow', bow),\n",
    "    ('naive_bayes', MultinomialNB())\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.76      0.27      0.40        96\n",
      "           0       0.84      0.60      0.70       455\n",
      "           1       0.67      0.92      0.78       529\n",
      "\n",
      "    accuracy                           0.73      1080\n",
      "   macro avg       0.76      0.60      0.63      1080\n",
      "weighted avg       0.75      0.73      0.71      1080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_bow.fit(X_train, y_train)\n",
    "y_pred_bow = nb_bow.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_bow))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams\n",
    "How to find best n-gram number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.76      0.32      0.45        96\n",
      "           0       0.86      0.63      0.73       455\n",
      "           1       0.69      0.92      0.79       529\n",
      "\n",
      "    accuracy                           0.75      1080\n",
      "   macro avg       0.77      0.63      0.66      1080\n",
      "weighted avg       0.77      0.75      0.73      1080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngram = CountVectorizer(\n",
    "    ngram_range= (1,3), # unigram\n",
    "    preprocessor= lambda x: x, # override preprocessing\n",
    "    tokenizer= lambda x: x, # override tokenization\n",
    ")\n",
    "\n",
    "ngram_nb = Pipeline([\n",
    "    ('ngram', ngram),\n",
    "    ('naive_bayes', MultinomialNB())\n",
    "])\n",
    "\n",
    "ngram_nb.fit(X_train, y_train)\n",
    "ngram_nb_y_pred = ngram_nb.predict(X_test)\n",
    "print(classification_report(y_test, ngram_nb_y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.90      0.09      0.17        96\n",
      "           0       0.87      0.56      0.68       455\n",
      "           1       0.65      0.95      0.77       529\n",
      "\n",
      "    accuracy                           0.71      1080\n",
      "   macro avg       0.81      0.54      0.54      1080\n",
      "weighted avg       0.77      0.71      0.68      1080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    preprocessor= lambda x: x, # override preprocessing\n",
    "    tokenizer= lambda x: x, # override tokenization\n",
    ")\n",
    "\n",
    "tfidf_nb = Pipeline([\n",
    "    ('tfidf', tfidf),\n",
    "    ('naive_bayes', MultinomialNB())\n",
    "])\n",
    "\n",
    "tfidf_nb.fit(X_train, y_train)\n",
    "tfidf_nb_y_pred = tfidf_nb.predict(X_test)\n",
    "print(classification_report(y_test, tfidf_nb_y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible parameters to tweak is window, cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = Word2Vec(\n",
    "    sentences=X,\n",
    "    window=10, # how many context words before and after target word to consider\n",
    "    min_count=2,\n",
    "    workers=4, # number of threads\n",
    "    sg=0, # 0 to use cbow, 1 for skipgram\n",
    "    cbow_mean=0, # 0 uses sum of context word vectors, 1 uses mean\n",
    ")\n",
    "cbow.train(X, total_examples=cbow.corpus_count,epochs=cbow.epochs)\n",
    "# After training, KeyedVector object is used\n",
    "cbow_model = cbow.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def normalize_vectors(input_vectors):\n",
    "    \"\"\"Normalizes vectors to be in range [0,1)\n",
    "    \n",
    "    Args:\n",
    "        input_vectors (np.ndarray): List of vectors, each vector is np.ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    return MinMaxScaler().fit(input_vectors).transform(input_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence(sentence, wv_model):\n",
    "    \"\"\"Convert a sentence to a vector by summing word vectors in the sentence.\n",
    "\n",
    "    Another option could be to average word vectors rather than just sum.\n",
    "    \n",
    "    Arg: \n",
    "        sentence (list): Tokenized sentence\n",
    "    Returns:\n",
    "        (np.ndarray): Vector representation of sentence\n",
    "    \"\"\"\n",
    "    vector_size = wv_model.vector_size\n",
    "    wv_res = np.zeros(vector_size)\n",
    "    count = 0\n",
    "    for w in sentence:\n",
    "        if w in wv_model:\n",
    "            # count += 1\n",
    "            wv_res += wv_model[w]\n",
    "    # wv_res = wv_res/count\n",
    "    return wv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00        96\n",
      "           0       0.75      0.26      0.39       455\n",
      "           1       0.54      0.94      0.69       529\n",
      "\n",
      "    accuracy                           0.57      1080\n",
      "   macro avg       0.43      0.40      0.36      1080\n",
      "weighted avg       0.58      0.57      0.50      1080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the embeddings of train and test set\n",
    "X_train_cbow = normalize_vectors(np.array([vectorize_sentence(x,cbow_model) for x in X_train]))\n",
    "X_test_cbow = normalize_vectors(np.array([vectorize_sentence(x,cbow_model) for x in X_test]))\n",
    "\n",
    "#test NB\n",
    "nb_cbow = MultinomialNB()\n",
    "nb_cbow.fit(X_train_cbow, y_train)\n",
    "y_pred_cbow = nb_cbow.predict(X_test_cbow)\n",
    "print(classification_report(y_test, y_pred_cbow, zero_division=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram = Word2Vec(\n",
    "    sentences=X,\n",
    "    window=10,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    sg=1, # 0 to use cbow, 1 for skipgram\n",
    ")\n",
    "# Train the model\n",
    "skipgram.train(X, total_examples=skipgram.corpus_count,epochs=skipgram.epochs)\n",
    "\n",
    "# After training, KeyedVector object is used\n",
    "skipgram_model = skipgram.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00        96\n",
      "           0       0.77      0.35      0.48       455\n",
      "           1       0.56      0.93      0.70       529\n",
      "\n",
      "    accuracy                           0.60      1080\n",
      "   macro avg       0.44      0.42      0.39      1080\n",
      "weighted avg       0.60      0.60      0.54      1080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the embeddings of train and test set\n",
    "X_train_sg = normalize_vectors(np.array([vectorize_sentence(x,skipgram_model) for x in X_train]))\n",
    "X_test_sg = normalize_vectors(np.array([vectorize_sentence(x,skipgram_model) for x in X_test]))\n",
    "#test on NB\n",
    "nb_sg = MultinomialNB()\n",
    "nb_sg.fit(X_train_sg, y_train)\n",
    "y_pred_sg = nb_sg.predict(X_test_sg)\n",
    "print(classification_report(y_test, y_pred_sg, zero_division=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pre-trained Word2Vec Model\n",
    "Using pre-trained word2vec model from Google, which has 3 million words. Trained on 100 billion words from the google news dataset. Unsure if it uses CBOW or SkipGram or both\n",
    "\n",
    "From https://thinkingneuron.com/how-to-classify-text-using-word2vec/\n",
    "Google Model can be downloaded from https://thinkingneuron.com/how-to-classify-text-using-word2vec/#:~:text=download%20link%3A%20https%3A%2F%2Fdrive.google.com%2Ffile%2Fd%2F0b7xkcwpi5kdynlnuttlss21pqmm%2Fedit%3Fusp%3Dsharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the word vectors from Google Model (1GB)\n",
    "# Will take time because 3 mil words. \n",
    "# Download model from https://thinkingneuron.com/how-to-classify-text-using-word2vec/#:~:text=download%20link%3A%20https%3A%2F%2Fdrive.google.com%2Ffile%2Fd%2F0b7xkcwpi5kdynlnuttlss21pqmm%2Fedit%3Fusp%3Dsharing\n",
    "GoogleModel = gensim.models.KeyedVectors.load_word2vec_format('/mnt/d/Andrea/Heriot-Watt/year-4/f20aa/f20aa-coursework-1/GoogleNews-vectors-negative300.bin', binary=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence_2(sentences, wv_model):\n",
    "    \"\"\"Convert a list of sentences to a vectors.\n",
    "\n",
    "    By summing word vectors in the sentence.\n",
    "    Another option could be to average word vectors rather than just sum.\n",
    "    \n",
    "    Arg: \n",
    "        sentence (pd.Series): Series consisting of list of tokenized texts \n",
    "    Returns:\n",
    "        (np.ndarray): Vector representation of sentences\n",
    "    \"\"\"\n",
    "    vector_size = wv_model.vector_size\n",
    "    count = 0\n",
    "    vector_sentences = []\n",
    "\n",
    "    for i in range(len(sentences)): \n",
    "        sentence = np.zeros(vector_size)\n",
    "        for word in sentences[i]:\n",
    "            count+=1\n",
    "            if word in wv_model:\n",
    "                sentence += wv_model[word]\n",
    "        # sentence = sentence/count\n",
    "        vector_sentences.append(sentence)\n",
    "    vector_sentences_df = pd.DataFrame(vector_sentences)\n",
    "    \n",
    "    return vector_sentences_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert text data to W2V vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_data = vectorize_sentence_2(X, GoogleModel)\n",
    "w2v_data.reset_index(inplace=True, drop=True)\n",
    "w2v_data['sentiment']=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.517578</td>\n",
       "      <td>2.149384</td>\n",
       "      <td>0.244385</td>\n",
       "      <td>1.057068</td>\n",
       "      <td>0.005615</td>\n",
       "      <td>-0.110962</td>\n",
       "      <td>-1.076660</td>\n",
       "      <td>-4.466125</td>\n",
       "      <td>2.398560</td>\n",
       "      <td>1.838623</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.183201</td>\n",
       "      <td>2.587189</td>\n",
       "      <td>-1.779480</td>\n",
       "      <td>1.421364</td>\n",
       "      <td>-3.537811</td>\n",
       "      <td>-1.348389</td>\n",
       "      <td>-1.448334</td>\n",
       "      <td>-0.404602</td>\n",
       "      <td>0.051758</td>\n",
       "      <td>4.679199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.202881</td>\n",
       "      <td>1.526184</td>\n",
       "      <td>1.248779</td>\n",
       "      <td>0.022171</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.535217</td>\n",
       "      <td>1.304497</td>\n",
       "      <td>-1.866699</td>\n",
       "      <td>1.610382</td>\n",
       "      <td>0.834839</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.435616</td>\n",
       "      <td>-0.429688</td>\n",
       "      <td>0.471130</td>\n",
       "      <td>0.044685</td>\n",
       "      <td>-0.072174</td>\n",
       "      <td>0.426514</td>\n",
       "      <td>1.126831</td>\n",
       "      <td>-0.328308</td>\n",
       "      <td>0.306915</td>\n",
       "      <td>1.507935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.591887</td>\n",
       "      <td>2.016541</td>\n",
       "      <td>1.312225</td>\n",
       "      <td>1.879883</td>\n",
       "      <td>-1.195282</td>\n",
       "      <td>-0.182129</td>\n",
       "      <td>2.341568</td>\n",
       "      <td>-2.139465</td>\n",
       "      <td>1.937492</td>\n",
       "      <td>1.381104</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.698547</td>\n",
       "      <td>2.054138</td>\n",
       "      <td>-2.196167</td>\n",
       "      <td>0.245865</td>\n",
       "      <td>-0.480743</td>\n",
       "      <td>0.292847</td>\n",
       "      <td>-0.172760</td>\n",
       "      <td>-0.098694</td>\n",
       "      <td>1.208496</td>\n",
       "      <td>0.037415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.187256</td>\n",
       "      <td>0.425644</td>\n",
       "      <td>0.694336</td>\n",
       "      <td>0.746582</td>\n",
       "      <td>-0.830971</td>\n",
       "      <td>0.188629</td>\n",
       "      <td>0.263794</td>\n",
       "      <td>-1.604248</td>\n",
       "      <td>0.192200</td>\n",
       "      <td>0.070190</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.465820</td>\n",
       "      <td>0.378479</td>\n",
       "      <td>-1.248840</td>\n",
       "      <td>0.507454</td>\n",
       "      <td>-0.283112</td>\n",
       "      <td>0.090332</td>\n",
       "      <td>0.951538</td>\n",
       "      <td>-0.135376</td>\n",
       "      <td>-0.143707</td>\n",
       "      <td>0.971436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.289551</td>\n",
       "      <td>0.470154</td>\n",
       "      <td>0.040894</td>\n",
       "      <td>1.148956</td>\n",
       "      <td>-0.361826</td>\n",
       "      <td>-0.663452</td>\n",
       "      <td>1.364746</td>\n",
       "      <td>-1.629761</td>\n",
       "      <td>0.779358</td>\n",
       "      <td>0.507080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206055</td>\n",
       "      <td>2.159424</td>\n",
       "      <td>-0.898987</td>\n",
       "      <td>1.384079</td>\n",
       "      <td>-0.363281</td>\n",
       "      <td>0.470703</td>\n",
       "      <td>-1.204590</td>\n",
       "      <td>-1.422424</td>\n",
       "      <td>-0.242188</td>\n",
       "      <td>-0.068848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5393</th>\n",
       "      <td>-0.779114</td>\n",
       "      <td>1.713898</td>\n",
       "      <td>0.965210</td>\n",
       "      <td>2.606506</td>\n",
       "      <td>-1.261993</td>\n",
       "      <td>0.792786</td>\n",
       "      <td>0.899628</td>\n",
       "      <td>0.057434</td>\n",
       "      <td>2.080322</td>\n",
       "      <td>1.345978</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.424438</td>\n",
       "      <td>-0.558105</td>\n",
       "      <td>-2.832397</td>\n",
       "      <td>-0.131470</td>\n",
       "      <td>1.480103</td>\n",
       "      <td>1.432678</td>\n",
       "      <td>-2.717224</td>\n",
       "      <td>-1.542297</td>\n",
       "      <td>0.498291</td>\n",
       "      <td>0.041809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5394</th>\n",
       "      <td>-0.779114</td>\n",
       "      <td>1.713898</td>\n",
       "      <td>0.965210</td>\n",
       "      <td>2.606506</td>\n",
       "      <td>-1.261993</td>\n",
       "      <td>0.792786</td>\n",
       "      <td>0.899628</td>\n",
       "      <td>0.057434</td>\n",
       "      <td>2.080322</td>\n",
       "      <td>1.345978</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.424438</td>\n",
       "      <td>-0.558105</td>\n",
       "      <td>-2.832397</td>\n",
       "      <td>-0.131470</td>\n",
       "      <td>1.480103</td>\n",
       "      <td>1.432678</td>\n",
       "      <td>-2.717224</td>\n",
       "      <td>-1.542297</td>\n",
       "      <td>0.498291</td>\n",
       "      <td>0.041809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5395</th>\n",
       "      <td>-0.779114</td>\n",
       "      <td>1.713898</td>\n",
       "      <td>0.965210</td>\n",
       "      <td>2.606506</td>\n",
       "      <td>-1.261993</td>\n",
       "      <td>0.792786</td>\n",
       "      <td>0.899628</td>\n",
       "      <td>0.057434</td>\n",
       "      <td>2.080322</td>\n",
       "      <td>1.345978</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.424438</td>\n",
       "      <td>-0.558105</td>\n",
       "      <td>-2.832397</td>\n",
       "      <td>-0.131470</td>\n",
       "      <td>1.480103</td>\n",
       "      <td>1.432678</td>\n",
       "      <td>-2.717224</td>\n",
       "      <td>-1.542297</td>\n",
       "      <td>0.498291</td>\n",
       "      <td>0.041809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5396</th>\n",
       "      <td>-0.170532</td>\n",
       "      <td>0.555901</td>\n",
       "      <td>0.524551</td>\n",
       "      <td>1.156067</td>\n",
       "      <td>-0.659790</td>\n",
       "      <td>0.719757</td>\n",
       "      <td>-0.405373</td>\n",
       "      <td>-0.833130</td>\n",
       "      <td>0.974854</td>\n",
       "      <td>0.811157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787354</td>\n",
       "      <td>1.457672</td>\n",
       "      <td>0.223541</td>\n",
       "      <td>0.326981</td>\n",
       "      <td>0.550018</td>\n",
       "      <td>-0.178589</td>\n",
       "      <td>0.676849</td>\n",
       "      <td>-0.140881</td>\n",
       "      <td>0.202820</td>\n",
       "      <td>-0.080078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5397</th>\n",
       "      <td>-0.910950</td>\n",
       "      <td>1.570068</td>\n",
       "      <td>0.804808</td>\n",
       "      <td>3.015259</td>\n",
       "      <td>-0.817932</td>\n",
       "      <td>-0.055786</td>\n",
       "      <td>-0.946167</td>\n",
       "      <td>-2.200195</td>\n",
       "      <td>0.376193</td>\n",
       "      <td>1.857975</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.392151</td>\n",
       "      <td>2.878784</td>\n",
       "      <td>-0.290070</td>\n",
       "      <td>1.094849</td>\n",
       "      <td>-1.044220</td>\n",
       "      <td>-1.327454</td>\n",
       "      <td>-2.159546</td>\n",
       "      <td>-0.651855</td>\n",
       "      <td>0.339622</td>\n",
       "      <td>1.169434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5398 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0    -1.517578  2.149384  0.244385  1.057068  0.005615 -0.110962 -1.076660   \n",
       "1    -2.202881  1.526184  1.248779  0.022171 -0.643616 -0.535217  1.304497   \n",
       "2    -1.591887  2.016541  1.312225  1.879883 -1.195282 -0.182129  2.341568   \n",
       "3    -0.187256  0.425644  0.694336  0.746582 -0.830971  0.188629  0.263794   \n",
       "4    -1.289551  0.470154  0.040894  1.148956 -0.361826 -0.663452  1.364746   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5393 -0.779114  1.713898  0.965210  2.606506 -1.261993  0.792786  0.899628   \n",
       "5394 -0.779114  1.713898  0.965210  2.606506 -1.261993  0.792786  0.899628   \n",
       "5395 -0.779114  1.713898  0.965210  2.606506 -1.261993  0.792786  0.899628   \n",
       "5396 -0.170532  0.555901  0.524551  1.156067 -0.659790  0.719757 -0.405373   \n",
       "5397 -0.910950  1.570068  0.804808  3.015259 -0.817932 -0.055786 -0.946167   \n",
       "\n",
       "           7         8         9    ...       290       291       292  \\\n",
       "0    -4.466125  2.398560  1.838623  ... -2.183201  2.587189 -1.779480   \n",
       "1    -1.866699  1.610382  0.834839  ... -2.435616 -0.429688  0.471130   \n",
       "2    -2.139465  1.937492  1.381104  ... -1.698547  2.054138 -2.196167   \n",
       "3    -1.604248  0.192200  0.070190  ... -0.465820  0.378479 -1.248840   \n",
       "4    -1.629761  0.779358  0.507080  ...  0.206055  2.159424 -0.898987   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5393  0.057434  2.080322  1.345978  ... -0.424438 -0.558105 -2.832397   \n",
       "5394  0.057434  2.080322  1.345978  ... -0.424438 -0.558105 -2.832397   \n",
       "5395  0.057434  2.080322  1.345978  ... -0.424438 -0.558105 -2.832397   \n",
       "5396 -0.833130  0.974854  0.811157  ...  0.787354  1.457672  0.223541   \n",
       "5397 -2.200195  0.376193  1.857975  ... -1.392151  2.878784 -0.290070   \n",
       "\n",
       "           293       294       295       296       297       298       299  \n",
       "0     1.421364 -3.537811 -1.348389 -1.448334 -0.404602  0.051758  4.679199  \n",
       "1     0.044685 -0.072174  0.426514  1.126831 -0.328308  0.306915  1.507935  \n",
       "2     0.245865 -0.480743  0.292847 -0.172760 -0.098694  1.208496  0.037415  \n",
       "3     0.507454 -0.283112  0.090332  0.951538 -0.135376 -0.143707  0.971436  \n",
       "4     1.384079 -0.363281  0.470703 -1.204590 -1.422424 -0.242188 -0.068848  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5393 -0.131470  1.480103  1.432678 -2.717224 -1.542297  0.498291  0.041809  \n",
       "5394 -0.131470  1.480103  1.432678 -2.717224 -1.542297  0.498291  0.041809  \n",
       "5395 -0.131470  1.480103  1.432678 -2.717224 -1.542297  0.498291  0.041809  \n",
       "5396  0.326981  0.550018 -0.178589  0.676849 -0.140881  0.202820 -0.080078  \n",
       "5397  1.094849 -1.044220 -1.327454 -2.159546 -0.651855  0.339622  1.169434  \n",
       "\n",
       "[5398 rows x 300 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and test values\n",
    "X_w2v = w2v_data.iloc[:,:-1].values\n",
    "y_w2v = w2v_data.iloc[:,-1].values\n",
    "\n",
    "X_w2v_scaled = normalize_vectors(X_w2v)\n",
    "\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(\n",
    "    X_w2v_scaled, y_w2v, test_size=0.2, random_state=42,\n",
    "    stratify= y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00        96\n",
      "           0       0.67      0.63      0.65       455\n",
      "           1       0.64      0.79      0.70       529\n",
      "\n",
      "    accuracy                           0.65      1080\n",
      "   macro avg       0.43      0.47      0.45      1080\n",
      "weighted avg       0.59      0.65      0.62      1080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_w2v, y_train_w2v)\n",
    "y_pred_w2v = nb.predict(X_test_w2v)\n",
    "print(classification_report(y_test_w2v, y_pred_w2v, zero_division=0))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Text Representation Models\n",
    "The different models are compared based on their weighted precision, recall, and f1 score.\n",
    "Values that are closer to one are optimal.\n",
    "\n",
    "|Model|Precision|Recall|F-1|\n",
    "|---|---|---|---|\n",
    "|Bag Of Words (Unigram)|0.75|0.73|0.71|\n",
    "|Trigrams|0.77|0.75|0.73|\n",
    "|TF-IDF|0.77|0.71|0.68|\n",
    "|CBOW (Trained on scraped tweets corpus)|0.58|0.57|0.5|\n",
    "|Skip-gram (Trained on scraped tweets corpus)|0.6|0.60|0.54|\n",
    "|Word2Vec model (Trained on Google News Corpus)|0.59|0.65|0.62|\n",
    "\n",
    "\n",
    "It was expected that the more sophisticated models - those which take into account the context of words - would perform better. However, based on the results the more primitive approaches performed better. This could be due to our small dataset.\n",
    "\n",
    "The tri-gram model was the best performer in terms of F-1 score, which is the harmonic mean of the precision and recall values.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
